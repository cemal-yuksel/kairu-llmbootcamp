"""
Retrieval-Augmented Generation (RAG) Sistemi
===========================================

Bu dosya, temel bir RAG (Retrieval-Augmented Generation) sisteminin adÄ±m adÄ±m nasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± gÃ¶sterir.
Her adÄ±m ve fonksiyon iÃ§in detaylÄ± TÃ¼rkÃ§e aÃ§Ä±klamalar ve yorumlar eklenmiÅŸtir.

RAG SÃ¼reci:
1. Belge veri tabanÄ± oluÅŸturma
2. Sorgu embedding'i Ã§Ä±karma
3. En yakÄ±n belgeyi bulma (retrieval)
4. LLM ile prompt oluÅŸturma ve yanÄ±t alma

KÄ±saca akÄ±ÅŸ:
Query â†’ Embedding â†’ Similarity Search â†’ Document Retrieval â†’ Prompt + Context â†’ LLM â†’ Response

Gerekli KÃ¼tÃ¼phaneler:
pip install sentence-transformers openai anthropic numpy python-dotenv
"""


# Gerekli kÃ¼tÃ¼phaneler import ediliyor
import numpy as np  # SayÄ±sal iÅŸlemler iÃ§in
import os  # Ortam deÄŸiÅŸkenleri ve dosya iÅŸlemleri iÃ§in
from sentence_transformers import SentenceTransformer  # Metinleri vektÃ¶re dÃ¶nÃ¼ÅŸtÃ¼rmek iÃ§in
from sklearn.metrics.pairwise import cosine_similarity  # VektÃ¶rler arasÄ± benzerlik Ã¶lÃ§Ã¼mÃ¼ iÃ§in
from typing import List, Dict, Tuple  # Tip ipuÃ§larÄ±
import json  # JSON iÅŸlemleri iÃ§in (bu Ã¶rnekte kullanÄ±lmÄ±yor)
from dotenv import load_dotenv  # .env dosyasÄ±ndan API anahtarÄ± okumak iÃ§in


# .env dosyasÄ±ndan API anahtarlarÄ±nÄ± yÃ¼kle
load_dotenv()


# OpenAI ile LLM kullanÄ±mÄ± iÃ§in import iÅŸlemi
try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False


print("ğŸ¤– RAG (Retrieval-Augmented Generation) Sistemi")
print("="*60)


# =====================
# 1. Belge Veri TabanÄ± OluÅŸturma
# =====================
print("\nğŸ“š 1. Belge Veri TabanÄ± OluÅŸturma")
print("-" * 40)

## Ã–rnek belge koleksiyonu
# Her belge bir sÃ¶zlÃ¼k (dict) olarak tanÄ±mlanÄ±r
documents = [
    {
        "id": "doc_1",
        "title": "Python Programlama",
        "content": """Python, yÃ¼ksek seviyeli, yorumlamalÄ± bir programlama dilidir. 
        1991 yÄ±lÄ±nda Guido van Rossum tarafÄ±ndan geliÅŸtirilmiÅŸtir. Python'un sÃ¶zdizimi 
        oldukÃ§a basit ve okunabilirdir. Web geliÅŸtirme, veri analizi, yapay zeka ve 
        bilimsel hesaplamalar iÃ§in yaygÄ±n olarak kullanÄ±lÄ±r. Django ve Flask gibi 
        popÃ¼ler web framework'leri vardÄ±r.""",
        "category": "Programlama"
    },
    {
        "id": "doc_2", 
        "title": "Yapay Zeka ve Machine Learning",
        "content": """Yapay zeka (AI), makinelerin insan benzeri zeka gerektiren gÃ¶revleri 
        yerine getirmesi anlamÄ±na gelir. Machine learning, AI'nin bir alt dalÄ±dÄ±r ve 
        makinelerin veriden Ã¶ÄŸrenmesini saÄŸlar. TensorFlow, PyTorch ve Scikit-learn 
        gibi kÃ¼tÃ¼phaneler ML geliÅŸtirme iÃ§in kullanÄ±lÄ±r. Supervised learning, 
        unsupervised learning ve reinforcement learning temel ML tÃ¼rleridir.""",
        "category": "Yapay Zeka"
    },
    {
        "id": "doc_3",
        "title": "Veri Bilimi ve Analitik",
        "content": """Veri bilimi, bÃ¼yÃ¼k veri setlerinden anlamlÄ± bilgiler Ã§Ä±karma sanatÄ±dÄ±r. 
        Pandas, NumPy ve Matplotlib gibi Python kÃ¼tÃ¼phaneleri veri manipÃ¼lasyonu ve 
        gÃ¶rselleÅŸtirme iÃ§in kullanÄ±lÄ±r. Veri temizleme, keÅŸif analizi, istatistiksel 
        modelleme ve makine Ã¶ÄŸrenmesi veri biliminin temel bileÅŸenleridir. 
        Ä°ÅŸ zekasÄ± ve karar verme sÃ¼reÃ§lerinde kritik role sahiptir.""",
        "category": "Veri Bilimi"
    },
    {
        "id": "doc_4",
        "title": "Web GeliÅŸtirme",
        "content": """Web geliÅŸtirme, internet iÃ§in web siteleri ve uygulamalarÄ± oluÅŸturma 
        sÃ¼recidir. Frontend geliÅŸtirme HTML, CSS ve JavaScript kullanÄ±r. Backend 
        geliÅŸtirme iÃ§in Python (Django, Flask), JavaScript (Node.js) veya diÄŸer 
        diller kullanÄ±labilir. Responsive tasarÄ±m, API geliÅŸtirme ve veritabanÄ± 
        yÃ¶netimi modern web geliÅŸtirmenin temel konularÄ±dÄ±r.""",
        "category": "Web GeliÅŸtirme"
    },
    {
        "id": "doc_5",
        "title": "VeritabanÄ± YÃ¶netimi",
        "content": """VeritabanÄ± yÃ¶netim sistemleri (DBMS), verilerin organize edilmesi, 
        saklanmasÄ± ve eriÅŸimi iÃ§in kullanÄ±lÄ±r. SQL (Structured Query Language) 
        iliÅŸkisel veritabanlarÄ± iÃ§in standart dildir. PostgreSQL, MySQL ve SQLite 
        popÃ¼ler iliÅŸkisel veritabanlarÄ±dÄ±r. NoSQL veritabanlarÄ± (MongoDB, Redis) 
        esnek veri modelleri sunar. ACID Ã¶zellikleri veri tutarlÄ±lÄ±ÄŸÄ±nÄ± saÄŸlar.""",
        "category": "VeritabanÄ±"
    }
]


# Belgelerin baÅŸarÄ±yla yÃ¼klendiÄŸini ekrana yazdÄ±r
print(f"âœ… {len(documents)} belge yÃ¼klendi:")
for doc in documents:
    print(f"   ğŸ“„ {doc['id']}: {doc['title']} ({doc['category']})")


# =====================
# 2. Embedding Model YÃ¼kleme ve Belge Embedding'leri
# =====================
print("\nğŸ§  2. Embedding Model YÃ¼kleme")
print("-" * 40)


# SentenceTransformer ile Ã¶nceden eÄŸitilmiÅŸ bir embedding modeli yÃ¼kleniyor
model = SentenceTransformer('all-MiniLM-L6-v2')
print(f"âœ… Model yÃ¼klendi: {model.get_sentence_embedding_dimension()} boyutlu embedding")


# Belgelerin iÃ§erikleri vektÃ¶rlere (embedding) dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼yor
print("\nğŸ”„ Belge embedding'leri oluÅŸturuluyor...")
document_texts = [doc['content'] for doc in documents]  # Her belgenin metni
document_embeddings = model.encode(document_texts)  # Embedding iÅŸlemi

print(f"âœ… {len(document_embeddings)} belge embedding'i oluÅŸturuldu")
print(f"ğŸ“Š Embedding ÅŸekli: {document_embeddings.shape}")


# =====================
# 3. RAG Pipeline FonksiyonlarÄ±
# =====================
print("\nâš™ï¸  3. RAG Pipeline FonksiyonlarÄ±")
print("-" * 40)


def retrieve_documents(query: str, top_k: int = 1) -> List[Tuple[Dict, float]]:
    """
    Sorgu iÃ§in en yakÄ±n belgeleri bulur
    
    Args:
        query: Arama sorgusu (kullanÄ±cÄ±dan gelen metin)
        top_k: KaÃ§ adet en yakÄ±n belge dÃ¶ndÃ¼rÃ¼lecek
    
    Returns:
        (belge, benzerlik_skoru) tuple'larÄ± listesi
    """
    print(f"ğŸ” Sorgu: '{query}'")
    
    # 1. Sorgu embedding'i Ã§Ä±karÄ±lÄ±r
    query_embedding = model.encode([query])
    print(f"ğŸ“Š Sorgu embedding boyutu: {query_embedding.shape}")
    
    # 2. Cosine similarity ile sorgu ve belgeler arasÄ±ndaki benzerlikler hesaplanÄ±r
    similarities = cosine_similarity(query_embedding, document_embeddings)[0]
    print(f"ğŸ’¯ Benzerlik skorlarÄ± hesaplandÄ±: {len(similarities)} belge")
    
    # 3. En yÃ¼ksek benzerlik skoruna sahip belgeler seÃ§ilir
    top_indices = np.argsort(similarities)[::-1][:top_k]
    
    results = []
    print(f"\nğŸ¯ En yakÄ±n {top_k} belge:")
    for i, idx in enumerate(top_indices):
        doc = documents[idx]
        score = similarities[idx]
        results.append((doc, score))
        print(f"   {i+1}. {doc['title']} (Skor: {score:.4f})")
    
    return results


def create_rag_prompt(query: str, context_docs: List[Dict]) -> str:
    """
    RAG iÃ§in LLM'e gÃ¶nderilecek prompt'u oluÅŸturur
    
    Args:
        query: KullanÄ±cÄ±dan gelen soru
        context_docs: Sorguya en yakÄ±n belgeler
    
    Returns:
        LLM'e gÃ¶nderilecek prompt metni
    """
    # BaÄŸlam belgeleri birleÅŸtirilir
    context_text = "\n\n".join([
        f"Belge {i+1} - {doc['title']}:\n{doc['content']}"
        for i, doc in enumerate(context_docs)
    ])
    
    # LLM'e gÃ¶nderilecek prompt formatÄ±
    prompt = f"""AÅŸaÄŸÄ±daki baÄŸlam bilgilerini kullanarak soruyu yanÄ±tla. Sadece verilen baÄŸlamda yer alan bilgileri kullan.

BAÄLAM:
{context_text}

SORU: {query}

YANIT:"""
    
    return prompt


def answer_with_openai(prompt: str) -> str:
    """
    OpenAI API ile LLM'den yanÄ±t almak iÃ§in fonksiyon
    Args:
        prompt: LLM'e gÃ¶nderilecek metin
    Returns:
        LLM yanÄ±tÄ± veya hata mesajÄ±
    """
    if not OPENAI_AVAILABLE:
        return "âŒ OpenAI kÃ¼tÃ¼phanesi yÃ¼klÃ¼ deÄŸil. 'pip install openai' komutu ile yÃ¼kleyin."
    
    api_key = os.getenv('OPENAI_API_KEY')
    if not api_key:
        return "âŒ OPENAI_API_KEY environment variable bulunamadÄ±. .env dosyasÄ±na ekleyin."
    try:
        # OpenAI istemcisi baÅŸlatÄ±lÄ±r
        client = openai.OpenAI(api_key=api_key)
        # ChatCompletion ile yanÄ±t alÄ±nÄ±r
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "Sen yardÄ±mcÄ± bir AI asistanÄ±sÄ±n. Verilen baÄŸlam bilgilerini kullanarak sorularÄ± yanÄ±tla."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=500,
            temperature=0.3,
            top_p=1.0
        )
        return response.choices[0].message.content.strip()
    except openai.AuthenticationError:
        return "âŒ OpenAI API anahtarÄ± geÃ§ersiz. LÃ¼tfen doÄŸru API anahtarÄ±nÄ± .env dosyasÄ±na ekleyin."
    except openai.RateLimitError:
        return "âŒ OpenAI API rate limit aÅŸÄ±ldÄ±. LÃ¼tfen daha sonra tekrar deneyin."
    except openai.APIConnectionError:
        return "âŒ OpenAI API'ye baÄŸlanÄ±lamadÄ±. Ä°nternet baÄŸlantÄ±nÄ±zÄ± kontrol edin."
    except Exception as e:
        return f"âŒ OpenAI API hatasÄ±: {str(e)}"



def rag_pipeline(query: str, llm_provider: str = "mock") -> Dict:
    """
    Tam RAG pipeline'Ä± adÄ±m adÄ±m Ã§alÄ±ÅŸtÄ±rÄ±r
    Args:
        query: KullanÄ±cÄ±dan gelen soru
        llm_provider: "openai" (gerÃ§ek LLM) veya "mock" (Ã¶rnek yanÄ±t)
    Returns:
        RAG sonuÃ§larÄ±nÄ± iÃ§eren sÃ¶zlÃ¼k
    """
    print(f"\nğŸš€ RAG Pipeline BaÅŸlatÄ±lÄ±yor...")
    print(f"ğŸ”§ LLM Provider: {llm_provider}")
    
    # 1. Retrieval - En yakÄ±n belgeleri bul
    print(f"\nğŸ“– ADIM 1: RETRIEVAL")
    retrieved_docs = retrieve_documents(query, top_k=2)
    context_docs = [doc for doc, score in retrieved_docs]
    
    # 2. Prompt oluÅŸturma
    print(f"\nâœï¸  ADIM 2: PROMPT OLUÅTURMA")
    prompt = create_rag_prompt(query, context_docs)
    print(f"ğŸ“ Prompt uzunluÄŸu: {len(prompt)} karakter")
    
    # 3. LLM ile yanÄ±t alma
    print(f"\nğŸ¤– ADIM 3: LLM YANITI")
    if llm_provider == "openai":
        response = answer_with_openai(prompt)
    else:
        # Mock yanÄ±t: GerÃ§ek LLM kullanÄ±lmadÄ±ÄŸÄ±nda Ã¶rnek Ã§Ä±ktÄ±
        response = f"""Bu bir mock yanÄ±ttÄ±r. GerÃ§ek RAG sistemi ÅŸu adÄ±mlarÄ± tamamladÄ±:

1. âœ… Sorgu embedding'i oluÅŸturuldu
2. âœ… En yakÄ±n belgeler bulundu:
   - {context_docs[0]['title']} (Skor: {retrieved_docs[0][1]:.4f})
   - {context_docs[1]['title']} (Skor: {retrieved_docs[1][1]:.4f})
3. âœ… Prompt oluÅŸturuldu ({len(prompt)} karakter)
4. ğŸ”„ LLM yanÄ±tÄ± bekleniyor...

GerÃ§ek LLM kullanmak iÃ§in API anahtarÄ±nÄ±zÄ± .env dosyasÄ±na ekleyin:
- OpenAI: OPENAI_API_KEY=sk-your-key-here"""
    return {
        'query': query,
        'retrieved_docs': retrieved_docs,
        'prompt': prompt,
        'response': response,
        'llm_provider': llm_provider
    }


# =====================
# 4. RAG Sistemini Test Etme
# =====================
print("\nğŸ§ª 4. RAG Sistemini Test Etme")
print("-" * 40)


# Test iÃ§in Ã¶rnek sorgular
test_queries = [
    "Python nedir ve ne iÃ§in kullanÄ±lÄ±r?",
    "Machine learning tÃ¼rleri nelerdir?",
    "Veri bilimi iÃ§in hangi Python kÃ¼tÃ¼phaneleri kullanÄ±lÄ±r?",
    "Web geliÅŸtirmede frontend ve backend arasÄ±ndaki fark nedir?",
    "SQL nedir?"
]

print(f"ğŸ“‹ {len(test_queries)} test sorusu hazÄ±rlandÄ±:")
for i, query in enumerate(test_queries, 1):
    print(f"   {i}. {query}")

# Ä°lk sorgu ile detaylÄ± test
print(f"\nğŸ” DETAYLI TEST - Ä°lk Sorgu")
print("="*50)

test_query = test_queries[0]
result = rag_pipeline(test_query, llm_provider="mock")

print(f"\nğŸ“Š RAG SONUÃ‡LARI:")
print(f"ğŸ”¤ Sorgu: {result['query']}")
print(f"\nğŸ“š Bulunan Belgeler:")
for i, (doc, score) in enumerate(result['retrieved_docs'], 1):
    print(f"   {i}. {doc['title']} - Skor: {score:.4f}")
    print(f"      Kategori: {doc['category']}")
    print(f"      Ä°Ã§erik: {doc['content'][:100]}...")

print(f"\nğŸ“ OluÅŸturulan Prompt (ilk 200 karakter):")
print(f"'{result['prompt'][:200]}...'")

print(f"\nğŸ¤– LLM YanÄ±tÄ±:")
print(result['response'])


# =====================
# 5. HÄ±zlÄ± Test - TÃ¼m Sorgular
# =====================
print(f"\nâš¡ 5. HÄ±zlÄ± Test - TÃ¼m Sorgular")
print("-" * 40)

for i, query in enumerate(test_queries, 1):
    print(f"\nğŸ“‹ Test {i}: {query}")
    retrieved = retrieve_documents(query, top_k=1)
    best_doc, score = retrieved[0]
    print(f"   ğŸ¯ En iyi eÅŸleÅŸme: {best_doc['title']} (Skor: {score:.4f})")


# =====================
# 6. RAG Sistem Metrikleri
# =====================
print(f"\nğŸ“ˆ 6. RAG Sistem Metrikleri")
print("-" * 40)

# Embedding kalitesi analizi: Sorgular ile belgeler arasÄ±ndaki benzerliklerin istatistikleri
all_similarities = []
for query in test_queries:
    query_embedding = model.encode([query])
    similarities = cosine_similarity(query_embedding, document_embeddings)[0]
    all_similarities.extend(similarities)

avg_similarity = np.mean(all_similarities)
max_similarity = np.max(all_similarities)
min_similarity = np.min(all_similarities)

print(f"ğŸ¯ Embedding PerformansÄ±:")
print(f"   Ortalama benzerlik: {avg_similarity:.4f}")
print(f"   Maksimum benzerlik: {max_similarity:.4f}")
print(f"   Minimum benzerlik: {min_similarity:.4f}")

print(f"\nğŸ”§ Sistem Ã–zellikleri:")
print(f"   ğŸ“š Toplam belge sayÄ±sÄ±: {len(documents)}")
print(f"   ğŸ§  Embedding boyutu: {document_embeddings.shape[1]}")
print(f"   ğŸ“Š Model: {model.get_sentence_embedding_dimension()}D sentence-transformer")


# =====================
# 7. RAG Sistem Ä°yileÅŸtirme Ã–nerileri
# =====================
print(f"\nğŸ’¡ 7. RAG Sistem Ä°yileÅŸtirme Ã–nerileri")
print("-" * 40)

# RAG sistemini daha iyi hale getirmek iÃ§in Ã¶neriler
improvement_tips = """
ğŸš€ PERFORMANS Ä°YÄ°LEÅTÄ°RMELERÄ°:

1. ğŸ“š Veri Kalitesi:
    â€¢ Belgeleri daha kÃ¼Ã§Ã¼k parÃ§alara (chunk) bÃ¶lmek
    â€¢ Metadata eklemek (kategori, tarih, kaynak)
    â€¢ Overlapping chunk'lar kullanmak

2. ğŸ” Retrieval Ä°yileÅŸtirme:
    â€¢ Hybrid search (anlamsal + anahtar kelime)
    â€¢ Re-ranking modelleri kullanmak
    â€¢ Query expansion uygulamak

3. ğŸ¤– LLM Optimizasyonu:
    â€¢ Prompt engineering
    â€¢ Few-shot Ã¶rnekler eklemek
    â€¢ YanÄ±t doÄŸrulama

4. ğŸ“Š DeÄŸerlendirme:
    â€¢ BLEU, ROUGE skorlarÄ±
    â€¢ Ä°nsan deÄŸerlendirmesi
    â€¢ A/B testing

5. ğŸ—ï¸ Mimari Ä°yileÅŸtirmeler:
    â€¢ VektÃ¶r veritabanÄ± kullanmak (Pinecone, Weaviate)
    â€¢ Caching stratejileri
    â€¢ Asenkron iÅŸlem
"""

print(improvement_tips)

print(f"\nâœ… RAG sistemi demonstrasyonu tamamlandÄ±!")

print(f"ğŸ“„ .env dosyanÄ±zda OPENAI_API_KEY deÄŸiÅŸkenini ayarlayarak gerÃ§ek LLM kullanabilirsiniz")