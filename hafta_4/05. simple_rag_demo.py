"""
Basit RAG (Retrieval-Augmented Generation) Demo
==============================================

Bu dosya, RAG (Retrieval-Augmented Generation) sisteminin temel adÄ±mlarÄ±nÄ± basit bir Ã¶rnekle gÃ¶sterir.
Her adÄ±m ve fonksiyon iÃ§in TÃ¼rkÃ§e aÃ§Ä±klamalar eklenmiÅŸtir.

RAG SÃ¼reci:
1. ğŸ“š 3 kÄ±sa belgeyi Chroma Vector DB'ye ekle
2. ğŸ” Sorgu ile Vector DB'de arama yap
3. ğŸ“„ En yakÄ±n belgeyi bul ve baÄŸlam olarak kullan
4. ğŸ¤– LLM (Dil Modeli) ile yanÄ±t Ã¼ret

RAG Pipeline:
Query â†’ Vector DB Search â†’ Retrieve â†’ Augment â†’ Generate
"""


# Gerekli kÃ¼tÃ¼phaneler
import numpy as np  # SayÄ±sal iÅŸlemler iÃ§in
from sentence_transformers import SentenceTransformer  # Metinleri vektÃ¶re dÃ¶nÃ¼ÅŸtÃ¼rmek iÃ§in
import chromadb  # VektÃ¶r veritabanÄ±
import os  # Ortam deÄŸiÅŸkenleri ve dosya iÅŸlemleri
from dotenv import load_dotenv  # .env dosyasÄ±ndan anahtarlarÄ± yÃ¼klemek iÃ§in


# .env dosyasÄ±nÄ± yÃ¼kle (API anahtarlarÄ± vs.)
load_dotenv()


# OpenAI kÃ¼tÃ¼phanesini yÃ¼klemeye Ã§alÄ±ÅŸ (LLM iÃ§in)
try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False


print("ğŸ¤– Basit RAG Demo - 4 AdÄ±mda RAG")
print("="*40)


# ADIM 1: ğŸ“š Chroma Vector DB Kurulumu ve Belge YÃ¼kleme
print("\nğŸ“š ADIM 1: Vector Database Setup")
print("-" * 30)


# ChromaDB istemcisi oluÅŸturuluyor
client = chromadb.Client()

# Koleksiyon (collection) oluÅŸturuluyor. EÄŸer Ã¶nceden varsa silinir.
collection_name = "simple_rag_demo"
try:
    client.delete_collection(collection_name)  # Ã–nceki koleksiyonu sil
except:
    pass

# Yeni koleksiyon oluÅŸturuluyor (vektÃ¶rler cosine benzerliÄŸi ile karÅŸÄ±laÅŸtÄ±rÄ±lacak)
collection = client.create_collection(
    name=collection_name,
    metadata={"hnsw:space": "cosine"}
)


# 3 kÄ±sa belge Ã¶rneÄŸi (id, metin, kategori)
documents = [
    {
        "id": "doc_1",
        "text": "Python kolay Ã¶ÄŸrenilebilen, gÃ¼Ã§lÃ¼ bir programlama dilidir. Web uygulamalarÄ±, veri analizi ve yapay zeka projelerinde kullanÄ±lÄ±r.",
        "category": "programming"
    },
    {
        "id": "doc_2", 
        "text": "JavaScript web tarayÄ±cÄ±larÄ±nda Ã§alÄ±ÅŸan dinamik bir dildir. Frontend ve backend geliÅŸtirme iÃ§in Node.js ile birlikte kullanÄ±labilir.",
        "category": "programming"
    },
    {
        "id": "doc_3",
        "text": "Machine learning algoritmalarÄ± verilerden pattern Ã¶ÄŸrenir. Supervised learning etiketli verilerle, unsupervised learning etiketsiz verilerle Ã§alÄ±ÅŸÄ±r.",
        "category": "ai"
    }
]


# Belgeler hazÄ±rlandÄ±, ekrana Ã¶zet bilgi yazdÄ±rÄ±lÄ±yor
print(f"âœ… {len(documents)} belge hazÄ±rlandÄ±:")
for i, doc in enumerate(documents, 1):
    print(f"{i}. [{doc['category']}] {doc['text'][:50]}...")


# ADIM 2: ğŸ—„ï¸ Belgeleri Vector DB'ye YÃ¼kleme
print("\nğŸ—„ï¸ ADIM 2: Vector DB'ye YÃ¼kleme")
print("-" * 30)


# Embedding (vektÃ¶rleÅŸtirme) modeli yÃ¼kleniyor
# Chroma kendi embedding yapabiliyor, burada kontrol amaÃ§lÄ± yÃ¼kleniyor
model = SentenceTransformer('all-MiniLM-L6-v2')
print(f"âœ… Embedding model: {model.get_sentence_embedding_dimension()}D")


# Belgeleri ChromaDB'ye eklemek iÃ§in metin, id ve metadata listeleri hazÄ±rlanÄ±yor
texts = [doc["text"] for doc in documents]
ids = [doc["id"] for doc in documents] 
metadatas = [{"category": doc["category"]} for doc in documents]

# Belgeler veritabanÄ±na ekleniyor
collection.add(
    documents=texts,
    metadatas=metadatas,
    ids=ids
)


print(f"âœ… {collection.count()} belge Vector DB'de saklandÄ±")
print(f"ğŸ“Š KullanÄ±lan similarity: cosine")


# ADIM 3: ğŸ” Vector DB'de Arama
print("\nğŸ” ADIM 3: Vector DB'de Arama")
print("-" * 30)


# Sorgu ile en yakÄ±n belgeyi bulmak iÃ§in fonksiyon
def search_vector_db(query, top_k=1):
    """
    Vector DB'de arama yapar.
    - query: KullanÄ±cÄ±nÄ±n sorgusu
    - top_k: KaÃ§ en yakÄ±n belge dÃ¶necek (default: 1)
    """
    print(f"ğŸ”¤ Sorgu: '{query}'")
    
    # ChromaDB'de arama yapÄ±lÄ±yor
    results = collection.query(
        query_texts=[query],
        n_results=top_k
    )
    
    print(f"ğŸ“Š Vector DB aramasÄ± tamamlandÄ±")
    print(f"ğŸ¯ Benzerlik skorlarÄ±: {[f'{d:.3f}' for d in results['distances'][0]]}")
    
    # En iyi sonucu al
    best_doc_id = results['ids'][0][0]
    best_score = results['distances'][0][0]
    best_document = results['documents'][0][0]
    best_metadata = results['metadatas'][0][0]
    
    print(f"ğŸ† En yakÄ±n belge: {best_doc_id} (Skor: {best_score:.3f})")
    print(f"ğŸ“‚ Kategori: {best_metadata['category']}")
    print(f"ğŸ“„ Ä°Ã§erik: {best_document[:100]}...")
    
    return {
        'id': best_doc_id,
        'text': best_document,
        'score': best_score,
        'metadata': best_metadata
    }


# OpenAI ile yanÄ±t Ã¼retmek iÃ§in fonksiyon
def answer_with_openai(prompt):
    """
    OpenAI GPT ile yanÄ±t Ã¼retir.
    - prompt: LLM'e gÃ¶nderilecek metin
    """
    if not OPENAI_AVAILABLE:
        return "âŒ OpenAI kÃ¼tÃ¼phanesi yÃ¼klÃ¼ deÄŸil"
    
    api_key = os.getenv('OPENAI_API_KEY')
    if not api_key:
        return "âŒ OPENAI_API_KEY environment variable bulunamadÄ±"
    
    try:
        client = openai.OpenAI(api_key=api_key)
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "Sen yardÄ±mcÄ± bir AI asistanÄ±sÄ±n. Verilen baÄŸlam bilgisini kullanarak sorularÄ± yanÄ±tla."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=200,
            temperature=0.3
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"âŒ OpenAI API hatasÄ±: {str(e)}"


# Test sorgusu (Ã¶rnek bir arama)
query = "Python hakkÄ±nda bilgi istiyorum"
retrieved_doc = search_vector_db(query)


# ADIM 4: ğŸ¤– RAG Pipeline - Prompt + LLM
print("\nğŸ¤– ADIM 4: RAG Pipeline - Prompt + LLM")
print("-" * 30)


# RAG iÃ§in prompt (LLM'e gÃ¶nderilecek metin) oluÅŸturma fonksiyonu
def create_rag_prompt(query, context):
    """
    RAG prompt oluÅŸturur.
    - query: KullanÄ±cÄ± sorusu
    - context: Vector DB'den dÃ¶nen en yakÄ±n belge
    """
    prompt = f"""AÅŸaÄŸÄ±daki baÄŸlam bilgisini kullanarak soruyu yanÄ±tla:

BAÄLAM: {context['text']}

SORU: {query}

YANIT:"""
    return prompt


# TÃ¼m RAG sÃ¼recini birleÅŸtiren fonksiyon
def rag_pipeline(query, use_openai=False):
    """
    Tam RAG pipeline'Ä± Ã§alÄ±ÅŸtÄ±rÄ±r.
    - query: KullanÄ±cÄ± sorgusu
    - use_openai: GerÃ§ek LLM mi kullanÄ±lacak (True) yoksa mock mu (False)
    """
    print(f"ğŸš€ RAG Pipeline baÅŸlatÄ±lÄ±yor...")
    
    # 1. Vector DB'de arama
    context = search_vector_db(query)
    

    # 2. Prompt oluÅŸtur
    prompt = create_rag_prompt(query, context)
    print(f"\nğŸ“ RAG Prompt oluÅŸturuldu ({len(prompt)} karakter)")
    
    # 3. LLM ile yanÄ±t al
    if use_openai:
        print(f"ğŸ¤– OpenAI GPT ile yanÄ±t alÄ±nÄ±yor...")
        response = answer_with_openai(prompt)
    else:
        print(f"ğŸ­ Mock yanÄ±t oluÅŸturuluyor...")
        response = f"""âœ… RAG Pipeline Demo TamamlandÄ±!

ğŸ” Retrieval Sonucu:
- Belge: {context['id']} ({context['metadata']['category']})
- Similarity Score: {context['score']:.3f}

ğŸ“ BaÄŸlam: "{context['text'][:100]}..."

ğŸ¤– GerÃ§ek LLM iÃ§in .env dosyasÄ±nda OPENAI_API_KEY ayarlayÄ±n.

Bu demo Vector DB + Context + LLM akÄ±ÅŸÄ±nÄ± gÃ¶sterir."""
    
    return {
        'query': query,
        'context': context,
        'prompt': prompt,
        'response': response
    }


# RAG pipeline'Ä± test etmek iÃ§in ana akÄ±ÅŸ
print(f"\n" + "="*50)
print(f"ğŸ¯ RAG PÄ°PELÄ°NE TEST")
print(f"="*50)

# OpenAI API anahtarÄ± varsa gerÃ§ek LLM kullanÄ±lÄ±r, yoksa mock yanÄ±t dÃ¶ner
api_key = os.getenv('OPENAI_API_KEY')
use_real_llm = api_key is not None and OPENAI_AVAILABLE

result = rag_pipeline(query, use_openai=use_real_llm)

print(f"\nğŸ­ RAG SONUCU:")
print("-" * 30)
print(result['response'])


# BONUS: FarklÄ± sorgularla Vector DB testleri
print(f"\nğŸ§ª BONUS: FarklÄ± Sorgular ile Vector DB Test")
print("-" * 40)

test_queries = [
    "JavaScript nedir?",
    "Machine learning nasÄ±l Ã§alÄ±ÅŸÄ±r?", 
    "Web geliÅŸtirme iÃ§in hangi dil?",
    "Yapay zeka algoritmalarÄ±"
]

for i, test_query in enumerate(test_queries, 1):
    print(f"\nğŸ” Test {i}: {test_query}")
    context = search_vector_db(test_query)
    print(f"   â¡ï¸ {context['id']} seÃ§ildi (Skor: {context['score']:.3f})")


# RAG sistemi Ã¶zet bilgisi
print(f"\nğŸ“‹ MODERN RAG SÄ°STEMÄ° Ã–ZETÄ°")
print("="*50)

summary = f"""
ğŸ”„ RAG PIPELINE:
1. ğŸ“š Documents â†’ Vector Database (Chroma)
2. ğŸ” Query â†’ Vector Search (Similarity)
3. ğŸ¯ Retrieve (En yakÄ±n belgeler)
4. âœï¸ Augment (Prompt + Context)
5. ğŸ¤– Generate (LLM Response)

ğŸ’¡ KULLANILAN TEKNOLOJÄ°LER:
â€¢ Vector DB: ChromaDB (cosine similarity)
â€¢ Embeddings: Sentence Transformers
â€¢ LLM: OpenAI GPT-3.5-turbo
â€¢ Framework: Python + dotenv

ğŸš€ PRODUCTION HAZIR:
â€¢ âœ… Vector Database entegrasyonu
â€¢ âœ… API key yÃ¶netimi (.env)
â€¢ âœ… Error handling
â€¢ âœ… Metadata desteÄŸi

ğŸ“Š DEMO Ä°STATÄ°STÄ°KLER:
â€¢ Toplam belge: {collection.count()}
â€¢ Vector DB: ChromaDB
â€¢ Embedding boyutu: {model.get_sentence_embedding_dimension()}D
â€¢ LLM: {"OpenAI GPT" if use_real_llm else "Mock Response"}
"""

print(summary)

print("âœ… Modern RAG demo tamamlandÄ±!")
print("ğŸ”— Daha kapsamlÄ± version iÃ§in: rag_system.py")