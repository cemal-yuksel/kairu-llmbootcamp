# Hafta 1: LLM Temelleri ve NLP DÃ¼nyasÄ±na Pratik Bir GiriÅŸ

![Durum](https://img.shields.io/badge/Durum-Tamamland%C4%B1-brightgreen)
![Odak](https://img.shields.io/badge/Odak-Temel%20Kavramlar%20&%20Model%20Farkl%C4%B1l%C4%B1klar%C4%B1-blue)
![Teknoloji](https://img.shields.io/badge/Teknoloji-Hugging%20Face%20Transformers-blueviolet)

## ğŸ¯ HaftanÄ±n Ã–zeti

Bu modÃ¼l, BÃ¼yÃ¼k Dil Modelleri (LLM) evrenine attÄ±ÄŸÄ±mÄ±z ilk adÄ±mdÄ±r. AmacÄ±mÄ±z, en temelden baÅŸlayarak Python ile metin iÅŸlemenin inceliklerini Ã¶ÄŸrenmek ve `Hugging Face Transformers` kÃ¼tÃ¼phanesi aracÄ±lÄ±ÄŸÄ±yla ilk Ã¼retken yapay zeka modellerimizi Ã§alÄ±ÅŸtÄ±rmaktÄ±r.

> Bu haftanÄ±n en kritik kazanÄ±mÄ± ÅŸudur: **Her model aynÄ± amaÃ§ iÃ§in yaratÄ±lmamÄ±ÅŸtÄ±r.** Basit bir konuÅŸma modeli (Conversational AI) ile modern bir talimat-takip eden model (Instruction-tuned) arasÄ±ndaki temel farklarÄ± bizzat deneyimleyerek, LLM'lerin gerÃ§ek dÃ¼nyadaki potansiyelini ve sÄ±nÄ±rlarÄ±nÄ± anlayacaÄŸÄ±z.

---

## ğŸ“ Ä°Ã§indekiler
* [ğŸ“‚ Kodlar ve Uygulamalar](#-kodlar-ve-uygulamalar)
* [ğŸ”¬ Ä°ki Modelin Hikayesi: DialoGPT vs. Qwen2](#-Ä°ki-modelin-hikayesi-karÅŸÄ±laÅŸtÄ±rmalÄ±-analiz-dialogpt-vs-qwen2)
* [âš™ï¸ Kurulum ve Ã‡alÄ±ÅŸtÄ±rma](#ï¸-kurulum-ve-Ã§alÄ±ÅŸtÄ±rma)
* [ğŸ¯ HaftanÄ±n KazanÄ±mlarÄ±](#-haftanÄ±n-kazanÄ±mlarÄ±)
* [âš ï¸ Ã–nemli Notlar ve UyarÄ±lar](#ï¸-Ã¶nemli-notlar-ve-uyarÄ±lar)
* [ğŸš€ Sonraki AdÄ±m: Hafta 2](#-sonraki-adÄ±m-hafta-2)

---

## ğŸ“‚ Kodlar ve Uygulamalar

Bu hafta geliÅŸtirilen komut dosyalarÄ± ve temel amaÃ§larÄ± aÅŸaÄŸÄ±daki tabloda Ã¶zetlenmiÅŸtir.

| Dosya AdÄ±           | AÃ§Ä±klama                                                                                                     | Temel Ã–ÄŸrenim                                                                           |
| ------------------- | ------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------- |
| `turkish_simple.py` | Temel TÃ¼rkÃ§e metin iÅŸleme adÄ±mlarÄ±nÄ± (temizleme, tokenizasyon) iÃ§eren bir baÅŸlangÄ±Ã§ script'i.                  | Python ile metin verisinin nasÄ±l manipÃ¼le edildiÄŸini anlamak.                           |
| `microsoft.py`      | Microsoft'un **DialoGPT** modelini kullanarak bir konuÅŸma botu oluÅŸturma denemesi.                             | Eski nesil, sadece sohbete odaklÄ± bir modelin davranÄ±ÅŸÄ±nÄ± ve sÄ±nÄ±rlÄ±lÄ±klarÄ±nÄ± gÃ¶zlemlemek. |
| `qwen.py`           | **Qwen2** gibi modern, talimat-takip eden (instruction-tuned) bir modelle metin Ã¼retme.                        | Ãœretim seviyesi (production-ready) modellerin tutarlÄ±lÄ±ÄŸÄ±nÄ± ve gÃ¼cÃ¼nÃ¼ gÃ¶rmek.          |

---

## ğŸ”¬ Ä°ki Modelin Hikayesi: KarÅŸÄ±laÅŸtÄ±rmalÄ± Analiz: DialoGPT vs. Qwen2

Bu haftanÄ±n en aydÄ±nlatÄ±cÄ± kÄ±smÄ±, bu iki modelin Ã§Ä±ktÄ±larÄ±ndaki gece ve gÃ¼ndÃ¼z kadar belirgin farkÄ± gÃ¶rmektir.

### ğŸ¤– Microsoft DialoGPT: Bir KonuÅŸma Modelinin SÄ±nÄ±rlarÄ±

DialoGPT, Reddit gibi platformlardaki gÃ¼ndelik konuÅŸma verileriyle eÄŸitilmiÅŸtir. AmacÄ± bilgi vermek deÄŸil, sohbeti sÃ¼rdÃ¼rmektir. Bu yÃ¼zden teknik veya spesifik sorulara verdiÄŸi yanÄ±tlar genellikle tutarsÄ±z, alakasÄ±z veya ÅŸaÅŸÄ±rtÄ±cÄ±dÄ±r.

### ğŸ§  DialoGPT DavranÄ±ÅŸ Analizi: Flashcard'lar

AÅŸaÄŸÄ±da, DialoGPT'nin davranÄ±ÅŸÄ±nÄ± daha iyi anlamak iÃ§in flashcard formatÄ±nda hazÄ±rlanmÄ±ÅŸ bir analiz bulunmaktadÄ±r. Bu kartlar, modelin neden beklentilerin dÄ±ÅŸÄ±nda cevaplar verdiÄŸini somutlaÅŸtÄ±rmaktadÄ±r.

| ğŸƒ Sorulan Soru                                                           | ğŸ’¡ AlÄ±nan YanÄ±t                                                                                                                                                                                      |
| :-------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Soru:**<br>> `Language models are AI systems that...`                                  | **Beklenti vs. GerÃ§eklik**<br><br>**âœ”ï¸ Beklenen (MantÄ±ksal):**<br>> `...process and understand human language.`<br><br>**âŒ GerÃ§ek (DialoGPT):**<br>> `But what does it say? Human: The model says human.` <br>---<br> **Analiz:**<br>Model, bilgi vermek yerine sohbeti devam ettirmeye odaklanmÄ±ÅŸtÄ±r. CevabÄ±, bir tanÄ±mÄ± tamamlamak yerine yeni bir diyalog baÅŸlatma giriÅŸimidir. |
| **Soru:**<br>> `AI is the simulation of human intelligence...`                           | **Beklenti vs. GerÃ§eklik**<br><br>**âœ”ï¸ Beklenen (MantÄ±ksal):**<br>> `...in machines.`<br><br>**âŒ GerÃ§ek (DialoGPT):**<br>> `I'm not human.` <br>---<br> **Analiz:**<br>Model, bir rol Ã¼stlenerek konuyu saptÄ±rmaktadÄ±r. Kendisini konuÅŸmanÄ±n bir parÃ§asÄ± olarak konumlandÄ±rÄ±r ve kiÅŸisel bir cevap vererek sohbete yÃ¶nelik doÄŸasÄ±nÄ± sergiler. |  

### âœ¨ Qwen2: Modern Bir Instruction Modelinin GÃ¼cÃ¼

Qwen2 ise, belirli komutlarÄ± takip etmek ve sorulara doÄŸru cevaplar vermek Ã¼zere Ã¶zel olarak "ayarlanmÄ±ÅŸ" (fine-tuned) bir modeldir. Bu tÃ¼r modeller, ChatGPT gibi modern sistemlerin temelini oluÅŸturur.

**Teknik Detay: Sadece Bot CevabÄ±nÄ± AyÄ±klama**

`Hugging Face pipeline`'Ä±, verdiÄŸiniz `prompt` ile birlikte modelin Ã¼rettiÄŸi cevabÄ± tek bir metin olarak dÃ¶ndÃ¼rÃ¼r. Biz sadece modelin yeni Ã¼rettiÄŸi kÄ±smÄ± almak istiyoruz.

```python
# Problem: Pipeline, girdi metnini ve Ã§Ä±ktÄ±yÄ± birleÅŸtirerek dÃ¶ndÃ¼rÃ¼r.
# "Soru: Yapay zeka nedir? Cevap: Yapay zeka, makinelerin..."
generated_text = response[0]["generated_text"]

# Ã‡Ã¶zÃ¼m: Girdi metninin uzunluÄŸunu kullanarak sadece yeni Ã¼retilen kÄ±smÄ± alÄ±yoruz.
# BÃ¶ylece sadece "Yapay zeka, makinelerin..." kÄ±smÄ±nÄ± elde ederiz.
bot_response = generated_text[len(prompt):].strip()
```

Bu basit ama etkili Ã§Ã¶zÃ¼m, metin Ã¼retme gÃ¶revlerinde sÄ±kÃ§a kullanÄ±lan bir tekniktir.

---

## âš™ï¸ Kurulum ve Ã‡alÄ±ÅŸtÄ±rma

KodlarÄ± Ã§alÄ±ÅŸtÄ±rmadan Ã¶nce aÅŸaÄŸÄ±daki adÄ±mlarÄ± izleyin.

**1. Sanal OrtamÄ± Aktif Hale Getirin**

Proje klasÃ¶rÃ¼nde terminali aÃ§Ä±n ve iÅŸletim sisteminize uygun komutu Ã§alÄ±ÅŸtÄ±rÄ±n:

```bash
# macOS / Linux iÃ§in
source llm_1/bin/activate

# Windows iÃ§in
llm_1\Scripts\activate
```

**2. Hugging Face API Token'Ä±nÄ±zÄ± AyarlayÄ±n**

Bu klasÃ¶rde `.env` adÄ±nda bir dosya oluÅŸturun ve iÃ§ine Hugging Face Hub'dan aldÄ±ÄŸÄ±nÄ±z API anahtarÄ±nÄ± ekleyin:

```
HF_TOKEN=hf_XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
```

**3. Komut DosyalarÄ±nÄ± Ã‡alÄ±ÅŸtÄ±rÄ±n**

Her bir script'i aÅŸaÄŸÄ±daki komutlarla ayrÄ± ayrÄ± Ã§alÄ±ÅŸtÄ±rabilirsiniz:

```bash
python turkish_simple.py
python microsoft.py
python qwen.py
```

---

## ğŸ¯ HaftanÄ±n KazanÄ±mlarÄ±

Bu modÃ¼lÃ¼n sonunda aÅŸaÄŸÄ±daki teorik ve pratik yetkinlikleri kazandÄ±k:

#### Teknik Beceriler
âœ… `Hugging Face Transformers` kÃ¼tÃ¼phanesini kullanarak model yÃ¼kleme ve `pipeline` oluÅŸturma.

âœ… `text-generation` gÃ¶revini farklÄ± modellerle deneyimleme.

âœ… `.env` dosyasÄ± ile API anahtarlarÄ±nÄ± gÃ¼venli bir ÅŸekilde yÃ¶netme.

#### Yapay Zeka ve Makine Ã–ÄŸrenmesi KavramlarÄ±
âœ… **Model TÃ¼rleri:** KonuÅŸma odaklÄ± (Conversational) ve Talimat-takip eden (Instruction-tuned) modeller arasÄ±ndaki temel farklarÄ± anlama.

âœ… **Verinin Etkisi:** Bir modelin eÄŸitildiÄŸi verinin, onun davranÄ±ÅŸÄ±nÄ± ve yeteneklerini nasÄ±l kÃ¶kten etkilediÄŸini gÃ¶rme.

âœ… **GerÃ§ekÃ§i Beklentiler:** Yapay zekanÄ±n "sihirli" olmadÄ±ÄŸÄ±nÄ±, her modelin belirli bir amaÃ§ ve sÄ±nÄ±rlÄ±lÄ±kla tasarlandÄ±ÄŸÄ±nÄ± kavrama.

âœ… **Prompt MÃ¼hendisliÄŸi:** Bir modele doÄŸru gÃ¶revi yaptÄ±rmak iÃ§in girdi metninin ne kadar Ã¶nemli olduÄŸuna dair ilk izlenimleri edinme.


---

## âš ï¸ Ã–nemli Notlar ve UyarÄ±lar

* **DialoGPT'nin DoÄŸasÄ±:** LÃ¼tfen DialoGPT'nin verdiÄŸi "saÃ§ma" cevaplarÄ±n moralinizi bozmasÄ±na izin vermeyin. Bu, bir modelin sÄ±nÄ±rlarÄ±nÄ± anlamak iÃ§in paha biÃ§ilmez bir derstir. GerÃ§ek dÃ¼nya uygulamalarÄ±nda bu tÃ¼r modeller yerine Qwen2 gibi daha stabil ve Ã¶ngÃ¶rÃ¼lebilir sistemler kullanÄ±lÄ±r.
* **DonanÄ±m ve Performans:** BÃ¼yÃ¼k modeller, Ã¶zellikle GPU olmadan Ã§alÄ±ÅŸtÄ±rÄ±ldÄ±ÄŸÄ±nda yavaÅŸ olabilir. Modellerin ilk indirilme sÃ¼resi internet baÄŸlantÄ±nÄ±za baÄŸlÄ± olarak zaman alabilir.
* **Token SÄ±nÄ±rlarÄ±:** Hugging Face gibi platformlarÄ±n Ã¼cretsiz kullanÄ±m iÃ§in belirli API istek limitleri vardÄ±r. Ã‡ok sÄ±k istek gÃ¶ndermekten kaÃ§Ä±nÄ±n.

---

## ğŸš€ Sonraki AdÄ±m: Hafta 2

Bu hafta, LLM'lerin ham potansiyelini ve sÄ±nÄ±rlarÄ±nÄ± gÃ¶rdÃ¼k. DialoGPT'nin neden "yetersiz" kaldÄ±ÄŸÄ±nÄ± anladÄ±k. Bu temel, **Hafta 2**'de Ã¼zerine inÅŸa edeceÄŸimiz yapÄ±nÄ±n temelini oluÅŸturuyor:

* **OpenAI API** ile endÃ¼stri standardÄ± modellere eriÅŸim.
* GeliÅŸmiÅŸ **Prompt Engineering** teknikleri.
* **Function Calling** ile LLM'lere harici araÃ§larÄ± kullanma yeteneÄŸi kazandÄ±rma.

Bu haftaki deneyim, gelecek haftalarda daha geliÅŸmiÅŸ ve gÃ¼Ã§lÃ¼ Ã§Ã¶zÃ¼mleri neden ve nasÄ±l kullandÄ±ÄŸÄ±mÄ±zÄ± anlamamÄ±zÄ± saÄŸlayacak.
