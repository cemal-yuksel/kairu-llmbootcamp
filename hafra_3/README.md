# ğŸ§  **Hafta 3: AutoTokenizer, AutoModel ve Pipeline Optimizasyonu**
![Durum](https://img.shields.io/badge/Durum-TamamlandÄ±-brightgreen)
![Odak](https://img.shields.io/badge/Odak-Transformers%20Temelleri%20%26%20Optimizasyon-blue)
![Teknoloji](https://img.shields.io/badge/Teknoloji-Hugging%20Face%20Transformers%20%7C%20PyTorch-blueviolet)

---

## ğŸ¯ HaftanÄ±n Ã–zeti
Bu hafta, Hugging Face Transformers kÃ¼tÃ¼phanesinin Ã§ekirdek bileÅŸenlerini derinlemesine test ederek, **AutoTokenizer**, **AutoModel** ve **Pipeline** yapÄ±larÄ± Ã¼zerinde tam hakimiyet kazandÄ±m.  
Performans Ã¶lÃ§Ã¼mleri, CPU/GPU optimizasyonlarÄ±, quantization ve batch processing teknikleri sayesinde yalnÄ±zca prototip Ã¼retmekle kalmayÄ±p, **akademik dÃ¼zeyde benchmark raporlarÄ±** oluÅŸturabilen gÃ¼Ã§lÃ¼ bir altyapÄ± edindim.  

---

## ğŸ“š Ä°Ã§erik

### 1. AutoTokenizer & AutoModel YapÄ±sÄ± + Pipeline ile HÄ±zlÄ± Model Ã‡aÄŸÄ±rma  
**Dosya:** `01_autotokenizer_automodel.py`  
- Tokenizer ile encode/decode sÃ¼reÃ§leri  
- AutoModel ile manuel model Ã§aÄŸÄ±rma  
- Pipeline abstraction: hÄ±z ve sadelik  
- Manual vs Pipeline kÄ±yaslamalarÄ±  
- Ã–zelleÅŸtirilmiÅŸ pipeline Ã¶rnekleri  

---

### 2. GPT, BERT ve T5 Modellerinin FarklarÄ± ve Pipeline Entegrasyonu  
**Dosya:** `02_gpt_bert_t5_comparison.py`  
- Model mimarilerinin karÅŸÄ±laÅŸtÄ±rmasÄ±  
- Her bir mimarinin gÃ¼Ã§lÃ¼ yÃ¶nleri ve kullanÄ±m baÄŸlamlarÄ±  
- Pipeline entegrasyonu ile tek satÄ±rlÄ±k testler  
- Parametre sayÄ±sÄ±, hÄ±z ve VRAM tÃ¼ketimi analizleri  

| Model | Mimari | GÃ¼Ã§lÃ¼ YÃ¶nler | KullanÄ±m AlanlarÄ± |
|-------|--------|--------------|-------------------|
| **GPT** | Decoder-only | Uzun metin Ã¼retimi | Creative writing, Conversational AI |
| **BERT** | Encoder-only | Bidirectional anlama | Classification, NER, QA |
| **T5** | Encoder-decoder | Text-to-text yaklaÅŸÄ±mÄ± | Translation, Summarization |

---

### 3. CPU/GPU Performans YÃ¶netimi ve Model Optimizasyonu  
**Dosya:** `03_cpu_gpu_optimization.py`  
- Optimal cihaz seÃ§imi (cuda/mps/cpu)  
- CPU thread tuning  
- GPU bellek yÃ¶netimi (amp, autocast)  
- Model quantization (8-bit, dynamic)  
- Batch processing optimizasyonu  
- Bellek dostu inference yÃ¶ntemleri  

**Kritik Teknikler:**  
- `torch.no_grad()` ile gereksiz gradient hesaplamalarÄ±nÄ± engelleme  
- `torch.cuda.empty_cache()` ve `gc.collect()` ile bellek temizliÄŸi  
- `BitsAndBytesConfig` ile quantization  
- Dynamic padding ile hÄ±zlanma  

---

### 4. Pipeline ile GPU/CPU PerformansÄ±nÄ± Ã–lÃ§me ve KÄ±yaslama  
**Dosya:** `04_performance_measurement.py`  
- PerformanceMeter sÄ±nÄ±fÄ± ile Ã¶lÃ§Ã¼mler  
- FarklÄ± taskâ€™larda benchmark testleri  
- Batch size varyasyonlarÄ±nÄ±n etkisi  
- DetaylÄ± raporlar ve gÃ¶rselleÅŸtirme  

**Metrikler:**  
- Inference sÃ¼resi  
- Memory kullanÄ±mÄ± (CPU/GPU)  
- Throughput (texts/sec)  
- Device utilization  
- Model yÃ¼kleme sÃ¼resi  

---

### ğŸ“ Manuel Kurulum

#### 1. Sanal Ortam OluÅŸtur
```bash
# macOS/Linux
python3 -m venv llm_bootcamp_env
source llm_bootcamp_env/bin/activate

# Windows
python -m venv llm_bootcamp_env
llm_bootcamp_env\Scripts\activate.bat
```

#### 2. BaÄŸÄ±mlÄ±lÄ±klarÄ± YÃ¼kle
```bash
pip install --upgrade pip
pip install -r requirements.txt
```

#### 3. ModÃ¼lleri Ã‡alÄ±ÅŸtÄ±r
```bash
# AutoTokenizer ve AutoModel Ã¶rnekleri
python 01_autotokenizer_automodel.py

# Model karÅŸÄ±laÅŸtÄ±rmasÄ±
python 02_gpt_bert_t5_comparison.py

# Performans optimizasyonu
python 03_cpu_gpu_optimization.py

# Performans Ã¶lÃ§Ã¼mÃ¼
python 04_performance_measurement.py
```

## ğŸ“‹ Gereksinimler

```bash
pip install transformers torch torchvision torchaudio
pip install psutil matplotlib numpy
pip install bitsandbytes  # Quantization iÃ§in (opsiyonel)
```

**GPU DesteÄŸi iÃ§in:**
- CUDA: `pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118`
- Apple Silicon: Otomatik olarak MPS desteÄŸi

---

 ## ğŸ’¡ En Ä°yi Uygulamalar

### Performans Optimizasyonu
```python
# âœ… Ä°yi
with torch.no_grad():
    outputs = model(**inputs)

# âŒ KÃ¶tÃ¼  
outputs = model(**inputs)  # Gradient hesaplanÄ±r
```

### Device YÃ¶netimi
```python
# âœ… Ä°yi
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)
inputs = {k: v.to(device) for k, v in inputs.items()}

# âŒ KÃ¶tÃ¼
model = model.to("cuda")  # CUDA olmayabilir
```

### Memory YÃ¶netimi
```python
# âœ… Ä°yi
del model
torch.cuda.empty_cache()
gc.collect()

# âŒ KÃ¶tÃ¼
# Memory leak'e sebep olabilir
```

## ğŸ“Š Benchmark SonuÃ§larÄ±

Tipik performans karÅŸÄ±laÅŸtÄ±rmasÄ± (Ã¶rnek sistem):

| Model | Device | Inference Time | Memory Usage |
|-------|--------|----------------|--------------|
| DistilBERT | CPU | 0.045s | 1.2 GB |
| DistilBERT | GPU | 0.012s | 2.1 GB |
| BERT-base | CPU | 0.089s | 2.1 GB |
| BERT-base | GPU | 0.021s | 3.2 GB |
---
## ğŸ“š Ek Kaynaklar

> Akademik altyapÄ±mÄ± gÃ¼Ã§lendirmek iÃ§in haftanÄ±n sonunda baÅŸvurduÄŸum **Ã¶nemli referanslar**:  

<details>
<summary>ğŸ“˜ Hugging Face Transformers Documentation</summary>
<a href="https://huggingface.co/docs/transformers/" target="_blank">https://huggingface.co/docs/transformers/</a>  
ğŸ” Modellerin, tokenizerâ€™larÄ±n ve pipeline mimarisinin resmi aÃ§Ä±klamalarÄ± ve ileri dÃ¼zey kullanÄ±m Ã¶rnekleri.  
</details>

<details>
<summary>âš¡ PyTorch Performance Tuning Guide</summary>
<a href="https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html" target="_blank">https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html</a>  
âš™ï¸ CPU/GPU hÄ±z optimizasyonlarÄ±, bellek yÃ¶netimi ve Ã¼retim ortamÄ± iÃ§in ileri seviye teknikler.  
</details>

<details>
<summary>ğŸ“‘ BERT Paper (Devlin et al., 2018)</summary>
<a href="https://arxiv.org/abs/1810.04805" target="_blank">https://arxiv.org/abs/1810.04805</a>  
ğŸ§© Bidirectional encoder mimarisinin metin anlama gÃ¶revlerinde devrim yaratan orijinal makalesi.  
</details>

<details>
<summary>ğŸ“ GPT Paper (Radford et al., 2018)</summary>
<a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf" target="_blank">https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf</a>  
âœ’ï¸ Unsupervised pretraining yaklaÅŸÄ±mÄ±nÄ±n ilk kez tanÄ±mlandÄ±ÄŸÄ±, generative transformer ailesinin baÅŸlangÄ±Ã§ noktasÄ±.  
</details>

<details>
<summary>ğŸ”„ T5 Paper (Raffel et al., 2019)</summary>
<a href="https://arxiv.org/abs/1910.10683" target="_blank">https://arxiv.org/abs/1910.10683</a>  
ğŸŒ â€œText-to-Text Transfer Transformerâ€ paradigmasÄ±nÄ± sunan, tÃ¼m NLP gÃ¶revlerini ortak bir formata indirgeyen Ã§alÄ±ÅŸma.  
</details>

