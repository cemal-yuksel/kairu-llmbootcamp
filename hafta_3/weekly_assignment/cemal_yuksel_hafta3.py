import torch
import time
import psutil
from transformers import pipeline, Pipeline
import matplotlib.pyplot as plt
from tabulate import tabulate
import pandas as pd
import logging
import os
import warnings

# CUDA kernel ve diÄŸer uyarÄ±larÄ± bastÄ±r
os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"] = ""
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"
os.environ["TRANSFORMERS_NO_ADVISORY_WARNINGS"] = "1"
os.environ["HF_HUB_DISABLE_PROGRESS_BARS"] = "1"
warnings.filterwarnings("ignore")
import logging
logging.getLogger("transformers").setLevel(logging.ERROR)
logging.getLogger("torch").setLevel(logging.ERROR)

# Log dosyasÄ± oluÅŸtur
logging.basicConfig(filename="model_performance.log", level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# Test metinleri (Ã§eÅŸitli uzunluk ve duyguda)
TEST_TEXTS = [
    "I love programming with Python. It's amazing for AI development and machine learning projects!",
    "The weather is terrible today and I feel so sad.",
    "Artificial intelligence is transforming the world in unprecedented ways.",
    "Kairu Bootcamp is a great opportunity to learn about LLMs and their real-world applications.",
    "Despite the challenges, the project was a huge success and everyone was happy.",
    "This is the worst movie I have ever seen. The plot was boring and the acting was terrible.",
    "Python is a versatile language used for web development, data science, and more.",
    "The food was delicious and the service was excellent.",
    "I am not sure if I can finish this assignment on time.",
    "The new policy will have a significant impact on the company's future."
]

# Model isimleri ve gÃ¶revleri
MODEL_CONFIGS = [
    {"name": "GPT-2", "pipeline": "text-generation", "model": "gpt2", "task": "text_generation"},
    {"name": "BERT", "pipeline": "sentiment-analysis", "model": "distilbert-base-uncased-finetuned-sst-2-english", "task": "sentiment_analysis"},
    {"name": "T5", "pipeline": "summarization", "model": "t5-small", "task": "summarization"}
]

# Ã‡Ä±ktÄ± kalitesi iÃ§in Ã¶rnek skor fonksiyonlarÄ±
def score_gpt2_output(output, input_text):
    # Basit Ã¶rnek: Ã‡Ä±ktÄ± input'tan anlamlÄ± ÅŸekilde farklÄ± mÄ± ve uzun mu?
    gen = output.get("generated_text", "")
    if len(gen) > len(input_text) + 10:
        return 1.0
    elif len(gen) > len(input_text):
        return 0.7
    else:
        return 0.3

def score_bert_output(output):
    # Pozitif/negatif ve gÃ¼ven skoruna gÃ¶re puan
    label = output.get("label", "")
    score = output.get("score", 0)
    if label in ["POSITIVE", "NEGATIVE"] and score > 0.9:
        return 1.0
    elif score > 0.7:
        return 0.7
    else:
        return 0.4

def score_t5_output(output, input_text):
    # Ã–zet gerÃ§ekten kÄ±salmÄ±ÅŸ mÄ± ve anlamlÄ± mÄ± (Ã¶rnek)
    summary = output.get("summary_text", "")
    if len(summary) < len(input_text) * 0.7:
        return 1.0
    elif len(summary) < len(input_text):
        return 0.7
    else:
        return 0.4

# Model yÃ¼kleme ve yÃ¼kleme sÃ¼resi Ã¶lÃ§Ã¼mÃ¼
def load_pipeline_safe(pipe_type, model_name):
    start = time.time()
    try:
        # GPU uyumsuzluÄŸunu Ã¶nlemek iÃ§in force_cpu parametresi
        pipe = pipeline(pipe_type, model=model_name, device=-1)
        elapsed = time.time() - start
        logging.info(f"{model_name} loaded in {elapsed:.2f} seconds.")
        return pipe, elapsed
    except Exception as e:
        logging.error(f"Error loading {model_name}: {e}")
        return None, None

# Model performansÄ±nÄ± Ã¶lÃ§en fonksiyon (process bazlÄ± memory)
def measure_performance(model_pipeline: Pipeline, text: str, task_name: str):
    """
    Modeli verilen gÃ¶revde Ã§alÄ±ÅŸtÄ±rÄ±r, sÃ¼re ve bellek kullanÄ±mÄ±nÄ± Ã¶lÃ§er.
    Ã‡Ä±ktÄ± ile birlikte performans metriklerini dÃ¶ndÃ¼rÃ¼r.
    """
    process = psutil.Process(os.getpid())
    start_time = time.time()
    start_memory = process.memory_info().rss / 1e9  # GB

    try:
        if task_name == "text_generation":
            result = model_pipeline(text, max_length=max(len(text.split()) + 10, 30), num_return_sequences=1, do_sample=False)
        elif task_name == "sentiment_analysis":
            result = model_pipeline(text)
        elif task_name == "summarization":
            # max_length input_length'ten kÃ¼Ã§Ã¼kse uyarÄ± Ã§Ä±kmasÄ±n diye ayarlanÄ±yor
            input_len = len(text.split())
            max_len = max(10, int(input_len * 0.7))
            min_len = min(10, max_len - 1) if max_len > 10 else 5
            result = model_pipeline(text, max_length=max_len, min_length=min_len, do_sample=False)
        else:
            result = model_pipeline(text)
    except Exception as e:
        result = [{"error": str(e)}]

    end_time = time.time()
    end_memory = process.memory_info().rss / 1e9

    return {
        'task': task_name,
        'time': round(end_time - start_time, 3),
        'memory': round(end_memory - start_memory, 2),
        'result': result
    }

# SonuÃ§larÄ± tablo olarak yazdÄ±r
def print_results_table(results_df):
    print("\n=== SonuÃ§lar Tablosu ===")
    print(tabulate(results_df, headers='keys', tablefmt='psql', showindex=False))

# SonuÃ§larÄ± CSV'ye kaydet
def save_results_csv(results_df, filename="model_performance_results.csv"):
    results_df.to_csv(filename, index=False)
    print(f"\nSonuÃ§lar CSV olarak kaydedildi: {filename}")

# Her modelin Ã§Ä±ktÄ±sÄ±nÄ± ve skorunu analiz et
def analyze_outputs(model_name, outputs, input_texts):
    scores = []
    for i, out in enumerate(outputs):
        if model_name == "GPT-2":
            score = score_gpt2_output(out, input_texts[i])
        elif model_name == "BERT":
            score = score_bert_output(out)
        elif model_name == "T5":
            score = score_t5_output(out, input_texts[i])
        else:
            score = 0.0
        scores.append(score)
    return scores

# GeliÅŸmiÅŸ gÃ¶rselleÅŸtirme
def plot_performance_advanced(results_df):
    plt.figure(figsize=(10,5))
    for model in results_df['Model'].unique():
        subset = results_df[results_df['Model']==model]
        plt.plot(subset['TextID'], subset['Time (s)'], marker='o', label=f"{model} Time")
    plt.xlabel("Text ID")
    plt.ylabel("Time (s)")
    plt.title("Model BazÄ±nda Ã‡alÄ±ÅŸma SÃ¼releri")
    plt.legend()
    plt.show()

    plt.figure(figsize=(10,5))
    for model in results_df['Model'].unique():
        subset = results_df[results_df['Model']==model]
        plt.plot(subset['TextID'], subset['Memory (GB)'], marker='x', label=f"{model} Memory")
    plt.xlabel("Text ID")
    plt.ylabel("Memory (GB)")
    plt.title("Model BazÄ±nda Bellek KullanÄ±mÄ±")
    plt.legend()
    plt.show()

    plt.figure(figsize=(10,5))
    for model in results_df['Model'].unique():
        subset = results_df[results_df['Model']==model]
        plt.plot(subset['TextID'], subset['Quality Score'], marker='s', label=f"{model} Quality")
    plt.xlabel("Text ID")
    plt.ylabel("Quality Score")
    plt.title("Model BazÄ±nda Ã‡Ä±ktÄ± Kalitesi")
    plt.legend()
    plt.show()

# Model Ã§Ä±ktÄ±larÄ±nÄ±n Ã¶rneklerini yazdÄ±r
def print_sample_outputs(model_name, outputs, input_texts):
    print(f"\n--- {model_name} Ã‡Ä±ktÄ± Ã–rnekleri ---")
    for i, out in enumerate(outputs[:2]):
        print(f"Text {i+1}: {input_texts[i][:60]}...")
        if model_name == "GPT-2":
            print("Generated:", out.get("generated_text", "")[:120])
        elif model_name == "BERT":
            print("Sentiment:", out.get("label", ""), "Confidence:", round(out.get("score", 0),2))
        elif model_name == "T5":
            print("Summary:", out.get("summary_text", "")[:120])
        print("-"*40)

# Genel analiz ve Ã¶neriler
def print_overall_analysis(results_df):
    print("\n=== Genel Analiz ve Ã–neriler ===")
    avg = results_df.groupby("Model").mean(numeric_only=True)
    print(tabulate(avg, headers='keys', tablefmt='github'))
    fastest = avg['Time (s)'].idxmin()
    efficient = avg['Memory (GB)'].idxmin()
    best_quality = avg['Quality Score'].idxmax()
    print(f"\nEn hÄ±zlÄ± model: {fastest}")
    print(f"En az bellek kullanan model: {efficient}")
    print(f"Ã‡Ä±ktÄ± kalitesi en yÃ¼ksek model: {best_quality}")
    print("\nÃ–neriler:")
    print("- HÄ±z ve verimlilik iÃ§in BERT Ã¶ne Ã§Ä±kÄ±yor.")
    print("- YaratÄ±cÄ± metin Ã¼retimi iÃ§in GPT-2, Ã¶zetleme iÃ§in T5 tercih edilmeli.")
    print("- Uzun metinlerde T5 Ã¶zetleri oldukÃ§a anlamlÄ± ve kÄ±sa.")
    print("- Sentiment analizi iÃ§in BERT Ã§ok hÄ±zlÄ± ve gÃ¼venilir.")

def main():
    # 1. Modelleri yÃ¼kle ve yÃ¼kleme sÃ¼relerini Ã¶lÃ§
    print("ğŸ“¦ Modeller yÃ¼kleniyor...")
    pipelines = {}
    load_times = {}
    for cfg in MODEL_CONFIGS:
        pipe, elapsed = load_pipeline_safe(cfg["pipeline"], cfg["model"])
        if pipe is None:
            print(f"{cfg['name']} yÃ¼klenemedi!")
            return
        pipelines[cfg["name"]] = pipe
        load_times[cfg["name"]] = elapsed
        print(f"{cfg['name']} yÃ¼klendi ({elapsed:.2f} sn)")

    print("\nâœ… TÃ¼m modeller baÅŸarÄ±yla yÃ¼klendi!\n")

    # 2. Her model ve her metin iÃ§in test yap
    results = []
    outputs_dict = {m["name"]: [] for m in MODEL_CONFIGS}
    for text_id, text in enumerate(TEST_TEXTS):
        print(f"\nğŸ§ª Test metni {text_id+1}/{len(TEST_TEXTS)}: {text[:60]}...")
        for cfg in MODEL_CONFIGS:
            model_name = cfg["name"]
            task = cfg["task"]
            pipe = pipelines[model_name]
            perf = measure_performance(pipe, text, task)
            # Ã‡Ä±ktÄ±dan ilk sonucu al
            out = perf['result'][0] if perf['result'] else {}
            outputs_dict[model_name].append(out)
            # Kalite skorunu hesapla
            if model_name == "GPT-2":
                quality = score_gpt2_output(out, text)
            elif model_name == "BERT":
                quality = score_bert_output(out)
            elif model_name == "T5":
                quality = score_t5_output(out, text)
            else:
                quality = 0.0
            # SonuÃ§larÄ± kaydet
            results.append({
                "TextID": text_id+1,
                "Model": model_name,
                "Task": task,
                "Time (s)": perf['time'],
                "Memory (GB)": perf['memory'],
                "Quality Score": quality,
                "Output": str(out)[:120]
            })

    # 3. SonuÃ§larÄ± DataFrame'e aktar
    results_df = pd.DataFrame(results)

    # 4. SonuÃ§larÄ± tablo olarak yazdÄ±r
    print_results_table(results_df)

    # 5. SonuÃ§larÄ± CSV'ye kaydet
    save_results_csv(results_df)

    # 6. Her model iÃ§in Ã§Ä±ktÄ± Ã¶rneklerini yazdÄ±r
    for cfg in MODEL_CONFIGS:
        print_sample_outputs(cfg["name"], outputs_dict[cfg["name"]], TEST_TEXTS)

    # 7. GeliÅŸmiÅŸ gÃ¶rselleÅŸtirme
    plot_performance_advanced(results_df)

    # 8. Genel analiz ve Ã¶neriler
    print_overall_analysis(results_df)

    print("\nâœ… TÃ¼m analizler tamamlandÄ±!")

if __name__ == "__main__":
    main()
