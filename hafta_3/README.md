# ğŸ§  **Hafta 3: AutoTokenizer, AutoModel ve Pipeline Optimizasyonu**
![Durum](https://img.shields.io/badge/Durum-TamamlandÄ±-brightgreen)
![Odak](https://img.shields.io/badge/Odak-Transformers%20Temelleri%20%26%20Optimizasyon-blue)
![Teknoloji](https://img.shields.io/badge/Teknoloji-Hugging%20Face%20Transformers%20%7C%20PyTorch-blueviolet)

---

## ğŸ“ Dosya YapÄ±sÄ±

Bu klasÃ¶rdeki ana dosyalar ve iÃ§erikleri:

| Dosya AdÄ±                        | AÃ§Ä±klama                                                                                   |
|-----------------------------------|-------------------------------------------------------------------------------------------|
| `01_autotokenizer_automodel.py`   | AutoTokenizer ve AutoModel ile temel encode/decode, manuel ve pipeline ile model Ã§aÄŸÄ±rma   |
| `02_gpt_bert_t5_comparison.py`    | GPT, BERT ve T5 mimarilerinin karÅŸÄ±laÅŸtÄ±rÄ±lmasÄ±, pipeline entegrasyonu ve analizler        |
| `03_cpu_gpu_optimization.py`      | CPU/GPU seÃ§imi, quantization, batch processing ve bellek dostu inference teknikleri        |
| `04_performance_measurement.py`   | Pipeline ile performans Ã¶lÃ§Ã¼mÃ¼, farklÄ± task ve batch size'larda benchmark ve raporlama     |
| `requirements.txt`                | Gerekli Python paketleri listesi                                                           |
| `README.md`                       | HaftanÄ±n Ã¶zeti, kullanÄ±m talimatlarÄ±, teknik aÃ§Ä±klamalar ve ek kaynaklar                   |

Her dosya, Hugging Face Transformers ve PyTorch ekosisteminde Ã¼retim kalitesinde NLP uygulamalarÄ± geliÅŸtirmek iÃ§in Ã¶rnekler ve optimizasyon teknikleri iÃ§erir.

---

## ğŸ¯ HaftanÄ±n Ã–zeti
Bu hafta, Hugging Face Transformers kÃ¼tÃ¼phanesinin temel bileÅŸenlerini kapsamlÄ± ve sistematik biÃ§imde analiz ederek, **AutoTokenizer**, **AutoModel** ve **Pipeline** yapÄ±larÄ±nÄ± ileri dÃ¼zeyde deÄŸerlendirdim.  
Ã‡alÄ±ÅŸmalarÄ±mda, performans Ã¶lÃ§Ã¼mleri, CPU/GPU optimizasyon stratejileri, quantization ve batch processing gibi modern teknikleri uygulayarak; yalnÄ±zca prototipleme deÄŸil, aynÄ± zamanda **Ã¼retim ve araÅŸtÄ±rma ortamlarÄ±nda kullanÄ±labilecek, akademik standartlarda benchmark raporlarÄ±** Ã¼retebilen, yÃ¼ksek verimli ve sÃ¼rdÃ¼rÃ¼lebilir bir NLP altyapÄ±sÄ± inÅŸa ettim.  
Her aÅŸamada, model mimarisi ve donanÄ±m uyumluluÄŸu aÃ§Ä±sÄ±ndan en iyi uygulamalarÄ± gÃ¶zeterek, **Ã¶lÃ§eklenebilir ve tekrarlanabilir deneysel sÃ¼reÃ§ler** tasarladÄ±m.

---

## ğŸš¦ Transformers ile Modelleme YolculuÄŸu

<p align="center" style="font-size:1.1em;">
  <b>ğŸš€ SÄ±radan koddan Ã¼retim kalitesinde NLP'ye: <span style="color:#2980B9;">AutoTokenizer</span>, <span style="color:#229954;">AutoModel</span> ve <span style="color:#CA6F1E;">Pipeline</span> ile <span style="color:#C0392B;">benchmark</span> odaklÄ±, <span style="color:#8E44AD;">optimize</span> bir yolculuk!<br>
  <span style="color:#F39C12;">Her adÄ±mda hÄ±z, gÃ¼Ã§ ve esneklik!</span></b>
</p>

```mermaid
flowchart TD
    %% Stil tanÄ±mlarÄ±
    style A1 fill:#D6EAF8,stroke:#2980B9,stroke-width:3px
    style B1 fill:#F9E79F,stroke:#B7950B,stroke-width:3px
    style B2 fill:#D5F5E3,stroke:#229954,stroke-width:3px
    style B3 fill:#FADBD8,stroke:#C0392B,stroke-width:3px
    style B4 fill:#E8DAEF,stroke:#8E44AD,stroke-width:3px
    style B5 fill:#D4E6F1,stroke:#2471A3,stroke-width:3px
    style B6 fill:#FDEBD0,stroke:#CA6F1E,stroke-width:3px
    style Z1 fill:#D5DBDB,stroke:#34495E,stroke-width:3px

    %% SatÄ±r 1 (Ã¼st)
    A1([<b>ğŸ”<br>Model SeÃ§imi</b>])
    B1([<b>ğŸ§©<br>AutoTokenizer<br><i>Encode/Decode</i></b>])
    B2([<b>ğŸ§ <br>AutoModel<br><i>Manuel Model</i></b>])

    %% SatÄ±r 2 (orta)
    B3([<b>âš¡<br>Pipeline<br><i>Abstraction & HÄ±z</i></b>])
    B4([<b>ğŸ—ï¸<br>Mimari KÄ±yas<br><i>GPT, BERT, T5</i></b>])
    B5([<b>ğŸ’»<br>Cihaz Optimizasyonu<br><i>CPU/GPU/Quant</i></b>])

    %% SatÄ±r 3 (alt)
    B6([<b>ğŸ“Š<br>Performans Ã–lÃ§Ã¼mÃ¼<br><i>Benchmark</i></b>])
    Z1([<b>ğŸš€<br>Ãœretime HazÄ±r Pipeline</b>])

    %% BaÄŸlantÄ±lar (kare ve Ã§apraz)
    A1 --> B1
    A1 --> B2
    B1 --> B3
    B2 --> B3
    B3 --> B4
    B4 --> B5
    B5 --> B6
    B6 --> Z1
    B3 -.-> B6
    B1 -.-> B4
    B2 -.-> B5
```

<p align="center" style="font-size:1.1em; margin-top:10px;">
  <b>âœ¨ <span style="color:#229954;">Fikirden</span> <span style="color:#C0392B;">benchmark'a</span>, <span style="color:#CA6F1E;">optimizasyondan</span> <span style="color:#8E44AD;">Ã¼retime</span>...<br>
  <span style="color:#2980B9;">Her adÄ±mda hÄ±z, gÃ¼Ã§ ve profesyonellik!</span> âœ¨</b>
</p>

---

## ğŸ“š Ä°Ã§erik

### 1. AutoTokenizer & AutoModel YapÄ±sÄ± + Pipeline ile HÄ±zlÄ± Model Ã‡aÄŸÄ±rma  
**Dosya:** `01_autotokenizer_automodel.py`  
- Tokenizer ile encode/decode sÃ¼reÃ§leri  
- AutoModel ile manuel model Ã§aÄŸÄ±rma  
- Pipeline abstraction: hÄ±z ve sadelik  
- Manual vs Pipeline kÄ±yaslamalarÄ±  
- Ã–zelleÅŸtirilmiÅŸ pipeline Ã¶rnekleri  

---

### 2. GPT, BERT ve T5 Modellerinin FarklarÄ± ve Pipeline Entegrasyonu  
**Dosya:** `02_gpt_bert_t5_comparison.py`  
- Model mimarilerinin karÅŸÄ±laÅŸtÄ±rmasÄ±  
- Her bir mimarinin gÃ¼Ã§lÃ¼ yÃ¶nleri ve kullanÄ±m baÄŸlamlarÄ±  
- Pipeline entegrasyonu ile tek satÄ±rlÄ±k testler  
- Parametre sayÄ±sÄ±, hÄ±z ve VRAM tÃ¼ketimi analizleri  

| Model | Mimari | GÃ¼Ã§lÃ¼ YÃ¶nler | KullanÄ±m AlanlarÄ± |
|-------|--------|--------------|-------------------|
| **GPT** | Decoder-only | Uzun metin Ã¼retimi | Creative writing, Conversational AI |
| **BERT** | Encoder-only | Bidirectional anlama | Classification, NER, QA |
| **T5** | Encoder-decoder | Text-to-text yaklaÅŸÄ±mÄ± | Translation, Summarization |

---

### 3. CPU/GPU Performans YÃ¶netimi ve Model Optimizasyonu  
**Dosya:** `03_cpu_gpu_optimization.py`  
- Optimal cihaz seÃ§imi (cuda/mps/cpu)  
- CPU thread tuning  
- GPU bellek yÃ¶netimi (amp, autocast)  
- Model quantization (8-bit, dynamic)  
- Batch processing optimizasyonu  
- Bellek dostu inference yÃ¶ntemleri  

**Kritik Teknikler:**  
- `torch.no_grad()` ile gereksiz gradient hesaplamalarÄ±nÄ± engelleme  
- `torch.cuda.empty_cache()` ve `gc.collect()` ile bellek temizliÄŸi  
- `BitsAndBytesConfig` ile quantization  
- Dynamic padding ile hÄ±zlanma  

---

### 4. Pipeline ile GPU/CPU PerformansÄ±nÄ± Ã–lÃ§me ve KÄ±yaslama  
**Dosya:** `04_performance_measurement.py`  
- PerformanceMeter sÄ±nÄ±fÄ± ile Ã¶lÃ§Ã¼mler  
- FarklÄ± taskâ€™larda benchmark testleri  
- Batch size varyasyonlarÄ±nÄ±n etkisi  
- DetaylÄ± raporlar ve gÃ¶rselleÅŸtirme  

**Metrikler:**  
- Inference sÃ¼resi  
- Memory kullanÄ±mÄ± (CPU/GPU)  
- Throughput (texts/sec)  
- Device utilization  
- Model yÃ¼kleme sÃ¼resi  

---

### ğŸ“ Manuel Kurulum

#### 1. Sanal Ortam OluÅŸtur
```bash
# macOS/Linux
python3 -m venv llm_bootcamp_env
source llm_bootcamp_env/bin/activate

# Windows
python -m venv llm_bootcamp_env
llm_bootcamp_env\Scripts\activate.bat
```

#### 2. BaÄŸÄ±mlÄ±lÄ±klarÄ± YÃ¼kle
```bash
pip install --upgrade pip
pip install -r requirements.txt
```

#### 3. ModÃ¼lleri Ã‡alÄ±ÅŸtÄ±r
```bash
# AutoTokenizer ve AutoModel Ã¶rnekleri
python 01_autotokenizer_automodel.py

# Model karÅŸÄ±laÅŸtÄ±rmasÄ±
python 02_gpt_bert_t5_comparison.py

# Performans optimizasyonu
python 03_cpu_gpu_optimization.py

# Performans Ã¶lÃ§Ã¼mÃ¼
python 04_performance_measurement.py
```

## ğŸ“‹ Gereksinimler

```bash
pip install transformers torch torchvision torchaudio
pip install psutil matplotlib numpy
pip install bitsandbytes  # Quantization iÃ§in (opsiyonel)
```

**GPU DesteÄŸi iÃ§in:**
- CUDA: `pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118`
- Apple Silicon: Otomatik olarak MPS desteÄŸi

---

## ğŸŒŸ HaftanÄ±n AÅŸamalarÄ± & SÄ±kÃ§a Sorulanlar

### 1. **AutoTokenizer & AutoModel ile Temel KullanÄ±m**
- **AmaÃ§:** Hugging Face modellerini hÄ±zlÄ±ca yÃ¼kleyip, metinleri encode/decode etmek.
- **Kod:**
  ```python
  from transformers import AutoTokenizer, AutoModel
  tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
  model = AutoModel.from_pretrained("bert-base-uncased")
  inputs = tokenizer("Transformers harika!", return_tensors="pt")
  outputs = model(**inputs)
  ```
- <div style="border:1px solid #2980B9; border-radius:8px; padding:12px; background:#F4F8FB; margin:10px 0;">
  <b>Soru:</b> AutoTokenizer ve AutoModel neden tercih edilir?<br>
  <b>Cevap:</b> Model ve tokenizer'Ä± otomatik olarak doÄŸru konfigÃ¼rasyonla yÃ¼kler, kod tekrarÄ±nÄ± ve hata riskini azaltÄ±r.
  </div>

---

### 2. **Pipeline ile HÄ±zlÄ± Model Ã‡aÄŸÄ±rma**
- **AmaÃ§:** Tek satÄ±rda inference, task abstraction ve hÄ±z.
- **Kod:**
  ```python
  from transformers import pipeline
  nlp = pipeline("sentiment-analysis", model="distilbert-base-uncased")
  print(nlp("Transformers Ã§ok gÃ¼Ã§lÃ¼!"))
  ```
- <div style="border:1px solid #229954; border-radius:8px; padding:12px; background:#F4FBF4; margin:10px 0;">
  <b>Soru:</b> Pipeline abstraction'Ä±n avantajÄ± nedir?<br>
  <b>Cevap:</b> Model, tokenizer ve task logic'i tek satÄ±rda birleÅŸtirir; hÄ±zlÄ± prototipleme ve test iÃ§in idealdir.
  </div>

---

### 3. **GPT, BERT ve T5 Mimarilerinin KÄ±yaslanmasÄ±**
- **AmaÃ§:** FarklÄ± transformer mimarilerinin gÃ¼Ã§lÃ¼ yÃ¶nlerini ve kullanÄ±m alanlarÄ±nÄ± anlamak.
- **Tablo:**

  | Model | Mimari | GÃ¼Ã§lÃ¼ YÃ¶nler | KullanÄ±m AlanlarÄ± |
  |-------|--------|--------------|-------------------|
  | **GPT** | Decoder-only | Uzun metin Ã¼retimi | Creative writing, Conversational AI |
  | **BERT** | Encoder-only | Bidirectional anlama | Classification, NER, QA |
  | **T5** | Encoder-decoder | Text-to-text yaklaÅŸÄ±mÄ± | Translation, Summarization |

- <div style="border:1px solid #C0392B; border-radius:8px; padding:12px; background:#FDF2F0; margin:10px 0;">
  <b>Soru:</b> Hangi mimari hangi gÃ¶revde Ã¶ne Ã§Ä±kar?<br>
  <b>Cevap:</b> GPT Ã¼retkenlikte, BERT anlamada, T5 ise Ã§oklu gÃ¶revlerde (text-to-text) Ã¼stÃ¼ndÃ¼r.
  </div>

---

### 4. **Cihaz ve Model Optimizasyonu**
- **AmaÃ§:** CPU/GPU seÃ§imi, quantization ve batch processing ile inference hÄ±zÄ±nÄ± artÄ±rmak.
- **Kod:**
  ```python
  import torch
  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
  model = model.to(device)
  with torch.no_grad():
      outputs = model(**{k: v.to(device) for k, v in inputs.items()})
  ```
- <div style="border:1px solid #8E44AD; border-radius:8px; padding:12px; background:#F7F1FA; margin:10px 0;">
  <b>Soru:</b> Quantization ve batch processing neden Ã¶nemlidir?<br>
  <b>Cevap:</b> Quantization ile bellek ve hÄ±z kazanÄ±lÄ±r; batch processing ile throughput artar.
  </div>

---

### 5. **Performans Ã–lÃ§Ã¼mÃ¼ & Benchmarking**
- **AmaÃ§:** Inference sÃ¼resi, memory kullanÄ±mÄ± ve throughput gibi metriklerle model performansÄ±nÄ± Ã¶lÃ§mek.
- **Kod:**
  ```python
  import time
  start = time.time()
  _ = nlp(["Test cÃ¼mlesi"] * 32)
  print("SÃ¼re:", time.time() - start)
  ```
- <div style="border:1px solid #34495E; border-radius:8px; padding:12px; background:#F4F6F7; margin:10px 0;">
  <b>Soru:</b> Hangi metrikler kritik?<br>
  <b>Cevap:</b> Inference sÃ¼resi, memory kullanÄ±mÄ±, throughput ve device utilization Ã¼retim iÃ§in belirleyicidir.
  </div>

---

## ğŸ’¡ En Ä°yi Uygulamalar

### Performans Optimizasyonu
```python
# âœ… Ä°yi
with torch.no_grad():
    outputs = model(**inputs)

# âŒ KÃ¶tÃ¼  
outputs = model(**inputs)  # Gradient hesaplanÄ±r
```

### Device YÃ¶netimi
```python
# âœ… Ä°yi
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)
inputs = {k: v.to(device) for k, v in inputs.items()}

# âŒ KÃ¶tÃ¼
model = model.to("cuda")  # CUDA olmayabilir
```

### Memory YÃ¶netimi
```python
# âœ… Ä°yi
del model
torch.cuda.empty_cache()
gc.collect()

# âŒ KÃ¶tÃ¼
# Memory leak'e sebep olabilir
```

---

<p align="center" style="font-size:1.1em;">
  <b>ğŸŒŸ <span style="color:#CA6F1E;">Transformers ile optimize pipeline</span>, <span style="color:#229954;">Ã¼retim kalitesinde NLP'nin anahtarÄ±dÄ±r!</span> ğŸŒŸ</b>
</p>

<br>

<table align="center">
  <thead>
    <tr>
      <th style="background:#D6EAF8; color:#2980B9;"><b>Model</b></th>
      <th style="background:#F9E79F; color:#B7950B;"><b>Device</b></th>
      <th style="background:#D5F5E3; color:#229954;"><b>Inference Time</b></th>
      <th style="background:#FADBD8; color:#C0392B;"><b>Memory Usage</b></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><b>DistilBERT</b></td>
      <td>CPU</td>
      <td><span style="color:#229954;"><b>0.045s</b></span></td>
      <td>1.2 GB</td>
    </tr>
    <tr>
      <td><b>DistilBERT</b></td>
      <td><b style="color:#2980B9;">GPU</b></td>
      <td><span style="color:#229954;"><b>0.012s</b></span></td>
      <td>2.1 GB</td>
    </tr>
    <tr>
      <td><b>BERT-base</b></td>
      <td>CPU</td>
      <td><span style="color:#229954;"><b>0.089s</b></span></td>
      <td>2.1 GB</td>
    </tr>
    <tr>
      <td><b>BERT-base</b></td>
      <td><b style="color:#2980B9;">GPU</b></td>
      <td><span style="color:#229954;"><b>0.021s</b></span></td>
      <td>3.2 GB</td>
    </tr>
  </tbody>
</table>

---

## ğŸ“š Ek Kaynaklar

> Akademik altyapÄ±mÄ± gÃ¼Ã§lendirmek iÃ§in haftanÄ±n sonunda baÅŸvurduÄŸum **Ã¶nemli referanslar**:  

<details>
<summary>ğŸ“˜ Hugging Face Transformers Documentation</summary>
<a href="https://huggingface.co/docs/transformers/" target="_blank">https://huggingface.co/docs/transformers/</a>  
ğŸ” Modellerin, tokenizerâ€™larÄ±n ve pipeline mimarisinin resmi aÃ§Ä±klamalarÄ± ve ileri dÃ¼zey kullanÄ±m Ã¶rnekleri.  
</details>

<details>
<summary>âš¡ PyTorch Performance Tuning Guide</summary>
<a href="https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html" target="_blank">https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html</a>  
âš™ï¸ CPU/GPU hÄ±z optimizasyonlarÄ±, bellek yÃ¶netimi ve Ã¼retim ortamÄ± iÃ§in ileri seviye teknikler.  
</details>

<details>
<summary>ğŸ“‘ BERT Paper (Devlin et al., 2018)</summary>
<a href="https://arxiv.org/abs/1810.04805" target="_blank">https://arxiv.org/abs/1810.04805</a>  
ğŸ§© Bidirectional encoder mimarisinin metin anlama gÃ¶revlerinde devrim yaratan orijinal makalesi.  
</details>

<details>
<summary>ğŸ“ GPT Paper (Radford et al., 2018)</summary>
<a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf" target="_blank">https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf</a>  
âœ’ï¸ Unsupervised pretraining yaklaÅŸÄ±mÄ±nÄ±n ilk kez tanÄ±mlandÄ±ÄŸÄ±, generative transformer ailesinin baÅŸlangÄ±Ã§ noktasÄ±.  
</details>

<details>
<summary>ğŸ”„ T5 Paper (Raffel et al., 2019)</summary>
<a href="https://arxiv.org/abs/1910.10683" target="_blank">https://arxiv.org/abs/1910.10683</a>  
ğŸŒ â€œText-to-Text Transfer Transformerâ€ paradigmasÄ±nÄ± sunan, tÃ¼m NLP gÃ¶revlerini ortak bir formata indirgeyen Ã§alÄ±ÅŸma.  
</details>

