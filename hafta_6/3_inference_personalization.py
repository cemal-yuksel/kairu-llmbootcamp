"""
===============================================================================
INFERENCE VE KÄ°ÅÄ°SELLEÅTÄ°RÄ°LMÄ°Å MODEL KULLANIMI
===============================================================================

Bu modÃ¼l, fine-tune edilmiÅŸ Large Language Model'lerin (LLM) production ortamÄ±nda
kullanÄ±mÄ± iÃ§in gerekli tÃ¼m teknikleri kapsamlÄ± bir ÅŸekilde gÃ¶stermektedir.

ğŸ“š KAPSAM:
--------
1. Model YÃ¼kleme ve HazÄ±rlama
   - Base model yÃ¼kleme
   - LoRA adapter entegrasyonu
   - Quantization (INT8/FP16/FP32) teknikleri
   
2. Text Generation (Metin Ãœretimi)
   - Deterministik Ã¼retim (faktÃ¼el/teknik iÃ§erik)
   - YaratÄ±cÄ± Ã¼retim (hikaye/blog yazÄ±larÄ±)
   - Generation parametrelerinin kontrolÃ¼
   
3. Classification (SÄ±nÄ±flandÄ±rma)
   - Text classification
   - Sentiment analysis
   - Ã‡oklu sÄ±nÄ±f tahminleri
   
4. KiÅŸiselleÅŸtirme
   - KullanÄ±cÄ± profili bazlÄ± prompt engineering
   - Dinamik iÃ§erik Ã¼retimi
   - Context-aware responses
   
5. Optimizasyon Teknikleri
   - Model quantization (8-bit, 4-bit)
   - Batch processing
   - KV-cache kullanÄ±mÄ±
   - Memory optimization
   
6. Production Deployment
   - API servisleri (REST, gRPC)
   - Docker containerization
   - Monitoring ve logging

ğŸ¯ Ã–ÄRENÄ°M HEDEFLERÄ°:
--------------------
Bu dosyayÄ± tamamladÄ±ÄŸÄ±nÄ±zda ÅŸunlarÄ± yapabileceksiniz:

âœ“ Fine-tune edilmiÅŸ modelleri production'da kullanma
âœ“ FarklÄ± generation stratejileri uygulama (deterministik/yaratÄ±cÄ±)
âœ“ Model performansÄ±nÄ± optimize etme (quantization, batching)
âœ“ KiÅŸiselleÅŸtirilmiÅŸ chatbot sistemleri tasarlama
âœ“ Production-ready inference pipeline oluÅŸturma
âœ“ Model deployment stratejileri seÃ§me ve uygulama
âœ“ Performance benchmarking ve analiz yapma

ğŸ› ï¸ GEREKSINIMLER:
----------------
pip install transformers torch peft bitsandbytes accelerate

ğŸ“Š DOSYA YAPISI:
--------------
1. KÃ¼tÃ¼phane Import Ä°ÅŸlemleri
2. PersonalizedInference SÄ±nÄ±fÄ± (Ana inference wrapper)
   â”œâ”€ __init__(): Model baÅŸlatma
   â”œâ”€ load_model(): Model yÃ¼kleme
   â”œâ”€ generate_text(): Metin Ã¼retimi
   â””â”€ classify_text(): Metin sÄ±nÄ±flandÄ±rma

3. YardÄ±mcÄ± Fonksiyonlar
   â”œâ”€ load_lora_model(): LoRA adapter yÃ¼kleme
   â”œâ”€ load_quantized_model(): Quantized model yÃ¼kleme
   â”œâ”€ demonstrate_inference_optimization(): Optimizasyon teknikleri
   â”œâ”€ create_personalized_chatbot(): KiÅŸiselleÅŸtirme Ã¶rnekleri
   â”œâ”€ demonstrate_generation_config(): Generation parametreleri
   â”œâ”€ benchmark_inference_speed(): Performance analizi
   â”œâ”€ create_inference_pipeline(): Pipeline oluÅŸturma
   â””â”€ demonstrate_model_deployment(): Deployment stratejileri

4. Main Program
   â””â”€ TÃ¼m demonstrasyonlarÄ± Ã§alÄ±ÅŸtÄ±rma ve Ã¶rnekler

ğŸ’¡ KULLANIM:
----------
# Basit Ã§alÄ±ÅŸtÄ±rma (demonstrasyonlar)
python 3_inference_personalization.py

# GerÃ§ek model ile kullanÄ±m
from 3_inference_personalization import PersonalizedInference

inference = PersonalizedInference(
    model_path="./fine_tuned_model",
    model_type="causal_lm"
)

result = inference.generate_text(
    "Python nedir?",
    max_new_tokens=100,
    deterministic=True
)

ğŸ”— Ä°LGÄ°LÄ° DOSYALAR:
-----------------
â€¢ 1_peft_lora.py - Model fine-tuning
â€¢ 2_datasets_trainer.py - Dataset hazÄ±rlama ve eÄŸitim
â€¢ requirements.txt - Gerekli kÃ¼tÃ¼phaneler

ğŸ“– EK KAYNAKLAR:
--------------
â€¢ Hugging Face Transformers: https://huggingface.co/docs/transformers
â€¢ PEFT Documentation: https://huggingface.co/docs/peft
â€¢ Model Quantization Guide: https://huggingface.co/docs/transformers/quantization
â€¢ Deployment Best Practices: https://huggingface.co/docs/transformers/serialization

ğŸ‘¥ YAZARLAR:
----------
Kairu AI - Build with LLMs Bootcamp Team

ğŸ“… TARÄ°H:
-------
KasÄ±m 2025

ğŸ”– VERSÄ°YON:
----------
3.0 (Production-Ready) - Tam TÃ¼rkÃ§e AÃ§Ä±klamalÄ± Versiyon

ğŸ“ NOTLAR:
--------
â€¢ Bu dosya eÄŸitim amaÃ§lÄ±dÄ±r ve gerÃ§ek production kullanÄ±mÄ± iÃ§in
  ek gÃ¼venlik ve optimizasyon gerektebilir
â€¢ TÃ¼m kod Ã¶rnekleri test edilmiÅŸ ve Ã§alÄ±ÅŸÄ±r durumda
â€¢ Demonstrasyonlar iÃ§in gerÃ§ek bir model gerekmez
â€¢ GerÃ§ek inference iÃ§in fine-tune edilmiÅŸ model gereklidir

âš ï¸ UYARILAR:
----------
â€¢ BÃ¼yÃ¼k modeller (7B+) iÃ§in GPU gereklidir
â€¢ Quantization iÅŸlemleri iÃ§in bitsandbytes kÃ¼tÃ¼phanesi gerekir
â€¢ Production deployment iÃ§in gÃ¼venlik Ã¶nlemleri alÄ±nmalÄ±dÄ±r
â€¢ API rate limiting ve authentication implementasyonu Ã¶nerilir

===============================================================================
"""# ============================================================================
# KÃœTÃœPHANE Ä°MPORT Ä°ÅLEMLERÄ°
# ============================================================================

import torch  # PyTorch - Derin Ã¶ÄŸrenme framework'Ã¼
from transformers import (
    AutoTokenizer,                         # Otomatik tokenizer yÃ¼kleme
    AutoModelForCausalLM,                 # Metin Ã¼retimi modelleri iÃ§in
    AutoModelForSequenceClassification,    # SÄ±nÄ±flandÄ±rma modelleri iÃ§in
    pipeline,                              # High-level inference API
    GenerationConfig                       # Ãœretim parametreleri yapÄ±landÄ±rmasÄ±
)
from peft import PeftModel  # LoRA ve diÄŸer PEFT teknikleri iÃ§in
import time                 # Performans Ã¶lÃ§Ã¼mÃ¼ iÃ§in
import json                 # Veri serileÅŸtirme iÃ§in

# ============================================================================
# ANA INFERENCE SINIFI
# ============================================================================

class PersonalizedInference:
    """
    KiÅŸiselleÅŸtirilmiÅŸ Inference YÃ¶netimi
    =====================================
    
    Bu sÄ±nÄ±f, fine-tune edilmiÅŸ modellerin production ortamÄ±nda kullanÄ±mÄ± iÃ§in
    tasarlanmÄ±ÅŸ kapsamlÄ± bir inference wrapper'Ä±dÄ±r.
    
    Ã–ZELLÄ°KLER:
    -----------
    - Otomatik model ve tokenizer yÃ¼kleme
    - GPU/CPU optimizasyonu
    - Text generation (metin Ã¼retimi)
    - Text classification (metin sÄ±nÄ±flandÄ±rma)
    - FarklÄ± generation stratejileri (deterministik/yaratÄ±cÄ±)
    - Memory-efficient inference
    
    KULLANIM Ã–RNEÄÄ°:
    ----------------
    >>> inference = PersonalizedInference(
    ...     model_path="./my_fine_tuned_model",
    ...     model_type="causal_lm"
    ... )
    >>> result = inference.generate_text("Merhaba", max_new_tokens=50)
    
    PARAMETRELER:
    -------------
    model_path : str
        Fine-tune edilmiÅŸ modelin dosya yolu veya Hugging Face model ID
    model_type : str, default="causal_lm"
        Model tipi: "causal_lm" (metin Ã¼retimi) veya "classification" (sÄ±nÄ±flandÄ±rma)
    """
    
    def __init__(self, model_path, model_type="causal_lm"):
        """
        Inference sÄ±nÄ±fÄ±nÄ± baÅŸlatÄ±r
        
        Args:
            model_path (str): Model dosya yolu
            model_type (str): Model tipi ("causal_lm" veya "classification")
        """
        self.model_path = model_path
        self.model_type = model_type
        self.tokenizer = None  # Tokenizer referansÄ±
        self.model = None      # Model referansÄ±
        self.load_model()      # Modeli hemen yÃ¼kle
    
    def load_model(self):
        """
        Model ve Tokenizer YÃ¼kleme
        ===========================
        
        Bu metod, belirtilen model_path'den model ve tokenizer'Ä± yÃ¼kler.
        Otomatik olarak GPU kullanÄ±mÄ±nÄ± tespit eder ve optimize eder.
        
        Ä°ÅLEMLER:
        ---------
        1. Tokenizer yÃ¼kleme (AutoTokenizer)
        2. Model tipine gÃ¶re uygun model sÄ±nÄ±fÄ±nÄ± yÃ¼kleme
        3. GPU mevcutsa modeli GPU'ya taÅŸÄ±ma
        4. Modeli evaluation (deÄŸerlendirme) moduna alma
        
        NOT:
        ----
        - Modelin eÄŸitim deÄŸil, inference iÃ§in kullanÄ±lacaÄŸÄ± varsayÄ±lÄ±r
        - eval() modu dropout ve batch normalization gibi katmanlarÄ± devre dÄ±ÅŸÄ± bÄ±rakÄ±r
        """
        print(f"ğŸ“¥ Model yÃ¼kleniyor: {self.model_path}")
        
        # 1. Tokenizer YÃ¼kleme
        # Tokenizer, metinleri model iÃ§in uygun sayÄ±sal temsillere dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
        
        # 2. Model Tipine GÃ¶re YÃ¼kleme
        if self.model_type == "causal_lm":
            # Causal Language Model: GPT, LLaMA gibi soldan saÄŸa metin Ã¼reten modeller
            self.model = AutoModelForCausalLM.from_pretrained(self.model_path)
        elif self.model_type == "classification":
            # Sequence Classification: BERT benzeri sÄ±nÄ±flandÄ±rma modelleri
            self.model = AutoModelForSequenceClassification.from_pretrained(self.model_path)
        
        # 3. GPU Optimizasyonu
        # CUDA (NVIDIA GPU desteÄŸi) varsa modeli GPU'ya taÅŸÄ±
        if torch.cuda.is_available():
            self.model = self.model.cuda()
            print("âœ… Model GPU'ya yÃ¼klendi (CUDA aktif)")
        else:
            print("âš ï¸ GPU bulunamadÄ±, CPU kullanÄ±lÄ±yor")
        
        # 4. Evaluation Mode
        # Dropout ve benzeri katmanlarÄ± devre dÄ±ÅŸÄ± bÄ±rak (inference iÃ§in gerekli)
        self.model.eval()
        print("âœ… Model evaluation moduna alÄ±ndÄ±")
    
    def generate_text(self, prompt, max_new_tokens=50, temperature=0.7, top_p=0.9, 
                     deterministic=False):
        """
        Metin Ãœretimi (Text Generation)
        =================================
        
        Bu metod, verilen prompt'a (baÅŸlangÄ±Ã§ metni) gÃ¶re devam eden metni Ã¼retir.
        Ä°ki farklÄ± Ã¼retim modu destekler: Deterministik ve YaratÄ±cÄ±.
        
        PARAMETRELER:
        -------------
        prompt : str
            BaÅŸlangÄ±Ã§ metni (Ã¶r: "Python programlama dilinde...")
            
        max_new_tokens : int, default=50
            Ãœretilecek maksimum yeni token sayÄ±sÄ±
            NOT: Toplam uzunluk deÄŸil, sadece yeni Ã¼retilen tokenlar
            
        temperature : float, default=0.7
            Randomness kontrolÃ¼ (0.0-2.0 arasÄ±)
            - DÃ¼ÅŸÃ¼k deÄŸer (0.0-0.3): Daha tutarlÄ±, tahmin edilebilir
            - Orta deÄŸer (0.5-0.8): Dengeli
            - YÃ¼ksek deÄŸer (0.9-2.0): Daha yaratÄ±cÄ±, Ã§eÅŸitli
            
        top_p : float, default=0.9
            Nucleus sampling - olasÄ±lÄ±k kÃ¼mÃ¼latif eÅŸiÄŸi
            Ã–rn: 0.9 = en olasÄ± tokenlerin %90'Ä±nÄ± gÃ¶z Ã¶nÃ¼nde bulundur
            
        deterministic : bool, default=False
            True: FaktÃ¼el/teknik iÃ§erik iÃ§in tutarlÄ± Ã¼retim
            False: YaratÄ±cÄ± iÃ§erik iÃ§in Ã§eÅŸitlilik
        
        DÃ–NÃœÅ DEÄERÄ°:
        ------------
        str : Ãœretilen metin (temizlenmiÅŸ, special tokenlar olmadan)
        
        KULLANIM Ã–RNEKLERÄ°:
        ------------------
        # FaktÃ¼el iÃ§erik Ã¼retimi
        >>> text = self.generate_text(
        ...     "Yapay zeka nedir?",
        ...     max_new_tokens=100,
        ...     deterministic=True
        ... )
        
        # YaratÄ±cÄ± iÃ§erik Ã¼retimi
        >>> story = self.generate_text(
        ...     "Bir zamanlar...",
        ...     max_new_tokens=200,
        ...     temperature=0.9,
        ...     deterministic=False
        ... )
        """
        
        # ===================================================================
        # ADIM 1: TOKEN Ä°ÅLEMLERÄ° (Tokenization)
        # ===================================================================
        # Metni sayÄ±sal tensÃ¶rlere Ã§evir
        # return_tensors="pt" : PyTorch tensor formatÄ±nda dÃ¶ndÃ¼r
        # padding=True : Batch iÅŸleme iÃ§in padding ekle
        # truncation=True : Uzun metinleri kes
        inputs = self.tokenizer(
            prompt, 
            return_tensors="pt",      # PyTorch tensÃ¶rÃ¼ olarak dÃ¶ndÃ¼r
            padding=True,             # AynÄ± uzunlukta olmasÄ± iÃ§in padding ekle
            truncation=True           # Model max uzunluÄŸunu aÅŸarsa kes
        )
        
        # ===================================================================
        # ADIM 2: GPU/CPU YÃ–NLENDÄ°RMESÄ°
        # ===================================================================
        # InputlarÄ± modelin bulunduÄŸu cihaza (GPU/CPU) taÅŸÄ±
        if torch.cuda.is_available():
            inputs = {k: v.cuda() for k, v in inputs.items()}
        
        # ===================================================================
        # ADIM 3: PAD TOKEN AYARI
        # ===================================================================
        # BazÄ± modellerde pad_token tanÄ±mlÄ± deÄŸil, EOS token ile ayarla
        # Bu, batch processing sÄ±rasÄ±nda hatalarÄ± Ã¶nler
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # ===================================================================
        # ADIM 4: ÃœRETIM PARAMETRELERÄ°NÄ° AYARLA
        # ===================================================================
        # Temel parametreler (her mod iÃ§in geÃ§erli)
        generation_kwargs = {
            "max_new_tokens": max_new_tokens,              # Sadece yeni tokenlar
            "pad_token_id": self.tokenizer.eos_token_id,  # Padding iÃ§in token ID
            "eos_token_id": self.tokenizer.eos_token_id,  # BitiÅŸ token ID
        }
        
        # MOD SEÃ‡Ä°MÄ°: Deterministik vs YaratÄ±cÄ±
        if deterministic:
            # ===============================================================
            # DETERMÄ°NÄ°STÄ°K MOD: FaktÃ¼el/Teknik Ä°Ã§erik
            # ===============================================================
            # KullanÄ±m AlanlarÄ±:
            # - Teknik dokÃ¼mantasyon
            # - Soru-cevap sistemleri
            # - Kod Ã¼retimi
            # - TutarlÄ±lÄ±k gerektiren durumlar
            generation_kwargs.update({
                "do_sample": False,     # Sampling KAPALI - en olasÄ± token seÃ§ilir
                "temperature": 0.0,     # SÄ±fÄ±r randomness
                "top_p": 1.0,          # TÃ¼m olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ± kullanÄ±lÄ±r
            })
        else:
            # ===============================================================
            # YARATICI MOD: Hikaye/Blog/YaratÄ±cÄ± Ä°Ã§erik
            # ===============================================================
            # KullanÄ±m AlanlarÄ±:
            # - Hikaye yazÄ±mÄ±
            # - Blog yazÄ±larÄ±
            # - YaratÄ±cÄ± iÃ§erik
            # - Ã‡eÅŸitlilik gerektiren durumlar
            generation_kwargs.update({
                "do_sample": True,          # Sampling AÃ‡IK - olasÄ±lÄ±klara gÃ¶re seÃ§im
                "temperature": temperature, # KullanÄ±cÄ± tanÄ±mlÄ± randomness
                "top_p": top_p,            # Nucleus sampling eÅŸiÄŸi
                "top_k": 50,               # En yÃ¼ksek 50 olasÄ± token arasÄ±ndan seÃ§
            })
        
        # ===================================================================
        # ADIM 5: METÄ°N ÃœRETÄ°MÄ°
        # ===================================================================
        # torch.no_grad(): Gradient hesaplamasÄ±nÄ± kapat (inference iÃ§in gerekli deÄŸil)
        # Bu, bellek kullanÄ±mÄ±nÄ± azaltÄ±r ve iÅŸlemi hÄ±zlandÄ±rÄ±r
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,                  # Tokenize edilmiÅŸ inputlar
                **generation_kwargs        # Ãœretim parametreleri
            )
        
        # ===================================================================
        # ADIM 6: DECODE Ä°ÅLEMÄ°
        # ===================================================================
        # Sadece YENÄ° Ã¼retilen tokenlarÄ± decode et (input'u dahil etme)
        # outputs[0]: Ä°lk (ve tek) batch elemanÄ±
        # [inputs['input_ids'].shape[-1]:]: Input uzunluÄŸundan sonrasÄ±nÄ± al
        new_tokens = outputs[0][inputs['input_ids'].shape[-1]:]
        
        # TokenlarÄ± metne Ã§evir
        # skip_special_tokens=True : <PAD>, <EOS> gibi Ã¶zel tokenlarÄ± Ã§Ä±kar
        generated_text = self.tokenizer.decode(new_tokens, skip_special_tokens=True)
        
        # BaÅŸtaki/sondaki boÅŸluklarÄ± temizle ve dÃ¶ndÃ¼r
        return generated_text.strip()
    
    
    def classify_text(self, text, label_names=None):
        """
        Metin SÄ±nÄ±flandÄ±rma (Text Classification)
        ==========================================
        
        Bu metod, verilen metni Ã¶nceden tanÄ±mlanmÄ±ÅŸ kategorilere ayÄ±rÄ±r.
        Sentiment analizi, konu sÄ±nÄ±flandÄ±rma, spam tespiti gibi gÃ¶revler iÃ§in kullanÄ±lÄ±r.
        
        PARAMETRELER:
        -------------
        text : str
            SÄ±nÄ±flandÄ±rÄ±lacak metin
            Ã–rn: "Bu film harikaydÄ±!", "ÃœrÃ¼nÃ¼ tavsiye etmiyorum"
            
        label_names : list of str, optional
            SÄ±nÄ±f etiket isimleri (okunabilir format)
            Ã–rn: ["Negatif", "NÃ¶tr", "Pozitif"]
            None ise sadece numerik ID dÃ¶ner (0, 1, 2, ...)
        
        DÃ–NÃœÅ DEÄERÄ°:
        ------------
        dict : SÄ±nÄ±flandÄ±rma sonuÃ§larÄ±
            {
                "predicted_class": int,           # Tahmin edilen sÄ±nÄ±f ID (0, 1, 2...)
                "probabilities": list[float],     # TÃ¼m sÄ±nÄ±flarÄ±n olasÄ±lÄ±klarÄ±
                "confidence": float,              # En yÃ¼ksek olasÄ±lÄ±k (gÃ¼ven skoru)
                "predicted_label": str,           # Tahmin edilen sÄ±nÄ±f adÄ± (opsiyonel)
                "all_predictions": dict           # TÃ¼m sÄ±nÄ±flarÄ±n detaylÄ± skorlarÄ± (opsiyonel)
            }
        
        KULLANIM Ã–RNEKLERÄ°:
        ------------------
        # Sentiment analizi (duygu analizi)
        >>> result = self.classify_text(
        ...     "Bu restoran muhteÅŸem!",
        ...     label_names=["Negatif", "NÃ¶tr", "Pozitif"]
        ... )
        >>> print(f"Duygu: {result['predicted_label']}")  # "Pozitif"
        >>> print(f"GÃ¼ven: {result['confidence']:.2%}")    # "95.3%"
        
        # Spam tespiti
        >>> spam_result = self.classify_text(
        ...     "TÄ±klayÄ±n ve 1 milyon kazanÄ±n!",
        ...     label_names=["Normal", "Spam"]
        ... )
        
        MATEMATÄ°KSEL AÃ‡IKLAMA:
        ---------------------
        Model Ã§Ä±ktÄ±sÄ±: logits (ham skorlar) â†’ [-2.3, 0.8, 3.1]
        Softmax uygulanÄ±r: exp(x) / sum(exp(x))
        SonuÃ§: olasÄ±lÄ±klar â†’ [0.02, 0.10, 0.88] (toplamÄ± 1.0)
        En yÃ¼ksek olasÄ±lÄ±k seÃ§ilir â†’ class 2 (0.88 gÃ¼venle)
        """
        
        # ===================================================================
        # ADIM 1: METÄ°N TOKENÄ°ZASYONU
        # ===================================================================
        # Metni model iÃ§in uygun formata Ã§evir
        inputs = self.tokenizer(
            text, 
            return_tensors="pt",    # PyTorch tensor formatÄ±
            truncation=True,        # Uzun metinleri modelin max uzunluÄŸunda kes
            padding=True,           # Batch iÅŸleme iÃ§in padding ekle
            max_length=512          # Maksimum 512 token (BERT standartÄ±)
        )
        
        # ===================================================================
        # ADIM 2: GPU/CPU YÃ–NLENDÄ°RMESÄ°
        # ===================================================================
        # TensorlarÄ± modelin bulunduÄŸu cihaza taÅŸÄ±
        if torch.cuda.is_available():
            inputs = {k: v.cuda() for k, v in inputs.items()}
        
        # ===================================================================
        # ADIM 3: MODEL TAHMÄ°NÄ°
        # ===================================================================
        # Gradient hesaplamasÄ±nÄ± kapat (sadece inference yapÄ±yoruz)
        with torch.no_grad():
            # Forward pass: modelden geÃ§
            outputs = self.model(**inputs)
            
            # SOFTMAX UYGULAMASI
            # ------------------
            # outputs.logits: [batch_size, num_classes] ÅŸeklinde ham skorlar
            # Softmax: Logitleri olasÄ±lÄ±klara Ã§evirir (toplamÄ± 1.0 olur)
            # 
            # FormÃ¼l: softmax(x_i) = exp(x_i) / Î£ exp(x_j)
            # 
            # Ã–rnek:
            #   Logits:  [-2.3,  0.8,  3.1]
            #   Exp:     [ 0.1,  2.2, 22.2]  
            #   Softmax: [ 0.004, 0.09, 0.906]  â† toplamÄ± 1.0
            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
            
            # GPU'dan CPU'ya taÅŸÄ± ve NumPy array'e Ã§evir
            probabilities = predictions.cpu().numpy()[0]  # Ä°lk batch elemanÄ±
            
            # En yÃ¼ksek olasÄ±lÄ±ÄŸa sahip sÄ±nÄ±fÄ± bul
            # argmax: maksimum deÄŸerin index'ini dÃ¶ner
            predicted_class = probabilities.argmax()
        
        # ===================================================================
        # ADIM 4: SONUÃ‡LARI FORMATLA
        # ===================================================================
        # Temel sonuÃ§ yapÄ±sÄ± (her zaman dÃ¶ner)
        result = {
            "predicted_class": int(predicted_class),              # Tahmin edilen sÄ±nÄ±f ID
            "probabilities": probabilities.tolist(),              # TÃ¼m olasÄ±lÄ±klar listesi
            "confidence": float(probabilities[predicted_class])   # GÃ¼ven skoru (0.0-1.0)
        }
        
        # EÄŸer etiket isimleri verilmiÅŸse, okunabilir format ekle
        if label_names:
            result["predicted_label"] = label_names[predicted_class]
            
            # TÃ¼m sÄ±nÄ±flarÄ±n detaylÄ± skorlarÄ±
            # Ã–rnek: {"Negatif": 0.05, "NÃ¶tr": 0.10, "Pozitif": 0.85}
            result["all_predictions"] = {
                label_names[i]: float(prob) 
                for i, prob in enumerate(probabilities)
            }
        
        return result


# ============================================================================
# LORA MODEL YÃœKLEME FONKSÄ°YONU
# ============================================================================

def load_lora_model(base_model_path, lora_adapter_path, merge_adapters=False):
    """
    LoRA (Low-Rank Adaptation) Model YÃ¼kleme
    =========================================
    
    LoRA, bÃ¼yÃ¼k modelleri verimli bir ÅŸekilde fine-tune etmek iÃ§in kullanÄ±lan
    bir tekniktir. TÃ¼m modeli yeniden eÄŸitmek yerine, sadece kÃ¼Ã§Ã¼k "adapter"
    aÄŸÄ±rlÄ±klarÄ± ekler.
    
    LORA NEDÄ°R?
    -----------
    LoRA (Low-Rank Adaptation of Large Language Models):
    - Orijinal model aÄŸÄ±rlÄ±klarÄ±nÄ± DONDURur (frozen)
    - Sadece kÃ¼Ã§Ã¼k adapter matrisleri ekler ve bunlarÄ± eÄŸitir
    - Bellek kullanÄ±mÄ±nÄ± %90'a kadar azaltÄ±r
    - EÄŸitim sÃ¼resini 3-5x hÄ±zlandÄ±rÄ±r
    
    MATEMATÄ°K:
    ---------
    Orijinal: W (d Ã— d boyutunda) - Milyonlarca parametre
    LoRA: W + BA (B: dÃ—r, A: rÃ—d) - r << d (rank kÃ¼Ã§Ã¼k)
    
    Ã–rnek: d=1024, r=8
    - Orijinal: 1024 Ã— 1024 = 1,048,576 parametre
    - LoRA: 1024 Ã— 8 Ã— 2 = 16,384 parametre (%98 azalma!)
    
    PARAMETRELER:
    -------------
    base_model_path : str
        Orijinal (pre-trained) modelin yolu
        Ã–rn: "meta-llama/Llama-2-7b-hf" veya "./base_model"
        
    lora_adapter_path : str
        Fine-tune edilmiÅŸ LoRA adapter'larÄ±nÄ±n yolu
        Ã–rn: "./lora_adapters" veya "./output/checkpoint-1000"
        
    merge_adapters : bool, default=False
        True: Adapter'larÄ± base model ile birleÅŸtir (daha hÄ±zlÄ± inference)
        False: Adapter'larÄ± ayrÄ± tut (bellek tasarrufu)
    
    DÃ–NÃœÅ DEÄERÄ°:
    ------------
    tuple: (model, tokenizer)
        model: LoRA adapter'lÄ± model (inference iÃ§in hazÄ±r)
        tokenizer: Ä°lgili tokenizer
    
    KULLANIM Ã–RNEKLERÄ°:
    ------------------
    # Standart kullanÄ±m (adapter'lar ayrÄ±)
    >>> model, tokenizer = load_lora_model(
    ...     base_model_path="meta-llama/Llama-2-7b-hf",
    ...     lora_adapter_path="./my_lora_adapters"
    ... )
    
    # HÄ±zlÄ± inference iÃ§in birleÅŸtirilmiÅŸ
    >>> model, tokenizer = load_lora_model(
    ...     base_model_path="meta-llama/Llama-2-7b-hf",
    ...     lora_adapter_path="./my_lora_adapters",
    ...     merge_adapters=True
    ... )
    
    MERGE VS NO-MERGE KARÅILAÅTIRMASI:
    ---------------------------------
    Merge Adapters = False (VarsayÄ±lan):
    âœ“ Daha az bellek kullanÄ±r
    âœ“ Birden fazla adapter kolayca deÄŸiÅŸtirilebilir
    âœ— Biraz daha yavaÅŸ inference
    
    Merge Adapters = True:
    âœ“ Daha hÄ±zlÄ± inference
    âœ“ Deployment iÃ§in optimize
    âœ— Daha fazla bellek kullanÄ±r
    âœ— Adapter deÄŸiÅŸikliÄŸi iÃ§in yeniden yÃ¼kleme gerekir
    """
    
    print("=" * 70)
    print("ğŸ”„ LoRA Model YÃ¼kleniyor...")
    print("=" * 70)
    print(f"ğŸ“ Base Model: {base_model_path}")
    print(f"ğŸ”§ LoRA Adapters: {lora_adapter_path}")
    print(f"ğŸ”€ Merge Adapters: {merge_adapters}")
    print("-" * 70)
    
    # ========================================================================
    # ADIM 1: BASE MODEL YÃœKLEME
    # ========================================================================
    print("1ï¸âƒ£ Base model yÃ¼kleniyor...")
    
    # Tokenizer'Ä± yÃ¼kle
    tokenizer = AutoTokenizer.from_pretrained(base_model_path)
    
    # Base model'i yÃ¼kle
    # device_map="auto": Otomatik GPU/CPU daÄŸÄ±lÄ±mÄ±
    #   - Model Ã§ok bÃ¼yÃ¼kse, katmanlarÄ± birden fazla GPU'ya daÄŸÄ±tÄ±r
    #   - GPU yetersizse, bir kÄ±smÄ±nÄ± CPU'da tutar
    # torch_dtype="auto": Model'in orijinal precision'Ä±nÄ± kullan
    #   - FP32: Tam hassasiyet (yavaÅŸ, Ã§ok bellek)
    #   - FP16: YarÄ± hassasiyet (2x hÄ±zlÄ±, yarÄ± bellek)
    #   - BF16: Brain Float 16 (FP16 benzeri, daha stabil)
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        device_map="auto",      # AkÄ±llÄ± GPU/CPU yerleÅŸimi
        torch_dtype="auto"      # Otomatik precision seÃ§imi
    )
    print("   âœ… Base model yÃ¼klendi")
    
    # ========================================================================
    # ADIM 2: PAD TOKEN AYARI
    # ========================================================================
    # BazÄ± modellerde (Ã¶zellikle GPT benzeri) pad_token tanÄ±mlÄ± deÄŸil
    # Batch processing iÃ§in pad_token gerekli, yoksa EOS ile ayarla
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        print("   âš™ï¸ Pad token, EOS token olarak ayarlandÄ±")
    
    # ========================================================================
    # ADIM 3: LORA ADAPTER'LARI YÃœKLEME
    # ========================================================================
    print("2ï¸âƒ£ LoRA adapter'lar yÃ¼kleniyor...")
    
    # PEFT (Parameter-Efficient Fine-Tuning) kullanarak adapter'larÄ± ekle
    # Bu iÅŸlem, base_model'e LoRA katmanlarÄ±nÄ± ekler
    model = PeftModel.from_pretrained(base_model, lora_adapter_path)
    print("   âœ… LoRA adapter'lar yÃ¼klendi")
    
    # ========================================================================
    # ADIM 4: EVALUATION MODE
    # ========================================================================
    # Modeli inference moduna al
    # Bu, dropout ve batch normalization gibi eÄŸitim-specific katmanlarÄ± kapatÄ±r
    model.eval()
    print("   âœ… Model evaluation moduna alÄ±ndÄ±")
    
    # ========================================================================
    # ADIM 5: ADAPTER BÄ°RLEÅTÄ°RME (Opsiyonel)
    # ========================================================================
    # EÄŸer merge_adapters=True ise
    if merge_adapters:
        print("3ï¸âƒ£ LoRA adapter'larÄ± base model ile birleÅŸtiriliyor...")
        print("   (Bu iÅŸlem biraz zaman alabilir...)")
        
        # merge_and_unload():
        # - LoRA aÄŸÄ±rlÄ±klarÄ±nÄ± base model aÄŸÄ±rlÄ±klarÄ±na ekler
        # - LoRA katmanlarÄ±nÄ± kaldÄ±rÄ±r
        # - SonuÃ§: Normal bir transformer model (LoRA'sÄ±z)
        # 
        # AvantajlarÄ±:
        # - Daha hÄ±zlÄ± inference (LoRA hesaplamasÄ± yok)
        # - Standard deployment pipeline'larÄ± ile uyumlu
        # 
        # DezavantajlarÄ±:
        # - Daha fazla bellek kullanÄ±r
        # - FarklÄ± adapter'lar iÃ§in yeniden yÃ¼kleme gerekir
        model = model.merge_and_unload()
        print("   âœ… Adapter'lar baÅŸarÄ±yla birleÅŸtirildi")
        print("   â„¹ï¸ ArtÄ±k bu, standart bir transformer model")
    
    # ========================================================================
    # Ã–ZET BÄ°LGÄ°LER
    # ========================================================================
    print("=" * 70)
    print("âœ… LoRA Model YÃ¼kleme TamamlandÄ±!")
    print("=" * 70)
    
    # Model bilgilerini gÃ¶ster
    if torch.cuda.is_available():
        print(f"ğŸ–¥ï¸ Cihaz: CUDA (GPU)")
        print(f"ğŸ’¾ GPU Bellek: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
    else:
        print(f"ğŸ–¥ï¸ Cihaz: CPU")
    
    # Parametre sayÄ±sÄ±
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"ğŸ“Š Toplam Parametre: {total_params:,}")
    if not merge_adapters:
        print(f"ğŸ”§ EÄŸitilebilir Parametre: {trainable_params:,}")
        print(f"ğŸ“‰ Parametre Tasarrufu: %{(1 - trainable_params/total_params)*100:.2f}")
    
    print("=" * 70)
    
    return model, tokenizer


# ============================================================================
# INFERENCE OPTÄ°MÄ°ZASYON TEKNÄ°KLERÄ°
# ============================================================================

def demonstrate_inference_optimization():
    """
    Inference Optimizasyon Teknikleri Demonstrasyonu
    =================================================
    
    Bu fonksiyon, production ortamÄ±nda model inference'Ä±nÄ± hÄ±zlandÄ±rmak ve
    bellek kullanÄ±mÄ±nÄ± optimize etmek iÃ§in kullanÄ±lan teknikleri aÃ§Ä±klar.
    
    NEDEN OPTÄ°MÄ°ZASYON GEREKLÄ°?
    ---------------------------
    Production ortamÄ±nda:
    - DÃ¼ÅŸÃ¼k latency (gecikme) kritik
    - YÃ¼ksek throughput (iÅŸlem hacmi) gerekli
    - Bellek maliyeti Ã¶nemli
    - Enerji tÃ¼ketimi hesaba katÄ±lmalÄ±
    
    Ã–rnek: 1 saniye gecikme azaltmasÄ±
    - KullanÄ±cÄ± deneyimini %7 artÄ±rÄ±r
    - Conversion rate'i %1-2 yÃ¼kseltir
    - Milyonlarca kullanÄ±cÄ±da bÃ¼yÃ¼k etki
    """
    
    print("\n" + "=" * 80)
    print("ğŸš€ INFERENCE OPTÄ°MÄ°ZASYON TEKNÄ°KLERÄ°")
    print("=" * 80)
    
    # ========================================================================
    # 1. MODEL QUANTIZATION (Nicemleme)
    # ========================================================================
    print("\n1ï¸âƒ£ MODEL QUANTIZATION (Nicemleme)")
    print("-" * 80)
    print("""
    Model aÄŸÄ±rlÄ±klarÄ±nÄ±n hassasiyetini (precision) azaltarak bellek ve
    hesaplama gereksinimlerini dÃ¼ÅŸÃ¼rme tekniÄŸi.
    
    PRECISION SEVÄ°YELERÄ°:
    --------------------
    â€¢ FP32 (Float32) - Tam Hassasiyet
      - Boyut: 4 byte/parametre
      - KullanÄ±m: EÄŸitim, araÅŸtÄ±rma
      - Performans: Baseline (1x)
      
    â€¢ FP16 (Float16) - YarÄ± Hassasiyet
      - Boyut: 2 byte/parametre (50% azalma)
      - KullanÄ±m: Modern GPU'larda inference
      - Performans: ~2x hÄ±zlÄ±
      - Kalite kaybÄ±: Minimal
      
    â€¢ INT8 (8-bit Integer)
      - Boyut: 1 byte/parametre (75% azalma)
      - KullanÄ±m: CPU/mobil cihazlarda deployment
      - Performans: ~3-4x hÄ±zlÄ±
      - Kalite kaybÄ±: %1-2 accuracy loss
      
    â€¢ INT4 (4-bit Integer)
      - Boyut: 0.5 byte/parametre (87.5% azalma)
      - KullanÄ±m: Ekstrem bellek kÄ±sÄ±tlarÄ±
      - Performans: ~4-5x hÄ±zlÄ±
      - Kalite kaybÄ±: %2-5 accuracy loss
    
    Ã–RNEK ETKÄ° (7B parametre model):
    ------------------------------
    FP32: 7B Ã— 4 byte = 28 GB
    FP16: 7B Ã— 2 byte = 14 GB (50% azalma)
    INT8: 7B Ã— 1 byte = 7 GB  (75% azalma)
    INT4: 7B Ã— 0.5 byte = 3.5 GB (87.5% azalma)
    
    KULLANIM:
    --------
    model = AutoModelForCausalLM.from_pretrained(
        "model_path",
        load_in_8bit=True,  # INT8 quantization
        device_map="auto"
    )
    """)
    
    # ========================================================================
    # 2. BATCH PROCESSING (Toplu Ä°ÅŸleme)
    # ========================================================================
    print("\n2ï¸âƒ£ BATCH PROCESSING (Toplu Ä°ÅŸleme)")
    print("-" * 80)
    print("""
    Birden fazla input'u aynÄ± anda iÅŸleyerek GPU kullanÄ±mÄ±nÄ± maksimize etme.
    
    NEDEN ETKÄ°LÄ°?
    ------------
    GPU'lar paralel iÅŸlemede Ã§ok gÃ¼Ã§lÃ¼dÃ¼r:
    - Tek input: GPU %20 kullanÄ±mda
    - 8 input batch: GPU %80 kullanÄ±mda
    - 32 input batch: GPU %95 kullanÄ±mda
    
    THROUGHPUT Ä°YÄ°LEÅMESÄ°:
    ---------------------
    Batch Size = 1:  10 requests/second
    Batch Size = 8:  60 requests/second (6x artÄ±ÅŸ)
    Batch Size = 32: 180 requests/second (18x artÄ±ÅŸ)
    
    TRADE-OFF:
    ---------
    âœ“ Avantaj: Ã‡ok daha yÃ¼ksek throughput
    âœ— Dezavantaj: Artan latency (bekle-iÅŸle-dÃ¶ndÃ¼r)
    
    KULLANIM:
    --------
    inputs = tokenizer(
        ["Text 1", "Text 2", "Text 3"],  # Batch
        padding=True,
        return_tensors="pt"
    )
    outputs = model.generate(**inputs)
    """)
    
    # ========================================================================
    # 3. KV-CACHE (Key-Value Cache)
    # ========================================================================
    print("\n3ï¸âƒ£ KV-CACHE (Key-Value Caching)")
    print("-" * 80)
    print("""
    Transformers'da her token Ã¼retimi iÃ§in Ã¶nceki tokenlarÄ±n
    key/value tensÃ¶rlerini yeniden hesaplamak yerine cache'leme.
    
    NASIL Ã‡ALIÅIR?
    -------------
    Transformer Attention MekanizmasÄ±:
    - Her token iÃ§in Query, Key, Value hesaplanÄ±r
    - Ã–nceki tokenlar iÃ§in Key/Value sabittir
    - Her adÄ±mda yeniden hesaplamak gereksiz
    
    Cache ile:
    Token 1: Q1, K1, V1 hesapla â†’ Cache'e kaydet
    Token 2: Q2 hesapla, K1,V1'i cache'ten al
    Token 3: Q3 hesapla, K1,V1,K2,V2'yi cache'ten al
    ...
    
    PERFORMANS ETKÄ°SÄ°:
    -----------------
    100 token Ã¼retimi:
    - KV-Cache YOK: ~5.2 saniye
    - KV-Cache VAR: ~1.3 saniye (4x hÄ±zlÄ±!)
    
    BELLEK KULLANIMI:
    ----------------
    Cache boyutu: batch_size Ã— seq_length Ã— hidden_size Ã— num_layers Ã— 2
    
    Ã–rnek: Batch=4, Seq=512, Hidden=4096, Layers=32
    Cache = 4 Ã— 512 Ã— 4096 Ã— 32 Ã— 2 Ã— 2 bytes = 1 GB
    
    OTOMATIK KULLANIM:
    -----------------
    Hugging Face transformers'da varsayÄ±lan olarak aktif!
    outputs = model.generate(input_ids, use_cache=True)  # Default
    """)
    
    # ========================================================================
    # 4. ONNX EXPORT (Model Format Optimizasyonu)
    # ========================================================================
    print("\n4ï¸âƒ£ ONNX EXPORT (Open Neural Network Exchange)")
    print("-" * 80)
    print("""
    PyTorch modelini optimize edilmiÅŸ ONNX formatÄ±na Ã§evirme.
    
    ONNX NEDÄ°R?
    ----------
    - Framework-agnostic model formatÄ±
    - Inference iÃ§in optimize edilmiÅŸ
    - FarklÄ± platformlarda Ã§alÄ±ÅŸÄ±r
    
    AVANTAJLARI:
    -----------
    âœ“ %20-50 daha hÄ±zlÄ± inference
    âœ“ Daha dÃ¼ÅŸÃ¼k bellek kullanÄ±mÄ±
    âœ“ Cross-platform uyumluluk
    âœ“ Mobile/Edge device desteÄŸi
    
    KULLANIM:
    --------
    # PyTorch'tan ONNX'e dÃ¶nÃ¼ÅŸtÃ¼rme
    torch.onnx.export(
        model, 
        dummy_input,
        "model.onnx",
        opset_version=14
    )
    
    # ONNX Runtime ile inference
    import onnxruntime as ort
    session = ort.InferenceSession("model.onnx")
    outputs = session.run(None, {"input": input_data})
    """)
    
    # ========================================================================
    # 5. TENSORRT (NVIDIA GPU Optimizasyonu)
    # ========================================================================
    print("\n5ï¸âƒ£ TENSORRT (NVIDIA GPU Optimizasyonu)")
    print("-" * 80)
    print("""
    NVIDIA TensorRT: GPU'larda ultra-hÄ±zlÄ± inference iÃ§in Ã¶zel optimizasyon.
    
    OPTÄ°MÄ°ZASYONLAR:
    ---------------
    â€¢ Kernel Fusion: Birden fazla operasyonu birleÅŸtirme
    â€¢ Precision Calibration: Otomatik quantization
    â€¢ Layer & Tensor Fusion: Gereksiz iÅŸlemleri birleÅŸtirme
    â€¢ Dynamic Tensor Memory: Bellek optimizasyonu
    
    PERFORMANS Ä°YÄ°LEÅMESÄ°:
    --------------------
    Standard PyTorch: 100 ms/inference
    TensorRT FP16:     25 ms/inference (4x hÄ±zlÄ±)
    TensorRT INT8:     15 ms/inference (6.6x hÄ±zlÄ±)
    
    KULLANIM:
    --------
    # TensorRT ile optimizasyon
    import tensorrt as trt
    # Model'i TensorRT engine'e dÃ¶nÃ¼ÅŸtÃ¼r
    # (DetaylÄ± implementasyon gerekli)
    
    NOT: NVIDIA GPU gerektirir (Tesla, A100, H100 vb.)
    """)
    
    # ========================================================================
    # 6. DYNAMIC BATCHING (Dinamik Toplu Ä°ÅŸleme)
    # ========================================================================
    print("\n6ï¸âƒ£ DYNAMIC BATCHING (Dinamik Toplu Ä°ÅŸleme)")
    print("-" * 80)
    print("""
    FarklÄ± uzunluktaki sequence'leri akÄ±llÄ±ca gruplandÄ±rma.
    
    PROBLEM:
    -------
    Batch iÃ§inde farklÄ± uzunluklar:
    - Seq 1: 10 tokens
    - Seq 2: 100 tokens
    - Seq 3: 50 tokens
    
    Ã‡Ã¶zÃ¼m: Padding â†’ 100 token'e tamamla
    SonuÃ§: %50 gereksiz hesaplama
    
    DYNAMIC BATCHING Ã‡Ã–ZÃœMÃœ:
    -----------------------
    1. Benzer uzunluktaki sequence'leri grupla
       Batch 1: [10, 12, 15] â†’ Max 15 token
       Batch 2: [95, 100, 98] â†’ Max 100 token
    
    2. Her batch iÃ§in optimal padding
       â†’ %80 hesaplama tasarrufu
    
    KULLANIM ALANLARI:
    -----------------
    â€¢ Production API servisleri
    â€¢ Real-time inference sistemleri
    â€¢ High-throughput uygulamalar
    
    Ã–RNEK KÃœTÃœPHANE:
    ---------------
    NVIDIA Triton Inference Server
    TorchServe
    Ray Serve
    """)
    
    # ========================================================================
    # Ã–ZET ve Ã–NERÄ°LER
    # ========================================================================
    print("\n" + "=" * 80)
    print("ğŸ’¡ OPTÄ°MÄ°ZASYON Ã–NERÄ°LERÄ°")
    print("=" * 80)
    print("""
    SENARYOYA GÃ–RE SEÃ‡Ä°M:
    --------------------
    
    ğŸ–¥ï¸ Sunucu (Server) Deployment:
       â†’ FP16 Quantization
       â†’ Batch Processing (batch_size=8-32)
       â†’ KV-Cache aktif
       â†’ TensorRT (NVIDIA GPU varsa)
    
    ğŸ“± Mobil/Edge Device:
       â†’ INT8 veya INT4 Quantization
       â†’ ONNX Runtime
       â†’ Model pruning/distillation
       â†’ KÃ¼Ã§Ã¼k model seÃ§imi
    
    âš¡ Real-time Uygulamalar:
       â†’ FP16 Quantization
       â†’ KÃ¼Ã§Ã¼k batch size (1-4)
       â†’ KV-Cache aktif
       â†’ GPU inference
    
    ğŸ’° Maliyet Optimizasyonu:
       â†’ INT8 Quantization
       â†’ Batch Processing (bÃ¼yÃ¼k batch)
       â†’ CPU inference
       â†’ Model sharing/multiplexing
    """)
    
    print("=" * 80)
    print("âœ… Optimizasyon teknikleri demonstrasyonu tamamlandÄ±!\n")


# ============================================================================
# QUANTIZED MODEL YÃœKLEME
# ============================================================================

def load_quantized_model(model_path, quantization="8bit"):
    """
    Quantized (Nicelenmis) Model YÃ¼kleme
    ====================================
    
    Bu fonksiyon, modeli farklÄ± quantization seviyeleriyle yÃ¼kler.
    Quantization, model boyutunu ve inference sÃ¼resini Ã¶nemli Ã¶lÃ§Ã¼de azaltÄ±r.
    
    QUANTIZATION NEDÄ°R?
    ------------------
    Normal bir model, her parametreyi 32-bit floating point (FP32) olarak saklar.
    Quantization, bu deÄŸerleri daha dÃ¼ÅŸÃ¼k bit geniÅŸliÄŸine dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r:
    
    FP32 â†’ FP16: YarÄ± hassasiyet (50% boyut azaltma)
    FP32 â†’ INT8: 8-bit integer (75% boyut azaltma)
    FP32 â†’ INT4: 4-bit integer (87.5% boyut azaltma)
    
    MATEMATÄ°KSEL Ã–RNEK:
    ------------------
    FP32: -3.14159265... (32 bit)
    FP16: -3.141      (16 bit)
    INT8: -100         (8 bit, -128 ile 127 arasÄ±)
    INT4: -8           (4 bit, -8 ile 7 arasÄ±)
    
    PARAMETRELER:
    -------------
    model_path : str
        Model dosya yolu veya Hugging Face model ID
        
    quantization : str, default="8bit"
        Quantization seviyesi:
        - "8bit": INT8 quantization (Ã¶nerilen, dengeli)
        - "4bit": INT4 quantization (ekstrem boyut azaltma)
        - "fp16": FP16 (yarÄ± hassasiyet)
        - None: Standard yÃ¼kleme (FP32)
    
    DÃ–NÃœÅ DEÄERÄ°:
    ------------
    tuple: (model, tokenizer)
        Quantized model ve tokenizer
    
    PERFORMANS KARÅILAÅTIRMASI:
    --------------------------
    Ã–rnek: 7B parametre model
    
    â”‚ Format â”‚ Boyut  â”‚ HÄ±z     â”‚ Kalite â”‚ Bellek â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ FP32   â”‚ 28 GB  â”‚ 1.0x    â”‚ 100%   â”‚ 28 GB  â”‚
    â”‚ FP16   â”‚ 14 GB  â”‚ 1.8x    â”‚ 99.9%  â”‚ 14 GB  â”‚
    â”‚ INT8   â”‚ 7 GB   â”‚ 2.5x    â”‚ 98-99% â”‚ 7 GB   â”‚
    â”‚ INT4   â”‚ 3.5 GB â”‚ 3.0x    â”‚ 95-97% â”‚ 3.5 GB â”‚
    """
    
    print("=" * 70)
    print(f"ğŸ”¢ Model Quantization: {quantization.upper()}")
    print("=" * 70)
    
    # ========================================================================
    # QUANTIZATION SEÃ‡Ä°MÄ°
    # ========================================================================
    
    if quantization == "8bit":
        # ====================================================================
        # INT8 QUANTIZATION (Ã–NERÄ°LEN)
        # ====================================================================
        print("ğŸ“¦ INT8 Quantization kullanÄ±lÄ±yor...")
        print("-" * 70)
        print("""
        INT8 QUANTIZATION Ã–ZELLÄ°KLERÄ°:
        -----------------------------
        âœ“ %75 boyut azaltma (FP32'ye gÃ¶re)
        âœ“ %98-99 model kalitesi korunur
        âœ“ 2-3x daha hÄ±zlÄ± inference
        âœ“ CPU ve GPU'da Ã§alÄ±ÅŸÄ±r
        
        KULLANIM ALANLARI:
        - Production deployment
        - Orta Ã¶lÃ§ekli sistemler
        - Maliyet/performans dengesi
        """)
        
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            load_in_8bit=True,          # 8-bit quantization aktif
            device_map="auto",           # Otomatik cihaz yerleÅŸimi
            torch_dtype=torch.float16   # Compute dtype (hesaplama iÃ§in)
        )
        print("âœ… INT8 model baÅŸarÄ±yla yÃ¼klendi")
        
    elif quantization == "4bit":
        # ====================================================================
        # INT4 QUANTIZATION (EKSTREM BOYUT AZALTMA)
        # ====================================================================
        print("ğŸ“¦ INT4 Quantization kullanÄ±lÄ±yor...")
        print("-" * 70)
        print("""
        INT4 QUANTIZATION Ã–ZELLÄ°KLERÄ°:
        -----------------------------
        âœ“ %87.5 boyut azaltma (FP32'ye gÃ¶re)
        âœ“ %95-97 model kalitesi
        âœ“ 3-4x daha hÄ±zlÄ± inference
        âœ“ Mobil cihazlarda Ã§alÄ±ÅŸabilir
        
        KULLANIM ALANLARI:
        - Ã‡ok bÃ¼yÃ¼k modeller (70B+)
        - SÄ±nÄ±rlÄ± bellek ortamlarÄ±
        - Edge deployment
        
        GEREKSINIMLER:
        - bitsandbytes kÃ¼tÃ¼phanesi
        - CUDA desteÄŸi (Ã¶nerilen)
        """)
        
        # bitsandbytes kÃ¼tÃ¼phanesi gerekli
        try:
            from transformers import BitsAndBytesConfig
            
            # 4-bit quantization konfigÃ¼rasyonu
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,                      # 4-bit yÃ¼kleme
                bnb_4bit_compute_dtype=torch.float16,   # Hesaplama iÃ§in FP16 kullan
                bnb_4bit_use_double_quant=True,         # Ã‡ift quantization (daha iyi kalite)
                bnb_4bit_quant_type="nf4"               # NormalFloat4 (Ã¶zel 4-bit format)
            )
            
            model = AutoModelForCausalLM.from_pretrained(
                model_path,
                quantization_config=quantization_config,
                device_map="auto"
            )
            print("âœ… INT4 model baÅŸarÄ±yla yÃ¼klendi")
            
        except ImportError:
            print("âŒ HATA: bitsandbytes kÃ¼tÃ¼phanesi bulunamadÄ±!")
            print("   Kurulum: pip install bitsandbytes")
            print("   Fallback: FP16 ile yÃ¼kleniyor...")
            
            model = AutoModelForCausalLM.from_pretrained(
                model_path,
                device_map="auto",
                torch_dtype=torch.float16
            )
    
    elif quantization == "fp16":
        # ====================================================================
        # FP16 (YARI HASSASÄ°YET)
        # ====================================================================
        print("ğŸ“¦ FP16 (Half Precision) kullanÄ±lÄ±yor...")
        print("-" * 70)
        print("""
        FP16 Ã–ZELLÄ°KLERÄ°:
        ----------------
        âœ“ %50 boyut azaltma
        âœ“ %99.9 model kalitesi
        âœ“ 1.5-2x daha hÄ±zlÄ±
        âœ“ Modern GPU'larda optimize
        
        KULLANIM ALANLARI:
        - GPU inference
        - YÃ¼ksek kalite gereksinimi
        - Modern donanÄ±m
        """)
        
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            device_map="auto",
            torch_dtype=torch.float16    # FP16 precision
        )
        print("âœ… FP16 model baÅŸarÄ±yla yÃ¼klendi")
        
    else:
        # ====================================================================
        # STANDARD YÃœKLEME (FP32)
        # ====================================================================
        print("ğŸ“¦ Standard (FP32) yÃ¼kleme...")
        print("-" * 70)
        print("""
        STANDARD YÃœKLEME:
        ----------------
        âœ“ Maksimum kalite
        âœ— BÃ¼yÃ¼k boyut
        âœ— YavaÅŸ inference
        
        KULLANIM ALANLARI:
        - AraÅŸtÄ±rma
        - Model geliÅŸtirme
        - Kalite benchmark
        """)
        
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            device_map="auto",
            torch_dtype="auto"    # Otomatik dtype seÃ§imi
        )
        print("âœ… Model baÅŸarÄ±yla yÃ¼klendi")
    
    # ========================================================================
    # TOKENIZER YÃœKLEME
    # ========================================================================
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    
    # Pad token ayarla (gerekirse)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        print("âš™ï¸ Pad token ayarlandÄ±")
    
    # ========================================================================
    # MODEL Ä°STATÄ°STÄ°KLERÄ°
    # ========================================================================
    print("\n" + "=" * 70)
    print("ğŸ“Š MODEL BÄ°LGÄ°LERÄ°")
    print("=" * 70)
    
    # Parametre sayÄ±sÄ±
    total_params = sum(p.numel() for p in model.parameters())
    print(f"ğŸ“ˆ Toplam Parametre: {total_params:,}")
    
    # Bellek kullanÄ±mÄ± (tahmini)
    if quantization == "8bit":
        memory_gb = total_params * 1 / 1e9  # 1 byte per param
    elif quantization == "4bit":
        memory_gb = total_params * 0.5 / 1e9  # 0.5 byte per param
    elif quantization == "fp16":
        memory_gb = total_params * 2 / 1e9  # 2 bytes per param
    else:
        memory_gb = total_params * 4 / 1e9  # 4 bytes per param
    
    print(f"ğŸ’¾ Tahmini Bellek: {memory_gb:.2f} GB")
    
    # GPU bilgisi
    if torch.cuda.is_available():
        print(f"ğŸ–¥ï¸ Cihaz: CUDA (GPU)")
        print(f"ğŸ“ GPU: {torch.cuda.get_device_name(0)}")
    else:
        print(f"ğŸ–¥ï¸ Cihaz: CPU")
    
    print("=" * 70)
    print("âœ… Quantized model hazÄ±r!\n")
    
    return model, tokenizer


# ============================================================================
# KÄ°ÅÄ°SELLEÅTÄ°RÄ°LMÄ°Å CHATBOT OLUÅTURMA
# ============================================================================

def create_personalized_chatbot():
    """
    KiÅŸiselleÅŸtirilmiÅŸ Chatbot Demonstrasyonu
    ==========================================
    
    Bu fonksiyon, kullanÄ±cÄ± profiline gÃ¶re Ã¶zelleÅŸtirilmiÅŸ yanÄ±tlar Ã¼reten
    bir chatbot sisteminin nasÄ±l tasarlanacaÄŸÄ±nÄ± gÃ¶sterir.
    
    KÄ°ÅÄ°SELLEÅTÄ°RME NEDÄ°R?
    ----------------------
    AynÄ± soru iÃ§in farklÄ± kullanÄ±cÄ±lara farklÄ± yanÄ±tlar vermek:
    
    Ã–rnek Soru: "Python nasÄ±l Ã¶ÄŸrenilir?"
    
    Yeni BaÅŸlayan â†’ "Python'a temellerden baÅŸlayalÄ±m..."
    Ä°leri Seviye â†’ "Advanced Python konularÄ±na odaklanalÄ±m..."
    Ã‡ocuk        â†’ "Python oyun gibi eÄŸlenceli bir dil..."
    
    KÄ°ÅÄ°SELLEÅTÄ°RME FAKTÃ–RLERÄ°:
    ---------------------------
    â€¢ KullanÄ±cÄ± Profili: Ä°sim, yaÅŸ, deneyim seviyesi
    â€¢ Ä°lgi AlanlarÄ±: Teknoloji, spor, sanat vb.
    â€¢ KonuÅŸma Stili: Formal, samimi, teknik
    â€¢ GeÃ§miÅŸ EtkileÅŸimler: Ã–nceki konuÅŸmalar, tercihler
    â€¢ BaÄŸlam: Zaman, yer, cihaz
    
    UYGULAMA ALANLARI:
    -----------------
    âœ“ E-ticaret: KiÅŸisel Ã¼rÃ¼n Ã¶nerileri
    âœ“ EÄŸitim: Seviyeye gÃ¶re iÃ§erik
    âœ“ SaÄŸlÄ±k: KiÅŸisel saÄŸlÄ±k tavsiyeleri
    âœ“ MÃ¼ÅŸteri Hizmetleri: Ã–zelleÅŸtirilmiÅŸ destek
    """
    
    print("\n" + "=" * 80)
    print("ğŸ¤– KÄ°ÅÄ°SELLEÅTÄ°RÄ°LMÄ°Å CHATBOT SÄ°STEMÄ°")
    print("=" * 80)
    
    # ========================================================================
    # KULLANICI PROFÄ°L TANIMI
    # ========================================================================
    print("\n1ï¸âƒ£ KULLANICI PROFÄ°LÄ° OLUÅTURMA")
    print("-" * 80)
    
    # Ã–rnek kullanÄ±cÄ± profili
    user_profile = {
        "name": "Ali",                              # KullanÄ±cÄ± adÄ±
        "age": 25,                                  # YaÅŸ
        "interests": ["teknoloji", "yapay zeka", "python"],  # Ä°lgi alanlarÄ±
        "expertise_level": "intermediate",          # Deneyim seviyesi
        "learning_style": "hands-on",               # Ã–ÄŸrenme stili
        "preferred_language": "Turkish",            # Tercih edilen dil
        "tone": "samimi ve yardÄ±msever",           # KonuÅŸma tonu
        "background": "bilgisayar mÃ¼hendisliÄŸi",   # Arka plan
    }
    
    print("Ã–rnek KullanÄ±cÄ± Profili:")
    print(json.dumps(user_profile, indent=2, ensure_ascii=False))
    
    # ========================================================================
    # KÄ°ÅÄ°SELLEÅTÄ°RÄ°LMÄ°Å PROMPT OLUÅTURMA
    # ========================================================================
    print("\n2ï¸âƒ£ KÄ°ÅÄ°SELLEÅTÄ°RÄ°LMÄ°Å PROMPT ENGINEERÄ°NG")
    print("-" * 80)
    
    def create_personalized_prompt(user_input, profile):
        """
        KullanÄ±cÄ± Profiline GÃ¶re Prompt OluÅŸturma
        =========================================
        
        Bu fonksiyon, kullanÄ±cÄ± bilgilerini kullanarak
        modele verilecek prompt'u dinamik olarak oluÅŸturur.
        
        PROMPT YAPISI:
        -------------
        1. Sistem TalimatlarÄ± (System Instructions)
        2. KullanÄ±cÄ± Profil Bilgileri
        3. BaÄŸlam (Context)
        4. KullanÄ±cÄ± Sorusu
        5. Format TalimatlarÄ±
        
        Args:
            user_input (str): KullanÄ±cÄ±nÄ±n sorusu/mesajÄ±
            profile (dict): KullanÄ±cÄ± profil bilgileri
        
        Returns:
            str: KiÅŸiselleÅŸtirilmiÅŸ prompt
        """
        
        # PROMPT TEMPLATE (Åablon)
        prompt = f"""Sen deneyimli bir AI asistanÄ±sÄ±n. AÅŸaÄŸÄ±daki kullanÄ±cÄ± profiline gÃ¶re yanÄ±t ver.

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                     KULLANICI PROFÄ°LÄ°                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ‘¤ Ä°sim: {profile['name']}
ğŸ‚ YaÅŸ: {profile['age']}
ğŸ“š Deneyim Seviyesi: {profile['expertise_level']}
ğŸ’¡ Ä°lgi AlanlarÄ±: {', '.join(profile['interests'])}
ğŸ¯ Ã–ÄŸrenme Stili: {profile['learning_style']}
ğŸ—£ï¸ KonuÅŸma Tonu: {profile['tone']}
ğŸ“ Arka Plan: {profile['background']}

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                      YANIT TALÄ°MATLARI                          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. KullanÄ±cÄ±nÄ±n deneyim seviyesine uygun aÃ§Ä±klama yap
2. Ä°lgi alanlarÄ±na referans ver
3. Belirtilen konuÅŸma tonunu kullan
4. Pratik Ã¶rnekler ver (hands-on Ã¶ÄŸrenme stili iÃ§in)
5. TÃ¼rkÃ§e olarak yanÄ±tla

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                      KULLANICI SORUSU                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

{profile['name']}: {user_input}

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                        AI YANITI                                â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Asistan:"""
        
        return prompt
    
    # ========================================================================
    # Ã–RNEK KONUÅMALAR
    # ========================================================================
    print("\n3ï¸âƒ£ Ã–RNEK KÄ°ÅÄ°SELLEÅTÄ°RÄ°LMÄ°Å KONUÅMALAR")
    print("-" * 80)
    
    # FarklÄ± kullanÄ±cÄ± sorularÄ±
    user_inputs = [
        "Merhaba! Python Ã¶ÄŸrenmek istiyorum, nereden baÅŸlamalÄ±yÄ±m?",
        "Makine Ã¶ÄŸrenmesi iÃ§in hangi kÃ¼tÃ¼phaneleri Ã¶nerirsin?",
        "Yapay zeka projelerinde kariyer yapmak istiyorum, tavsiyelen neler?"
    ]
    
    for idx, user_input in enumerate(user_inputs, 1):
        print(f"\nğŸ“ Ã–RNEK {idx}")
        print("â”€" * 80)
        
        # KiÅŸiselleÅŸtirilmiÅŸ prompt oluÅŸtur
        personalized_prompt = create_personalized_prompt(user_input, user_profile)
        
        print("OLUÅTURULAN PROMPT:")
        print("â”Œ" + "â”€" * 78 + "â”")
        for line in personalized_prompt.split('\n'):
            print(f"â”‚ {line:<76} â”‚")
        print("â””" + "â”€" * 78 + "â”˜")
        
        # NOT: GerÃ§ek inference burada yapÄ±lÄ±r
        print("\nğŸ’¬ BEKLENENbir YANIT Ã–RNEÄÄ°:")
        print("â”Œ" + "â”€" * 78 + "â”")
        
        if idx == 1:
            sample_response = f"""Merhaba {user_profile['name']}! Python Ã¶ÄŸrenmeye baÅŸlamak harika bir karar! ğŸ‰
            
Senin iÃ§in intermediate seviyeye yÃ¶nelik bir yol haritasÄ± hazÄ±rladÄ±m:

1ï¸âƒ£ Temel KonularÄ± PekiÅŸtir:
   - Data structures (list, dict, set, tuple)
   - OOP concepts (class, inheritance, polymorphism)
   - File handling ve error handling

2ï¸âƒ£ Yapay Zeka OdaklÄ± Ä°lerle (ilgi alanÄ±na uygun):
   - NumPy: SayÄ±sal hesaplamalar
   - Pandas: Veri analizi
   - Matplotlib/Seaborn: GÃ¶rselleÅŸtirme

3ï¸âƒ£ Pratik Projeler Yap (hands-on stili):
   - Kendi veri setinle analiz yap
   - Simple ML modeli kur
   - GitHub'da paylaÅŸ

BaÅŸlangÄ±Ã§ iÃ§in Ã¶nereceÄŸim kaynak: "Python for Data Science" kitabÄ±."""
            
        elif idx == 2:
            sample_response = f"""Harika soru {user_profile['name']}! Makine Ã¶ÄŸrenmesi iÃ§in temel kÃ¼tÃ¼phaneler:

ğŸ”§ Temel Stack:
   â€¢ NumPy: Matematiksel iÅŸlemler
   â€¢ Pandas: Veri manipÃ¼lasyonu
   â€¢ Scikit-learn: ML algoritmalarÄ±

ğŸ§  Deep Learning:
   â€¢ TensorFlow: Google'Ä±n framework'Ã¼
   â€¢ PyTorch: AraÅŸtÄ±rma ve production
   â€¢ Keras: Kolay API

ğŸ“Š GÃ¶rselleÅŸtirme:
   â€¢ Matplotlib: Temel grafikler
   â€¢ Seaborn: Ä°statistiksel vizler
   â€¢ Plotly: Ä°nteraktif dashboard

Senin intermediate seviyende olduÄŸun iÃ§in, Ã¶nce Scikit-learn ile baÅŸla,
sonra PyTorch'a geÃ§. Yapay zeka ilgine uygun! ğŸš€"""
            
        else:
            sample_response = f"""Yapay zeka kariyeri iÃ§in sÃ¼per bir hedef {user_profile['name']}! ğŸ’¼

Tavsiyelerim:

1ï¸âƒ£ Teknik Beceriler:
   âœ“ Python + ML kÃ¼tÃ¼phaneleri (zaten ilgileniyorsun!)
   âœ“ Deep Learning framework'leri
   âœ“ MLOps tools (Docker, Kubernetes)

2ï¸âƒ£ Pratik Deneyim:
   âœ“ Kaggle competitions
   âœ“ Open-source contribute
   âœ“ Kendi projelerini GitHub'da yayÄ±nla

3ï¸âƒ£ Network:
   âœ“ LinkedIn'de AI community'lerine katÄ±l
   âœ“ Meetup'lara git
   âœ“ Blog yaz (deneyimlerini paylaÅŸ)

Bilgisayar mÃ¼hendisliÄŸi arka planÄ±n Ã§ok avantaj! Teorik bilgin var,
ÅŸimdi pratik yaparak portfÃ¶y oluÅŸtur. ğŸ¯"""
        
        for line in sample_response.split('\n'):
            print(f"â”‚ {line:<76} â”‚")
        print("â””" + "â”€" * 78 + "â”˜")
    
    # ========================================================================
    # GELÄ°ÅMÄ°Å KÄ°ÅÄ°SELLEÅTÄ°RME TEKNÄ°KLERÄ°
    # ========================================================================
    print("\n" + "=" * 80)
    print("4ï¸âƒ£ GELÄ°ÅMÄ°Å KÄ°ÅÄ°SELLEÅTÄ°RME TEKNÄ°KLERÄ°")
    print("=" * 80)
    
    advanced_techniques = {
        "ğŸ§  Conversation Memory": """
            Ã–nceki konuÅŸmalarÄ± hatÄ±rlama ve referans verme
            
            Ã–rnek:
            User: "DÃ¼n bahsettiÄŸin NumPy kÃ¼tÃ¼phanesini kurmaya Ã§alÄ±ÅŸtÄ±m"
            Bot: "Harika! NumPy kurulumunda sorun yaÅŸadÄ±n mÄ±? GeÃ§en 
                  konuÅŸmamÄ±zda pandas ile beraber kullanacaÄŸÄ±nÄ± sÃ¶ylemiÅŸtin."
        """,
        
        "ğŸ¯ Adaptive Difficulty": """
            KullanÄ±cÄ±nÄ±n ilerlemesine gÃ¶re zorluk seviyesi ayarlama
            
            Ä°lk gÃ¼n: "Liste nedir? Bir dizi veridir..."
            2. hafta: "List comprehension ile daha verimli..."
            1. ay: "Generator expressions ve memory optimization..."
        """,
        
        "ğŸ“Š Preference Learning": """
            KullanÄ±cÄ± tercihlerini Ã¶ÄŸrenme
            
            GÃ¶zlem: KullanÄ±cÄ± hep kod Ã¶rnekleri istiyor
            Adaptasyon: AÃ§Ä±klamalara otomatik kod snippet ekle
            
            GÃ¶zlem: Video kaynak linklerini aÃ§mÄ±yor
            Adaptasyon: YazÄ±lÄ± kaynaklarÄ± Ã¶nceliklendir
        """,
        
        "ğŸŒ Contextual Awareness": """
            BaÄŸlamsal farkÄ±ndalÄ±k
            
            Sabah: "GÃ¼naydÄ±n! BugÃ¼n yeni bir konu Ã¶ÄŸrenmeye hazÄ±r mÄ±sÄ±n?"
            AkÅŸam: "BugÃ¼n Ã¶ÄŸrendiklerini pekiÅŸtirmek iÃ§in pratik yapalÄ±m"
            
            Hafta iÃ§i: KÄ±sa, hÄ±zlÄ± yanÄ±tlar
            Hafta sonu: DetaylÄ±, kapsamlÄ± aÃ§Ä±klamalar
        """,
        
        "ğŸ’¬ Sentiment Analysis": """
            KullanÄ±cÄ± duygusunu anlama ve buna gÃ¶re yanÄ±t verme
            
            Pozitif: "Harika! Enerjini seviyorum! Devam edelim ğŸš€"
            Negatif: "AnlÄ±yorum, biraz zorlandÄ±n. AdÄ±m adÄ±m gidelim ğŸ’ª"
            Confused: "KarÄ±ÅŸÄ±k geldi galiba, farklÄ± anlatayÄ±m ğŸ¤”"
        """
    }
    
    for technique, description in advanced_techniques.items():
        print(f"\n{technique}")
        print("â”€" * 80)
        print(description.strip())
    
    # ========================================================================
    # IMPLEMENTATION NOTES
    # ========================================================================
    print("\n" + "=" * 80)
    print("ğŸ’¡ UYGULAMA NOTLARI")
    print("=" * 80)
    print("""
    KÄ°ÅÄ°SELLEÅTÄ°RME Ä°Ã‡Ä°N GEREKLÄ°LER:
    --------------------------------
    
    1. ğŸ—„ï¸ KullanÄ±cÄ± VeritabanÄ±:
       - Profil bilgileri
       - KonuÅŸma geÃ§miÅŸi
       - Tercihler ve ayarlar
       
    2. ğŸ”’ Gizlilik ve GÃ¼venlik:
       - GDPR/KVKK uyumluluÄŸu
       - Veri ÅŸifreleme
       - KullanÄ±cÄ± onayÄ±
       
    3. ğŸ“ˆ Analytics ve Tracking:
       - KullanÄ±cÄ± davranÄ±ÅŸ analizi
       - A/B testing
       - Performans metrikleri
       
    4. ğŸ”„ SÃ¼rekli Ä°yileÅŸtirme:
       - KullanÄ±cÄ± geri bildirimleri
       - Model fine-tuning
       - Prompt optimization
    """)
    
    print("=" * 80)
    print("âœ… KiÅŸiselleÅŸtirilmiÅŸ chatbot demonstrasyonu tamamlandÄ±!\n")


# ============================================================================
# GENERATION CONFIGURATION (Ãœretim YapÄ±landÄ±rmasÄ±)
# ============================================================================

def demonstrate_generation_config():
    """
    Generation Configuration Parametreleri
    =======================================
    
    Bu fonksiyon, farklÄ± kullanÄ±m senaryolarÄ± iÃ§in optimal text generation
    parametrelerini gÃ¶sterir.
    
    GENERATION PARAMETRELERI NEDÄ°R?
    -------------------------------
    Model metni Ã¼retirken birÃ§ok parametre ile kontrol edilir:
    
    â€¢ temperature: Randomness (rastgelelik) seviyesi
    â€¢ top_p: Nucleus sampling eÅŸiÄŸi
    â€¢ top_k: En yÃ¼ksek k olasÄ± token
    â€¢ repetition_penalty: Tekrar cezasÄ±
    â€¢ max_tokens: Maksimum token sayÄ±sÄ±
    â€¢ do_sample: Sampling aktif/pasif
    
    PARAMETRELER NASIL ETKÄ°LER?
    ---------------------------
    
    ğŸŒ¡ï¸ TEMPERATURE (0.0 - 2.0):
    ---------------------------
    DÃ¼ÅŸÃ¼k (0.0-0.3):
        âœ“ TutarlÄ±, Ã¶ngÃ¶rÃ¼lebilir
        âœ“ FaktÃ¼el doÄŸruluk
        âœ— TekrarlayÄ±cÄ± olabilir
        KullanÄ±m: Teknik dÃ¶kÃ¼mantasyon, QA
    
    Orta (0.5-0.8):
        âœ“ Dengeli
        âœ“ YaratÄ±cÄ± ama tutarlÄ±
        KullanÄ±m: Genel amaÃ§lÄ± chatbot
    
    YÃ¼ksek (0.9-2.0):
        âœ“ Ã‡ok yaratÄ±cÄ±
        âœ“ Ã‡eÅŸitli Ã§Ä±ktÄ±lar
        âœ— TutarsÄ±z olabilir
        KullanÄ±m: Hikaye, ÅŸiir, brainstorming
    
    ğŸ¯ TOP_P (0.0 - 1.0) - Nucleus Sampling:
    ---------------------------------------
    Top-p, kÃ¼mÃ¼latif olasÄ±lÄ±k eÅŸiÄŸidir.
    
    NasÄ±l Ã‡alÄ±ÅŸÄ±r?
    - Tokenlar olasÄ±lÄ±ÄŸa gÃ¶re sÄ±ralanÄ±r
    - KÃ¼mÃ¼latif toplam p'yi geÃ§ene kadar eklenir
    - Sadece bu set'ten Ã¶rnekleme yapÄ±lÄ±r
    
    Ã–rnek (top_p=0.9):
        Token A: 40% â†’ KÃ¼mÃ¼latif: 40%
        Token B: 30% â†’ KÃ¼mÃ¼latif: 70%
        Token C: 20% â†’ KÃ¼mÃ¼latif: 90% â† buraya kadar
        Token D: 10% â†’ (dahil edilmez)
    
    DÃ¼ÅŸÃ¼k (0.1-0.5): Sadece en olasÄ± tokenlar
    YÃ¼ksek (0.9-1.0): Daha fazla Ã§eÅŸitlilik
    
    ğŸ”¢ TOP_K (1 - 100):
    -------------------
    En yÃ¼ksek k olasÄ± token arasÄ±ndan seÃ§.
    
    top_k=1: Her zaman en olasÄ± token (greedy)
    top_k=10: En olasÄ± 10 token arasÄ±ndan Ã¶rnekle
    top_k=50: Daha fazla Ã§eÅŸitlilik
    
    ğŸ” REPETITION_PENALTY (1.0 - 2.0):
    ---------------------------------
    Tekrarlayan tokenlarÄ± cezalandÄ±rma.
    
    1.0: Ceza yok (default)
    1.1-1.2: Hafif ceza (Ã¶nerilen)
    1.5+: AÄŸÄ±r ceza (tekrar neredeyse imkansÄ±z)
    """
    
    print("\n" + "=" * 80)
    print("âš™ï¸ GENERATION CONFIGURATION PARAMETRELERÄ°")
    print("=" * 80)
    
    # ========================================================================
    # KULLANIM SENARYOLARINA GÃ–RE KONFÄ°GÃœRASYONLAR
    # ========================================================================
    
    configs = {
        "ğŸ“ YaratÄ±cÄ± YazarlÄ±k (Creative Writing)": {
            "temperature": 0.8,
            "top_p": 0.9,
            "top_k": 50,
            "repetition_penalty": 1.1,
            "max_new_tokens": 200,
            "do_sample": True,
            "description": """
            Hikaye, ÅŸiir, blog yazÄ±larÄ± iÃ§in ideal
            
            Ã–ZELLÄ°KLER:
            â€¢ YÃ¼ksek yaratÄ±cÄ±lÄ±k (temp=0.8)
            â€¢ GeniÅŸ token Ã§eÅŸitliliÄŸi (top_p=0.9, top_k=50)
            â€¢ TekrarlarÄ± minimize et (penalty=1.1)
            â€¢ Sampling aktif (Ã§eÅŸitli Ã§Ä±ktÄ±lar)
            
            Ã–RNEK Ã‡IKTI:
            "Bir zamanlar, uzak bir galakside, yÄ±ldÄ±zlar arasÄ±nda 
            dans eden kÃ¼Ã§Ã¼k bir robot yaÅŸardÄ±. Her gÃ¼n yeni gezegen
            keÅŸfetmek iÃ§in maceraya atÄ±lÄ±rdÄ±..."
            """
        },
        
        "â“ FaktÃ¼el Soru-Cevap (Factual QA)": {
            "temperature": 0.1,
            "top_p": 0.5,
            "top_k": 10,
            "repetition_penalty": 1.0,
            "max_new_tokens": 100,
            "do_sample": False,
            "description": """
            Teknik sorular, bilgi talebi iÃ§in ideal
            
            Ã–ZELLÄ°KLER:
            â€¢ Minimal randomness (temp=0.1)
            â€¢ DaraltÄ±lmÄ±ÅŸ token seÃ§imi (top_p=0.5, top_k=10)
            â€¢ Tekrar cezasÄ± yok (teknik terimler tekrar edilebilir)
            â€¢ Sampling kapalÄ± (deterministik)
            
            Ã–RNEK Ã‡IKTI:
            "Python, 1991 yÄ±lÄ±nda Guido van Rossum tarafÄ±ndan 
            geliÅŸtirilmiÅŸ, yÃ¼ksek seviyeli, yorumlamalÄ± bir 
            programlama dilidir. Nesne yÃ¶nelimli programlamayÄ± 
            destekler."
            """
        },
        
        "ğŸ’» Kod Ãœretimi (Code Generation)": {
            "temperature": 0.2,
            "top_p": 0.6,
            "top_k": 20,
            "repetition_penalty": 1.05,
            "max_new_tokens": 150,
            "do_sample": True,
            "description": """
            Python, JavaScript, SQL kod Ã¼retimi iÃ§in
            
            Ã–ZELLÄ°KLER:
            â€¢ DÃ¼ÅŸÃ¼k randomness (temp=0.2) - Syntax hatasÄ± Ã¶nleme
            â€¢ Orta token seÃ§imi (best practices iÃ§in)
            â€¢ Hafif tekrar cezasÄ± (loop'larda gerekli olabilir)
            â€¢ Sampling aÃ§Ä±k ama kÄ±sÄ±tlÄ±
            
            Ã–RNEK Ã‡IKTI:
            ```python
            def fibonacci(n):
                if n <= 1:
                    return n
                return fibonacci(n-1) + fibonacci(n-2)
            ```
            """
        },
        
        "ğŸ’¬ Genel Chatbot (General Conversation)": {
            "temperature": 0.7,
            "top_p": 0.85,
            "top_k": 40,
            "repetition_penalty": 1.1,
            "max_new_tokens": 120,
            "do_sample": True,
            "description": """
            GÃ¼nlÃ¼k konuÅŸmalar, genel asistan gÃ¶revleri iÃ§in
            
            Ã–ZELLÄ°KLER:
            â€¢ Dengeli randomness (temp=0.7)
            â€¢ Dengeli token seÃ§imi
            â€¢ Tekrar Ã¶nleme (monoton olmamak iÃ§in)
            â€¢ DoÄŸal konuÅŸma akÄ±ÅŸÄ±
            
            Ã–RNEK Ã‡IKTI:
            "Tabii ki yardÄ±mcÄ± olabilirim! Python Ã¶ÄŸrenmek harika 
            bir baÅŸlangÄ±Ã§. Size adÄ±m adÄ±m bir yol haritasÄ± 
            Ã§Ä±karabilirim. Ã–nce temel syntax'Ä± Ã¶ÄŸrenmenizi Ã¶neririm."
            """
        },
        
        "ğŸ“° Haber Ã–zeti (News Summary)": {
            "temperature": 0.3,
            "top_p": 0.7,
            "top_k": 25,
            "repetition_penalty": 1.2,
            "max_new_tokens": 80,
            "do_sample": True,
            "description": """
            Haber metinlerini Ã¶zetleme iÃ§in
            
            Ã–ZELLÄ°KLER:
            â€¢ DÃ¼ÅŸÃ¼k randomness (tutarlÄ± Ã¶zet)
            â€¢ Orta token seÃ§imi (Ã§eÅŸitli ifade)
            â€¢ YÃ¼ksek tekrar cezasÄ± (aynÄ± kelimeyi kullanmamak iÃ§in)
            â€¢ KÄ±sa output (Ã¶zet olmalÄ±)
            
            Ã–RNEK Ã‡IKTI:
            "BugÃ¼n yapÄ±lan toplantÄ±da ekonomik tedbirler gÃ¶rÃ¼ÅŸÃ¼ldÃ¼.
            Enflasyonla mÃ¼cadele iÃ§in yeni stratejiler belirlendi.
            Merkez BankasÄ± faiz kararÄ±nÄ± aÃ§Ä±kladÄ±."
            """
        },
        
        "ğŸ­ Karakter Dialog (Character Dialogue)": {
            "temperature": 0.9,
            "top_p": 0.95,
            "top_k": 60,
            "repetition_penalty": 1.15,
            "max_new_tokens": 180,
            "do_sample": True,
            "description": """
            RPG, oyun karakterleri, drama diyaloglarÄ± iÃ§in
            
            Ã–ZELLÄ°KLER:
            â€¢ YÃ¼ksek yaratÄ±cÄ±lÄ±k (benzersiz konuÅŸmalar)
            â€¢ GeniÅŸ token Ã§eÅŸitliliÄŸi (farklÄ± ifadeler)
            â€¢ Tekrar Ã¶nleme (doÄŸal konuÅŸma)
            â€¢ Uzun yanÄ±tlara izin ver
            
            Ã–RNEK Ã‡IKTI:
            "Ah, maceracÄ±! Sonunda geldin. Ejderhalar daÄŸlarda 
            uyanmÄ±ÅŸ, kÃ¶ylÃ¼ler korkudan titriyor. Efsanevi kÄ±lÄ±cÄ± 
            bulmak iÃ§in yola Ã§Ä±kmalÄ±yÄ±z, ama Ã¶nce hazÄ±rlÄ±k yapmalÄ±yÄ±z!"
            """
        }
    }
    
    # ========================================================================
    # KONFÄ°GÃœRASYONLARI GÃ–STER
    # ========================================================================
    
    for use_case, config in configs.items():
        print(f"\n{use_case}")
        print("=" * 80)
        
        # Parametreleri gÃ¶ster
        print("\nğŸ“Š PARAMETRELER:")
        print("-" * 80)
        
        param_descriptions = {
            "temperature": "ğŸŒ¡ï¸ Randomness Seviyesi",
            "top_p": "ğŸ¯ Nucleus Sampling",
            "top_k": "ğŸ”¢ Token Ã‡eÅŸitliliÄŸi",
            "repetition_penalty": "ğŸ” Tekrar CezasÄ±",
            "max_new_tokens": "ğŸ“ Maksimum Token",
            "do_sample": "ğŸ² Sampling Durumu"
        }
        
        for param in ["temperature", "top_p", "top_k", "repetition_penalty", 
                      "max_new_tokens", "do_sample"]:
            if param in config:
                value = config[param]
                desc = param_descriptions.get(param, param)
                print(f"  {desc:<25} : {value}")
        
        # AÃ§Ä±klamayÄ± gÃ¶ster
        print("\nğŸ“– AÃ‡IKLAMA:")
        print("-" * 80)
        print(config["description"].strip())
    
    # ========================================================================
    # PARAMETRE ETKÄ°LEÅÄ°MLERÄ°
    # ========================================================================
    
    print("\n" + "=" * 80)
    print("ğŸ”„ PARAMETRE ETKÄ°LEÅÄ°MLERÄ°")
    print("=" * 80)
    print("""
    PARAMETRE KOMBÄ°NASYONLARI:
    -------------------------
    
    1ï¸âƒ£ Deterministik Mod (Her zaman aynÄ± Ã§Ä±ktÄ±):
       temperature = 0.0
       do_sample = False
       â†’ Soru-cevap, dokÃ¼mantasyon iÃ§in
    
    2ï¸âƒ£ KontrollÃ¼ YaratÄ±cÄ±lÄ±k:
       temperature = 0.7
       top_p = 0.9
       top_k = 50
       repetition_penalty = 1.1
       â†’ Chatbot, blog yazÄ±larÄ±
    
    3ï¸âƒ£ Maksimum Ã‡eÅŸitlilik:
       temperature = 1.5
       top_p = 0.95
       top_k = 100
       repetition_penalty = 1.2
       â†’ Brainstorming, yaratÄ±cÄ± yazarlÄ±k
    
    4ï¸âƒ£ Teknik Ä°Ã§erik:
       temperature = 0.2
       top_p = 0.6
       repetition_penalty = 1.0
       â†’ Kod, teknik aÃ§Ä±klamalar
    
    âš ï¸ DÄ°KKAT:
    ----------
    â€¢ temperature=0 iken top_p ve top_k etkisizdir
    â€¢ Ã‡ok yÃ¼ksek repetition_penalty tutarsÄ±z Ã§Ä±ktÄ±lar verebilir
    â€¢ Uzun metinler iÃ§in max_tokens'Ä± artÄ±rÄ±n
    â€¢ Her model farklÄ± tepki verebilir, test edin!
    """)
    
    # ========================================================================
    # PRATIK KOD Ã–RNEÄÄ°
    # ========================================================================
    
    print("\n" + "=" * 80)
    print("ğŸ’» PRATIK KOD Ã–RNEÄÄ°")
    print("=" * 80)
    
    code_example = '''
# Generation config oluÅŸturma
from transformers import GenerationConfig

# YaratÄ±cÄ± yazarlÄ±k iÃ§in config
creative_config = GenerationConfig(
    max_new_tokens=200,
    temperature=0.8,
    top_p=0.9,
    top_k=50,
    repetition_penalty=1.1,
    do_sample=True,
    pad_token_id=tokenizer.eos_token_id
)

# FaktÃ¼el QA iÃ§in config
factual_config = GenerationConfig(
    max_new_tokens=100,
    temperature=0.1,
    top_p=0.5,
    do_sample=False,
    pad_token_id=tokenizer.eos_token_id
)

# KullanÄ±m
outputs = model.generate(
    input_ids,
    generation_config=creative_config  # veya factual_config
)

# Alternatif: DoÄŸrudan parametre
outputs = model.generate(
    input_ids,
    max_new_tokens=150,
    temperature=0.7,
    top_p=0.9,
    top_k=40,
    repetition_penalty=1.1,
    do_sample=True
)
'''
    
    print(code_example)
    
    print("=" * 80)
    print("âœ… Generation configuration demonstrasyonu tamamlandÄ±!\n")


# ============================================================================
# INFERENCE HIZ BENCHMARK
# ============================================================================

def benchmark_inference_speed():
    """
    Inference HÄ±z Benchmark ve Performans Analizi
    ==============================================
    
    Bu fonksiyon, farklÄ± optimizasyon tekniklerinin inference hÄ±zÄ±na
    etkisini gÃ¶sterir ve karÅŸÄ±laÅŸtÄ±rÄ±r.
    
    NEDEN BENCHMARK GEREKLÄ°?
    -----------------------
    Production ortamÄ±nda:
    â€¢ KullanÄ±cÄ± deneyimi: <200ms ideal
    â€¢ Throughput: Saniyede iÅŸlenebilecek istek sayÄ±sÄ±
    â€¢ Maliyet: Daha hÄ±zlÄ± = daha az sunucu
    â€¢ SLA (Service Level Agreement) gereklilikleri
    
    BENCHMARK METRÄ°KLERÄ°:
    --------------------
    â€¢ Latency: Tek bir inference sÃ¼resi (ms)
    â€¢ Throughput: Saniyede iÅŸlenebilen istek sayÄ±sÄ±
    â€¢ GPU/CPU KullanÄ±mÄ±: Kaynak verimliliÄŸi (%)
    â€¢ Memory Footprint: Bellek kullanÄ±mÄ± (GB)
    â€¢ Cost per 1M tokens: Maliyet analizi
    
    HIZ OPTÄ°MÄ°ZASYONUNU ETKÄ°LEYEN FAKTÃ–RLER:
    ---------------------------------------
    1. Model Boyutu (7B, 13B, 70B parametreler)
    2. Quantization Seviyesi (FP32, FP16, INT8, INT4)
    3. Hardware (CPU, GPU, TPU)
    4. Batch Size (1, 8, 32, 64)
    5. Sequence Length (input + output token sayÄ±sÄ±)
    """
    
    print("\n" + "=" * 80)
    print("âš¡ INFERENCE HIZ BENCHMARK")
    print("=" * 80)
    
    # ========================================================================
    # SENARYO 1: QUANTIZATION ETKÄ°SÄ° (7B Model, GPU, Batch=1, 100 tokens)
    # ========================================================================
    
    print("\nğŸ“Š SENARYO 1: QUANTIZATION ETKÄ°SÄ°NÄ°N HIZ ÃœZERÄ°NDEKÄ° ETKÄ°SÄ°")
    print("-" * 80)
    print("Test KoÅŸullarÄ±: 7B Model, NVIDIA A100 GPU, Batch Size=1, 100 token")
    print("-" * 80)
    
    scenarios_quantization = {
        "FP32 (Tam Hassasiyet)": {
            "latency_ms": 1500,
            "throughput": 0.67,      # requests/second
            "memory_gb": 28,
            "gpu_util": 85,
            "quality": 100
        },
        "FP16 (YarÄ± Hassasiyet)": {
            "latency_ms": 850,
            "throughput": 1.18,
            "memory_gb": 14,
            "gpu_util": 75,
            "quality": 99.9
        },
        "INT8 (8-bit Quantization)": {
            "latency_ms": 580,
            "throughput": 1.72,
            "memory_gb": 7,
            "gpu_util": 65,
            "quality": 98.5
        },
        "INT4 (4-bit Quantization)": {
            "latency_ms": 420,
            "throughput": 2.38,
            "memory_gb": 3.5,
            "gpu_util": 55,
            "quality": 96.0
        }
    }
    
    print(f"\n{'YÃ¶ntem':<30} {'Latency':>12} {'Throughput':>12} {'Bellek':>10} {'GPU':>8} {'Kalite':>8}")
    print(f"{'':<30} {'(ms)':>12} {'(req/s)':>12} {'(GB)':>10} {'(%)':>8} {'(%)':>8}")
    print("â”€" * 90)
    
    baseline_latency = scenarios_quantization["FP32 (Tam Hassasiyet)"]["latency_ms"]
    
    for scenario, metrics in scenarios_quantization.items():
        latency = metrics["latency_ms"]
        throughput = metrics["throughput"]
        memory = metrics["memory_gb"]
        gpu_util = metrics["gpu_util"]
        quality = metrics["quality"]
        
        speedup = baseline_latency / latency
        
        print(f"{scenario:<30} {latency:>10.0f}ms {throughput:>10.2f}  {memory:>8.1f}GB {gpu_util:>6}% {quality:>6.1f}%")
        print(f"{'':>30} {'â†‘ ' + str(round(speedup, 1)) + 'x hÄ±zlÄ±':>12} {'':>12} {'â†“ ' + str(round((1-memory/28)*100, 1)) + '% azalma':>10}")
        print()
    
    # ========================================================================
    # SENARYO 2: CPU vs GPU KARÅILAÅTIRMASI
    # ========================================================================
    
    print("\nğŸ“Š SENARYO 2: CPU vs GPU PERFORMANS KARÅILAÅTIRMASI")
    print("-" * 80)
    print("Test KoÅŸullarÄ±: 7B Model, INT8 Quantization, Batch Size=1, 100 token")
    print("-" * 80)
    
    scenarios_hardware = {
        "CPU - Intel Xeon": {
            "latency_ms": 3200,
            "cost_per_hour": 0.50,
            "throughput": 0.31
        },
        "CPU - Optimized (ONNX)": {
            "latency_ms": 1800,
            "cost_per_hour": 0.50,
            "throughput": 0.56
        },
        "GPU - NVIDIA T4": {
            "latency_ms": 850,
            "cost_per_hour": 0.95,
            "throughput": 1.18
        },
        "GPU - NVIDIA A100": {
            "latency_ms": 420,
            "cost_per_hour": 3.50,
            "throughput": 2.38
        },
        "GPU - NVIDIA H100": {
            "latency_ms": 210,
            "cost_per_hour": 8.00,
            "throughput": 4.76
        }
    }
    
    print(f"\n{'Hardware':<25} {'Latency':>12} {'Throughput':>14} {'Maliyet':>12} {'Verimlilik':>15}")
    print(f"{'':<25} {'(ms)':>12} {'(req/s)':>14} {'($/saat)':>12} {'(req/$/saat)':>15}")
    print("â”€" * 80)
    
    for hw, metrics in scenarios_hardware.items():
        latency = metrics["latency_ms"]
        cost = metrics["cost_per_hour"]
        throughput = metrics["throughput"]
        efficiency = throughput * 3600 / cost  # requests per dollar per hour
        
        print(f"{hw:<25} {latency:>10.0f}ms {throughput:>12.2f}  ${cost:>10.2f}  {efficiency:>12.0f}")
    
    # ========================================================================
    # SENARYO 3: BATCH SIZE ETKÄ°SÄ°
    # ========================================================================
    
    print("\n\nğŸ“Š SENARYO 3: BATCH SIZE'IN THROUGHPUT ÃœZERÄ°NDEKÄ° ETKÄ°SÄ°")
    print("-" * 80)
    print("Test KoÅŸullarÄ±: 7B Model, NVIDIA A100 GPU, INT8, 100 token")
    print("-" * 80)
    
    scenarios_batch = {
        "Batch Size = 1": {
            "latency_ms": 420,
            "throughput": 2.38,
            "gpu_util": 45,
            "memory_gb": 7.2
        },
        "Batch Size = 4": {
            "latency_ms": 580,
            "throughput": 6.90,      # 4 items in 580ms
            "gpu_util": 72,
            "memory_gb": 9.5
        },
        "Batch Size = 8": {
            "latency_ms": 920,
            "throughput": 8.70,      # 8 items in 920ms
            "gpu_util": 88,
            "memory_gb": 12.8
        },
        "Batch Size = 16": {
            "latency_ms": 1650,
            "throughput": 9.70,      # 16 items in 1650ms
            "gpu_util": 95,
            "memory_gb": 18.5
        },
        "Batch Size = 32": {
            "latency_ms": 3100,
            "throughput": 10.32,     # 32 items in 3100ms
            "gpu_util": 98,
            "memory_gb": 28.0
        }
    }
    
    print(f"\n{'Batch Size':<20} {'Latency':>12} {'Throughput':>14} {'GPU KullanÄ±mÄ±':>16} {'Bellek':>12}")
    print(f"{'':<20} {'(ms)':>12} {'(req/s)':>14} {'(%)':>16} {'(GB)':>12}")
    print("â”€" * 80)
    
    for batch, metrics in scenarios_batch.items():
        latency = metrics["latency_ms"]
        throughput = metrics["throughput"]
        gpu_util = metrics["gpu_util"]
        memory = metrics["memory_gb"]
        
        print(f"{batch:<20} {latency:>10.0f}ms {throughput:>12.2f}  {gpu_util:>14}%  {memory:>10.1f}GB")
    
    # ========================================================================
    # SENARYO 4: MODEL BOYUTU ETKÄ°SÄ°
    # ========================================================================
    
    print("\n\nğŸ“Š SENARYO 4: MODEL BOYUTUNUN PERFORMANSA ETKÄ°SÄ°")
    print("-" * 80)
    print("Test KoÅŸullarÄ±: NVIDIA A100 GPU, INT8, Batch Size=1, 100 token")
    print("-" * 80)
    
    scenarios_model_size = {
        "1.3B Parametreli Model": {
            "latency_ms": 85,
            "memory_gb": 1.3,
            "quality_score": 75
        },
        "7B Parametreli Model": {
            "latency_ms": 420,
            "memory_gb": 7.0,
            "quality_score": 85
        },
        "13B Parametreli Model": {
            "latency_ms": 780,
            "memory_gb": 13.0,
            "quality_score": 90
        },
        "30B Parametreli Model": {
            "latency_ms": 1850,
            "memory_gb": 30.0,
            "quality_score": 93
        },
        "70B Parametreli Model": {
            "latency_ms": 4200,
            "memory_gb": 70.0,
            "quality_score": 96
        }
    }
    
    print(f"\n{'Model':<28} {'Latency':>12} {'Bellek':>12} {'Kalite':>12}")
    print(f"{'':<28} {'(ms)':>12} {'(GB)':>12} {'(Skor)':>12}")
    print("â”€" * 70)
    
    for model, metrics in scenarios_model_size.items():
        latency = metrics["latency_ms"]
        memory = metrics["memory_gb"]
        quality = metrics["quality_score"]
        
        print(f"{model:<28} {latency:>10.0f}ms {memory:>10.1f}GB {quality:>10}/100")
    
    # ========================================================================
    # Ã–NERÄ°LER ve EN Ä°YÄ° UYGULAMALAR
    # ========================================================================
    
    print("\n" + "=" * 80)
    print("ğŸ’¡ PERFORMANS OPTÄ°MÄ°ZASYON Ã–NERÄ°LERÄ°")
    print("=" * 80)
    print("""
    HIZLI BAÅLANGIÃ‡ (Quick Wins):
    -----------------------------
    1. INT8 Quantization kullan
       â†’ 2-3x hÄ±z artÄ±ÅŸÄ±, minimal kalite kaybÄ±
    
    2. Batch Processing uygula
       â†’ Throughput'u 3-5x artÄ±rabilir
    
    3. KV-Cache'i aktif tut
       â†’ Model.generate() default olarak aÃ§Ä±k
    
    4. GPU kullan (mÃ¼mkÃ¼nse)
       â†’ CPU'ya gÃ¶re 5-10x hÄ±zlÄ±
    
    SENARYOYA GÃ–RE SEÃ‡Ä°M:
    --------------------
    
    ğŸ’° DÃ¼ÅŸÃ¼k Maliyet:
       â€¢ CPU + INT8/INT4
       â€¢ BÃ¼yÃ¼k batch size
       â€¢ KÃ¼Ã§Ã¼k model (7B)
    
    âš¡ DÃ¼ÅŸÃ¼k Latency:
       â€¢ GPU (A100/H100)
       â€¢ INT8/FP16
       â€¢ Batch size = 1-4
    
    ğŸ“ˆ YÃ¼ksek Throughput:
       â€¢ GPU + Batch size 16-32
       â€¢ INT8 quantization
       â€¢ Dynamic batching
    
    ğŸ¯ YÃ¼ksek Kalite:
       â€¢ BÃ¼yÃ¼k model (30B-70B)
       â€¢ FP16/FP32
       â€¢ GPU gerekli
    
    BENCHMARK NASIL YAPILIR:
    -----------------------
    1. Ãœretim senaryonuza benzer test verisi hazÄ±rlayÄ±n
    2. FarklÄ± konfigÃ¼rasyonlarÄ± test edin
    3. Latency, throughput, maliyet Ã¶lÃ§Ã¼n
    4. Trade-off analizi yapÄ±n
    5. Production'a en uygun olanÄ± seÃ§in
    
    Ã–RNEK KOD:
    ---------
    import time
    
    # Benchmark fonksiyonu
    def benchmark_model(model, inputs, num_runs=100):
        times = []
        for _ in range(num_runs):
            start = time.time()
            model.generate(**inputs)
            times.append(time.time() - start)
        
        avg_latency = sum(times) / len(times)
        throughput = 1 / avg_latency
        
        print(f"Avg Latency: {avg_latency*1000:.2f}ms")
        print(f"Throughput: {throughput:.2f} req/s")
    """)
    
    print("=" * 80)
    print("âœ… Inference hÄ±z benchmark demonstrasyonu tamamlandÄ±!\n")


# ============================================================================
# INFERENCE PIPELINE OLUÅTURMA
# ============================================================================

def create_inference_pipeline():
    """
    Production-Ready Inference Pipeline
    ====================================
    
    Bu fonksiyon, model inference'Ä± iÃ§in eksiksiz bir pipeline (boru hattÄ±)
    oluÅŸturmayÄ± ve farklÄ± pipeline yaklaÅŸÄ±mlarÄ±nÄ± gÃ¶sterir.
    
    PIPELINE NEDÄ°R?
    ---------------
    Inference pipeline, input'dan output'a kadar tÃ¼m adÄ±mlarÄ±
    otomatize eden bir sistemdir:
    
    Input â†’ Preprocessing â†’ Tokenization â†’ Model Inference â†’ 
    Postprocessing â†’ Output
    
    PIPELINE AVANTAJLARI:
    --------------------
    âœ“ TutarlÄ±lÄ±k: Her zaman aynÄ± adÄ±mlar
    âœ“ Hata YÃ¶netimi: Merkezi error handling
    âœ“ Logging: Her adÄ±m loglanabilir
    âœ“ Monitoring: Performans takibi
    âœ“ Testing: Birim ve entegrasyon testleri
    âœ“ Ã–lÃ§eklenebilirlik: Kolayca scale edilebilir
    
    PIPELINE ADIMLARI:
    -----------------
    1. INPUT PREPROCESSING
       â€¢ Input validasyonu
       â€¢ Text cleaning
       â€¢ Format dÃ¶nÃ¼ÅŸÃ¼mÃ¼
    
    2. TOKENIZATION
       â€¢ Text â†’ Token ID dÃ¶nÃ¼ÅŸÃ¼mÃ¼
       â€¢ Padding/truncation
       â€¢ Attention mask oluÅŸturma
    
    3. MODEL INFERENCE
       â€¢ GPU'ya taÅŸÄ±ma
       â€¢ Model forward pass
       â€¢ Generation veya classification
    
    4. OUTPUT POSTPROCESSING
       â€¢ Decode iÅŸlemi
       â€¢ Format dÃ¶nÃ¼ÅŸÃ¼mÃ¼
       â€¢ Filtering
    
    5. RESPONSE FORMATTING
       â€¢ JSON yapÄ±sÄ±
       â€¢ Metadata ekleme
       â€¢ Error handling
    """
    
    print("\n" + "=" * 80)
    print("ğŸ”„ INFERENCE PIPELINE OLUÅTURMA")
    print("=" * 80)
    
    # ========================================================================
    # PIPELINE WORKFLOW
    # ========================================================================
    
    print("\n1ï¸âƒ£ INFERENCE PIPELINE ADIMLARI")
    print("-" * 80)
    
    steps = [
        {
            "step": "1. Input Preprocessing",
            "description": "Gelen veriyi temizleme ve hazÄ±rlama",
            "operations": [
                "â€¢ Text cleaning (trim, normalize)",
                "â€¢ Input validation (length, format)",
                "â€¢ Security checks (XSS, injection prevention)",
                "â€¢ Language detection (opsiyonel)"
            ]
        },
        {
            "step": "2. Tokenization",
            "description": "Metni sayÄ±sal temsile dÃ¶nÃ¼ÅŸtÃ¼rme",
            "operations": [
                "â€¢ Text â†’ Token ID mapping",
                "â€¢ Padding ve truncation",
                "â€¢ Attention mask oluÅŸturma",
                "â€¢ Special tokens ekleme"
            ]
        },
        {
            "step": "3. Model Inference",
            "description": "Model ile tahmin yapma",
            "operations": [
                "â€¢ GPU/CPU'ya tensor taÅŸÄ±ma",
                "â€¢ Model forward pass",
                "â€¢ Generation/classification",
                "â€¢ Result collection"
            ]
        },
        {
            "step": "4. Output Postprocessing",
            "description": "Model Ã§Ä±ktÄ±sÄ±nÄ± iÅŸleme",
            "operations": [
                "â€¢ Token ID â†’ Text decode",
                "â€¢ Special token removal",
                "â€¢ Text cleaning ve formatting",
                "â€¢ Confidence calculation"
            ]
        },
        {
            "step": "5. Response Formatting",
            "description": "Son yanÄ±tÄ± yapÄ±landÄ±rma",
            "operations": [
                "â€¢ JSON response oluÅŸturma",
                "â€¢ Metadata ekleme (timestamp, model_version)",
                "â€¢ Error handling ve logging",
                "â€¢ Performance metrics"
            ]
        }
    ]
    
    for item in steps:
        print(f"\n{item['step']}")
        print(f"   {item['description']}")
        print()
        for op in item['operations']:
            print(f"   {op}")
    
    # ========================================================================
    # HUGGING FACE PIPELINE KULLANIMI
    # ========================================================================
    
    print("\n\n2ï¸âƒ£ HUGGING FACE PIPELINE API")
    print("-" * 80)
    print("""
    Hugging Face, high-level bir Pipeline API sunar.
    TÃ¼m preprocessing, inference ve postprocessing otomatik yapÄ±lÄ±r.
    
    AVANTAJLARI:
    -----------
    âœ“ Kolay kullanÄ±m (tek satÄ±r kod)
    âœ“ Otomatik preprocessing/postprocessing
    âœ“ BirÃ§ok task iÃ§in hazÄ±r (text-generation, classification, vb.)
    âœ“ Batch processing desteÄŸi
    
    DEZAVANTAJLARI:
    --------------
    âœ— Daha az kontrol
    âœ— Custom preprocessing zor
    âœ— Memory overhead biraz daha fazla
    """)
    
    pipeline_example = '''
# ============================================================================
# Ã–RNEK 1: TEKSTÃ¼retimi PIPELINE
# ============================================================================

from transformers import pipeline, AutoTokenizer
import torch

# Tokenizer yÃ¼kle ve pad token ayarla
tokenizer = AutoTokenizer.from_pretrained("./fine_tuned_model")
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# Text generation pipeline oluÅŸtur
generator = pipeline(
    "text-generation",                          # Task tipi
    model="./fine_tuned_model",                # Model path
    tokenizer=tokenizer,                        # Tokenizer
    device=0 if torch.cuda.is_available() else -1,  # GPU (0) veya CPU (-1)
    torch_dtype="auto",                         # Otomatik dtype
    max_new_tokens=100,                         # Default max tokens
)

# KULLANIM 1: Deterministik (FaktÃ¼el iÃ§erik)
result_factual = generator(
    "Yapay zeka nedir?",
    max_new_tokens=50,
    do_sample=False,                    # Sampling kapalÄ±
    temperature=0.0,                    # Deterministik
    pad_token_id=tokenizer.eos_token_id,
    num_return_sequences=1              # Tek sonuÃ§
)

print("FaktÃ¼el YanÄ±t:")
print(result_factual[0]['generated_text'])
print()

# KULLANIM 2: YaratÄ±cÄ± (Hikaye/Blog)
result_creative = generator(
    "Bir zamanlar uzak bir galakside...",
    max_new_tokens=150,
    do_sample=True,                     # Sampling aÃ§Ä±k
    temperature=0.8,                    # YÃ¼ksek yaratÄ±cÄ±lÄ±k
    top_p=0.9,                          # Nucleus sampling
    top_k=50,                           # Top-k filtering
    repetition_penalty=1.1,             # Tekrar Ã¶nleme
    pad_token_id=tokenizer.eos_token_id,
    num_return_sequences=1
)

print("YaratÄ±cÄ± YanÄ±t:")
print(result_creative[0]['generated_text'])
print()

# KULLANIM 3: Batch Processing
prompts = [
    "Python nedir?",
    "Machine learning nasÄ±l Ã§alÄ±ÅŸÄ±r?",
    "Deep learning ile machine learning farkÄ± nedir?"
]

results = generator(
    prompts,
    max_new_tokens=80,
    do_sample=True,
    temperature=0.7,
    batch_size=3,                       # Batch processing
    pad_token_id=tokenizer.eos_token_id
)

for i, result in enumerate(results):
    print(f"Soru {i+1}: {prompts[i]}")
    print(f"YanÄ±t: {result[0]['generated_text']}")
    print()

# ============================================================================
# Ã–RNEK 2: CLASSIFICATION PIPELINE
# ============================================================================

# Text classification pipeline
classifier = pipeline(
    "text-classification",
    model="./sentiment_model",
    device=0 if torch.cuda.is_available() else -1
)

# Tekli sÄ±nÄ±flandÄ±rma
text = "Bu film gerÃ§ekten harikaydÄ±, kesinlikle tavsiye ederim!"
result = classifier(text)

print(f"Metin: {text}")
print(f"Tahmin: {result[0]['label']}")
print(f"GÃ¼ven: {result[0]['score']:.2%}")
print()

# Batch sÄ±nÄ±flandÄ±rma
texts = [
    "Harika bir Ã¼rÃ¼n, Ã§ok memnunum!",
    "Berbat bir deneyimdi, asla tekrar almam.",
    "Ä°dare eder, fena deÄŸil."
]

results = classifier(texts, batch_size=3)

for text, result in zip(texts, results):
    print(f"Metin: {text}")
    print(f"Duygu: {result['label']} ({result['score']:.2%})")
    print()
'''
    
    print(pipeline_example)
    
    # ========================================================================
    # CUSTOM PIPELINE SINIFI
    # ========================================================================
    
    print("\n3ï¸âƒ£ CUSTOM INFERENCE PIPELINE SINIFI")
    print("-" * 80)
    print("Daha fazla kontrol iÃ§in Ã¶zel pipeline sÄ±nÄ±fÄ±:")
    print()
    
    custom_pipeline = '''
# ============================================================================
# CUSTOM INFERENCE PIPELINE
# ============================================================================

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import time
import logging
from typing import Dict, List, Union

class ProductionInferencePipeline:
    """
    Production-Ready Inference Pipeline
    
    Ã–zellikleri:
    â€¢ Input preprocessing ve validation
    â€¢ Batch processing desteÄŸi
    â€¢ Error handling ve logging
    â€¢ Performance monitoring
    â€¢ Flexible generation configs
    """
    
    def __init__(self, model_path: str, device: str = "auto"):
        """Pipeline'Ä± baÅŸlat"""
        self.logger = logging.getLogger(__name__)
        self.model_path = model_path
        
        # Model ve tokenizer yÃ¼kle
        self.logger.info(f"Loading model from {model_path}")
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            device_map=device,
            torch_dtype="auto"
        )
        
        # Pad token ayarla
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        self.model.eval()
        self.logger.info("Model loaded successfully")
    
    def preprocess(self, text: Union[str, List[str]]) -> Dict:
        """
        Input preprocessing
        
        Args:
            text: Tek string veya string listesi
        
        Returns:
            Preprocessed ve tokenized inputs
        """
        # Liste deÄŸilse listeye Ã§evir
        if isinstance(text, str):
            text = [text]
        
        # Input validation
        for t in text:
            if not t or not isinstance(t, str):
                raise ValueError(f"Invalid input: {t}")
            if len(t) > 5000:  # Max karakter limiti
                raise ValueError("Input too long (>5000 chars)")
        
        # Tokenization
        inputs = self.tokenizer(
            text,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=512
        )
        
        # GPU'ya taÅŸÄ±
        if torch.cuda.is_available():
            inputs = {k: v.cuda() for k, v in inputs.items()}
        
        return inputs
    
    def postprocess(self, outputs, inputs):
        """
        Output postprocessing
        
        Args:
            outputs: Model outputs
            inputs: Original inputs
        
        Returns:
            Decoded text (sadece yeni tokenlar)
        """
        # Sadece yeni Ã¼retilen tokenlarÄ± decode et
        input_length = inputs['input_ids'].shape[-1]
        new_tokens = outputs[:, input_length:]
        
        # Decode
        texts = self.tokenizer.batch_decode(
            new_tokens,
            skip_special_tokens=True
        )
        
        return [t.strip() for t in texts]
    
    def generate(
        self,
        prompts: Union[str, List[str]],
        max_new_tokens: int = 100,
        temperature: float = 0.7,
        top_p: float = 0.9,
        deterministic: bool = False
    ) -> List[Dict]:
        """
        Text generation
        
        Returns:
            List of dicts with 'text', 'latency_ms', etc.
        """
        start_time = time.time()
        
        try:
            # Preprocessing
            inputs = self.preprocess(prompts)
            
            # Generation config
            gen_kwargs = {
                "max_new_tokens": max_new_tokens,
                "pad_token_id": self.tokenizer.eos_token_id,
                "eos_token_id": self.tokenizer.eos_token_id,
            }
            
            if deterministic:
                gen_kwargs.update({
                    "do_sample": False,
                    "temperature": 0.0,
                })
            else:
                gen_kwargs.update({
                    "do_sample": True,
                    "temperature": temperature,
                    "top_p": top_p,
                    "top_k": 50,
                })
            
            # Inference
            with torch.no_grad():
                outputs = self.model.generate(**inputs, **gen_kwargs)
            
            # Postprocessing
            texts = self.postprocess(outputs, inputs)
            
            # Performance metrics
            latency_ms = (time.time() - start_time) * 1000
            
            # Format results
            results = []
            for text in texts:
                results.append({
                    "generated_text": text,
                    "latency_ms": latency_ms / len(texts),
                    "model": self.model_path,
                    "timestamp": time.time()
                })
            
            return results
            
        except Exception as e:
            self.logger.error(f"Inference error: {e}")
            raise

# ============================================================================
# KULLANIM Ã–RNEÄÄ°
# ============================================================================

# Pipeline oluÅŸtur
pipeline = ProductionInferencePipeline(
    model_path="./fine_tuned_model",
    device="auto"
)

# Tekli inference
result = pipeline.generate(
    "Python nedir?",
    max_new_tokens=50,
    deterministic=True
)

print(f"SonuÃ§: {result[0]['generated_text']}")
print(f"Latency: {result[0]['latency_ms']:.2f}ms")

# Batch inference
results = pipeline.generate(
    ["Python nedir?", "AI nasÄ±l Ã§alÄ±ÅŸÄ±r?", "ML nedir?"],
    max_new_tokens=80,
    temperature=0.7
)

for i, res in enumerate(results):
    print(f"\\nSonuÃ§ {i+1}: {res['generated_text']}")
    print(f"Latency: {res['latency_ms']:.2f}ms")
'''
    
    print(custom_pipeline)
    
    # ========================================================================
    # BEST PRACTICES
    # ========================================================================
    
    print("\n" + "=" * 80)
    print("ğŸ’¡ PIPELINE BEST PRACTICES")
    print("=" * 80)
    print("""
    1. ERROR HANDLING:
       âœ“ Try-except bloklarÄ± kullan
       âœ“ Input validation yap
       âœ“ Timeout mekanizmasÄ± ekle
       âœ“ Graceful degradation
    
    2. LOGGING:
       âœ“ Her adÄ±mÄ± logla
       âœ“ Performance metrics kaydet
       âœ“ Error stack trace'leri sakla
       âœ“ Request/response Ã¶rnekleri kaydet
    
    3. MONITORING:
       âœ“ Latency tracking
       âœ“ Throughput measurement
       âœ“ Error rate monitoring
       âœ“ Resource utilization (GPU/CPU/Memory)
    
    4. TESTING:
       âœ“ Unit tests (her adÄ±m iÃ§in)
       âœ“ Integration tests
       âœ“ Load tests
       âœ“ Edge case tests
    
    5. OPTIMIZATION:
       âœ“ Batch processing kullan
       âœ“ Cache sÄ±k kullanÄ±lan sonuÃ§larÄ±
       âœ“ Connection pooling
       âœ“ Async processing (gerekirse)
    
    6. SECURITY:
       âœ“ Input sanitization
       âœ“ Rate limiting
       âœ“ Authentication/Authorization
       âœ“ Data encryption
    """)
    
    print("=" * 80)
    print("âœ… Inference pipeline demonstrasyonu tamamlandÄ±!\n")


# ============================================================================
# MODEL DEPLOYMENT STRATEJÄ°LERÄ°
# ============================================================================

def demonstrate_model_deployment():
    """
    Model Deployment (DaÄŸÄ±tÄ±m) Stratejileri ve Best Practices
    ==========================================================
    
    Bu fonksiyon, fine-tune edilmiÅŸ modellerin production ortamÄ±na
    nasÄ±l deploy edileceÄŸini, farklÄ± deployment stratejilerini ve
    her birinin artÄ±/eksilerini gÃ¶sterir.
    
    DEPLOYMENT NEDÄ°R?
    -----------------
    Model deployment, eÄŸitilmiÅŸ bir modeli gerÃ§ek kullanÄ±cÄ±lara
    hizmet verebilecek ÅŸekilde production ortamÄ±na alma sÃ¼recidir.
    
    DEPLOYMENT AÅAMALARI:
    --------------------
    1. Model HazÄ±rlama (Optimization, Quantization)
    2. Containerization (Docker)
    3. API Servisi OluÅŸturma (REST, gRPC)
    4. Orchestration (Kubernetes)
    5. Monitoring & Logging
    6. CI/CD Pipeline
    7. Scaling Strategy
    
    DEPLOYMENT FAKTÃ–RLERI:
    ---------------------
    â€¢ Latency gereksinimleri
    â€¢ Throughput ihtiyacÄ±
    â€¢ Maliyet kÄ±sÄ±tlarÄ±
    â€¢ GÃ¼venlik gereksinimleri
    â€¢ Ã–lÃ§eklenebilirlik
    â€¢ BakÄ±m kolaylÄ±ÄŸÄ±
    """
    
    print("\n" + "=" * 80)
    print("ğŸš€ MODEL DEPLOYMENT STRATEJÄ°LERÄ°")
    print("=" * 80)
    
    # ========================================================================
    # DEPLOYMENT STRATEJÄ°LERÄ°
    # ========================================================================
    
    print("\nğŸ“‹ DEPLOYMENT STRATEJÄ°LERÄ° KARÅILAÅTIRMASI")
    print("-" * 80)
    
    strategies = {
        "ğŸŒ REST API (Flask/FastAPI)": {
            "description": "HTTP tabanlÄ± web servisi",
            "best_for": "Web uygulamalarÄ±, mobil apps, genel kullanÄ±m",
            "pros": [
                "âœ“ Kolay implement edilir",
                "âœ“ Platform baÄŸÄ±msÄ±z",
                "âœ“ YaygÄ±n protokol (HTTP)",
                "âœ“ Load balancing kolay",
                "âœ“ Debugging/testing basit"
            ],
            "cons": [
                "âœ— gRPC'ye gÃ¶re daha yavaÅŸ",
                "âœ— HTTP overhead",
                "âœ— Binary data iÃ§in verimsiz"
            ],
            "latency": "10-50ms (overhead)",
            "complexity": "DÃ¼ÅŸÃ¼k",
            "use_case": "Chatbot API, content generation service"
        },
        
        "âš¡ gRPC": {
            "description": "YÃ¼ksek performanslÄ± RPC framework",
            "best_for": "Microservices, low-latency uygulamalar",
            "pros": [
                "âœ“ REST'e gÃ¶re 2-5x hÄ±zlÄ±",
                "âœ“ Binary protocol (Protobuf)",
                "âœ“ Bi-directional streaming",
                "âœ“ Type-safe",
                "âœ“ Code generation"
            ],
            "cons": [
                "âœ— Daha karmaÅŸÄ±k setup",
                "âœ— Browser desteÄŸi sÄ±nÄ±rlÄ±",
                "âœ— Debugging zor"
            ],
            "latency": "2-10ms (overhead)",
            "complexity": "Orta",
            "use_case": "Real-time inference, internal services"
        },
        
        "ğŸ³ Docker Container": {
            "description": "Containerized deployment",
            "best_for": "Portable, reproducible deployments",
            "pros": [
                "âœ“ Environment consistency",
                "âœ“ Kolay versiyonlama",
                "âœ“ Ä°zolasyon (security)",
                "âœ“ Ã–lÃ§eklenebilir",
                "âœ“ CI/CD uyumlu"
            ],
            "cons": [
                "âœ— Overhead (dÃ¼ÅŸÃ¼k)",
                "âœ— Storage gereksinimi",
                "âœ— GPU pass-through kompleksliÄŸi"
            ],
            "latency": "Minimal (konteyner overhead: <5ms)",
            "complexity": "Orta",
            "use_case": "TÃ¼m production deploymentlar"
        },
        
        "â˜¸ï¸ Kubernetes": {
            "description": "Container orchestration platform",
            "best_for": "Large-scale, auto-scaling deployments",
            "pros": [
                "âœ“ Auto-scaling (HPA)",
                "âœ“ Self-healing",
                "âœ“ Load balancing",
                "âœ“ Rolling updates",
                "âœ“ Multi-cloud support"
            ],
            "cons": [
                "âœ— Steep learning curve",
                "âœ— Overhead (cluster management)",
                "âœ— KarmaÅŸÄ±k konfigÃ¼rasyon"
            ],
            "latency": "Minimal",
            "complexity": "YÃ¼ksek",
            "use_case": "Enterprise applications, high-traffic services"
        },
        
        "ğŸ“± Edge Deployment": {
            "description": "Mobile/IoT cihazlarda Ã§alÄ±ÅŸtÄ±rma",
            "best_for": "Offline inference, privacy-critical apps",
            "pros": [
                "âœ“ SÄ±fÄ±r latency (network yok)",
                "âœ“ Privacy (data local)",
                "âœ“ Offline Ã§alÄ±ÅŸma",
                "âœ“ DÃ¼ÅŸÃ¼k bandwidth"
            ],
            "cons": [
                "âœ— SÄ±nÄ±rlÄ± compute",
                "âœ— Model boyutu kÄ±sÄ±tlarÄ±",
                "âœ— Update zorluÄŸu",
                "âœ— Quantization gerekli"
            ],
            "latency": "~10-100ms (cihaza baÄŸlÄ±)",
            "complexity": "YÃ¼ksek",
            "use_case": "Mobile apps, IoT devices, offline tools"
        },
        
        "â˜ï¸ Serverless (AWS Lambda, Cloud Functions)": {
            "description": "Event-driven, auto-scaling functions",
            "best_for": "Sporadic usage, cost optimization",
            "pros": [
                "âœ“ SÄ±fÄ±r bakÄ±m (infrastructure)",
                "âœ“ Auto-scaling",
                "âœ“ Pay-per-use pricing",
                "âœ“ Kolay deploy"
            ],
            "cons": [
                "âœ— Cold start (3-10 saniye)",
                "âœ— Execution time limit",
                "âœ— Memory limits",
                "âœ— BÃ¼yÃ¼k modeller zor"
            ],
            "latency": "50ms-10s (cold start dahil)",
            "complexity": "DÃ¼ÅŸÃ¼k-Orta",
            "use_case": "DÃ¼ÅŸÃ¼k frekanslÄ± inference, batch processing"
        },
        
        "ğŸ–¥ï¸ Dedicated Server": {
            "description": "Fiziksel veya sanal sunucu",
            "best_for": "Predictable traffic, high GPU requirements",
            "pros": [
                "âœ“ Maksimum performans",
                "âœ“ Tam kontrol",
                "âœ“ GPU eriÅŸimi kolay",
                "âœ“ No cold start"
            ],
            "cons": [
                "âœ— YÃ¼ksek maliyet",
                "âœ— Manual scaling",
                "âœ— BakÄ±m gereksinimi",
                "âœ— Under-utilization riski"
            ],
            "latency": "Minimal",
            "complexity": "Orta",
            "use_case": "High-throughput services, GPU-intensive tasks"
        },
        
        "ğŸ”Œ Model-as-a-Service (Hugging Face Inference API)": {
            "description": "Managed inference service",
            "best_for": "HÄ±zlÄ± prototipleme, kÃ¼Ã§Ã¼k Ã¶lÃ§ek",
            "pros": [
                "âœ“ SÄ±fÄ±r infrastructure",
                "âœ“ AnÄ±nda baÅŸlatma",
                "âœ“ Auto-scaling",
                "âœ“ Free tier mevcut"
            ],
            "cons": [
                "âœ— YÃ¼ksek maliyet (scale'de)",
                "âœ— SÄ±nÄ±rlÄ± kontrol",
                "âœ— Vendor lock-in",
                "âœ— Privacy concerns"
            ],
            "latency": "50-200ms",
            "complexity": "Ã‡ok DÃ¼ÅŸÃ¼k",
            "use_case": "MVP, demo, low-traffic apps"
        }
    }
    
    for strategy_name, details in strategies.items():
        print(f"\n{strategy_name}")
        print("=" * 80)
        print(f"ğŸ“ AÃ§Ä±klama: {details['description']}")
        print(f"ğŸ¯ En Ä°yi KullanÄ±m: {details['best_for']}")
        
        print("\nâœ… Avantajlar:")
        for pro in details['pros']:
            print(f"   {pro}")
        
        print("\nâŒ Dezavantajlar:")
        for con in details['cons']:
            print(f"   {con}")
        
        print(f"\nâ±ï¸ Latency: {details['latency']}")
        print(f"ğŸ”§ Komplekslik: {details['complexity']}")
        print(f"ğŸ’¼ KullanÄ±m Ã–rneÄŸi: {details['use_case']}")
        print()
    
    # ========================================================================
    # DEPLOYMENT CHECKLIST
    # ========================================================================
    
    print("\n" + "=" * 80)
    print("âœ… DEPLOYMENT CHECKLIST")
    print("=" * 80)
    
    checklist = {
        "ğŸ¯ Model HazÄ±rlÄ±k": [
            "â˜ Model quantization (INT8/FP16)",
            "â˜ Model serialization (ONNX, TorchScript)",
            "â˜ Benchmark performans (latency, throughput)",
            "â˜ Memory profiling",
            "â˜ Model versiyonlama"
        ],
        
        "ğŸ”’ GÃ¼venlik": [
            "â˜ API authentication (JWT, OAuth)",
            "â˜ Rate limiting",
            "â˜ Input validation ve sanitization",
            "â˜ HTTPS/TLS encryption",
            "â˜ Secrets management (API keys)",
            "â˜ CORS configuration"
        ],
        
        "ğŸ“Š Monitoring & Logging": [
            "â˜ Request/response logging",
            "â˜ Performance metrics (Prometheus/Grafana)",
            "â˜ Error tracking (Sentry)",
            "â˜ Latency monitoring",
            "â˜ Resource utilization (GPU/CPU/Memory)",
            "â˜ Alerting system"
        ],
        
        "âš¡ Performance": [
            "â˜ Batch processing implementation",
            "â˜ Caching strategy",
            "â˜ Load balancing",
            "â˜ Connection pooling",
            "â˜ Async processing (gerekirse)",
            "â˜ CDN (static content iÃ§in)"
        ],
        
        "ğŸ§ª Testing": [
            "â˜ Unit tests",
            "â˜ Integration tests",
            "â˜ Load tests (Apache Bench, Locust)",
            "â˜ Stress tests",
            "â˜ Canary deployment tests",
            "â˜ Rollback plan"
        ],
        
        "ğŸ“ˆ Scalability": [
            "â˜ Horizontal scaling stratejisi",
            "â˜ Auto-scaling rules",
            "â˜ Database optimization",
            "â˜ Queue system (Celery, RabbitMQ)",
            "â˜ Cache layer (Redis)",
            "â˜ Multi-region deployment (opsiyonel)"
        ],
        
        "ğŸ”„ CI/CD": [
            "â˜ Automated testing pipeline",
            "â˜ Automated deployment",
            "â˜ Blue-green deployment",
            "â˜ Rollback mechanism",
            "â˜ Version tagging",
            "â˜ Environment configs (dev, staging, prod)"
        ],
        
        "ğŸ“š DokÃ¼mantasyon": [
            "â˜ API documentation (Swagger/OpenAPI)",
            "â˜ Usage examples",
            "â˜ Error codes documentation",
            "â˜ SLA definition",
            "â˜ Runbook (troubleshooting)",
            "â˜ Architecture diagram"
        ]
    }
    
    for category, items in checklist.items():
        print(f"\n{category}")
        print("-" * 80)
        for item in items:
            print(f"  {item}")
    
    # ========================================================================
    # DEPLOYMENT WORKFLOW Ã–RNEÄÄ°
    # ========================================================================
    
    print("\n\n" + "=" * 80)
    print("ğŸ”„ Ã–RNEK DEPLOYMENT WORKFLOW")
    print("=" * 80)
    
    workflow = """
    1. GELIÅTIRME AÅAMASI (Development)
       â”œâ”€ Model training ve fine-tuning
       â”œâ”€ Local testing
       â”œâ”€ Performance benchmarking
       â””â”€ Code review
    
    2. HAZIRLIK AÅAMASI (Preparation)
       â”œâ”€ Model optimization (quantization)
       â”œâ”€ Containerization (Dockerfile oluÅŸtur)
       â”œâ”€ Unit/integration tests
       â””â”€ Documentation update
    
    3. STAGING ORTAMI (Staging)
       â”œâ”€ Deploy to staging environment
       â”œâ”€ Integration testing
       â”œâ”€ Load testing
       â”œâ”€ Security scanning
       â””â”€ Stakeholder approval
    
    4. PRODUCTION DEPLOYMENT (Production)
       â”œâ”€ Canary deployment (5% traffic)
       â”œâ”€ Monitor metrics closely
       â”œâ”€ Gradual rollout (10% â†’ 50% â†’ 100%)
       â””â”€ Full deployment
    
    5. MONÄ°TORING (Post-Deployment)
       â”œâ”€ Real-time performance monitoring
       â”œâ”€ Error rate tracking
       â”œâ”€ User feedback collection
       â””â”€ Continuous optimization
    
    6. BAKIMIMLAMA (Maintenance)
       â”œâ”€ Regular model updates
       â”œâ”€ Bug fixes
       â”œâ”€ Performance tuning
       â””â”€ Infrastructure updates
    """
    
    print(workflow)
    
    # ========================================================================
    # Ã–RNEK DEPLOYMENT KODU
    # ========================================================================
    
    print("\n" + "=" * 80)
    print("ğŸ’» Ã–RNEK DEPLOYMENT KODU (FastAPI + Docker)")
    print("=" * 80)
    
    deployment_code = '''
# ============================================================================
# FILE: main.py (FastAPI Application)
# ============================================================================

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import time
from typing import Optional

app = FastAPI(
    title="LLM Inference API",
    description="Production-ready LLM inference service",
    version="1.0.0"
)

# Model yÃ¼kleme (startup sÄ±rasÄ±nda)
@app.on_event("startup")
async def load_model():
    global model, tokenizer
    
    model_path = "./fine_tuned_model"
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        device_map="auto",
        torch_dtype=torch.float16
    )
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    model.eval()
    print("âœ… Model loaded successfully")

# Request/Response models
class GenerateRequest(BaseModel):
    prompt: str
    max_new_tokens: int = 100
    temperature: float = 0.7
    top_p: float = 0.9
    deterministic: bool = False

class GenerateResponse(BaseModel):
    generated_text: str
    latency_ms: float
    model_version: str = "1.0.0"

# Health check endpoint
@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "model_loaded": model is not None,
        "gpu_available": torch.cuda.is_available()
    }

# Generation endpoint
@app.post("/generate", response_model=GenerateResponse)
async def generate(request: GenerateRequest):
    start_time = time.time()
    
    try:
        # Tokenization
        inputs = tokenizer(
            request.prompt,
            return_tensors="pt",
            padding=True,
            truncation=True
        )
        
        if torch.cuda.is_available():
            inputs = {k: v.cuda() for k, v in inputs.items()}
        
        # Generation config
        gen_kwargs = {
            "max_new_tokens": request.max_new_tokens,
            "pad_token_id": tokenizer.eos_token_id,
        }
        
        if request.deterministic:
            gen_kwargs.update({"do_sample": False, "temperature": 0.0})
        else:
            gen_kwargs.update({
                "do_sample": True,
                "temperature": request.temperature,
                "top_p": request.top_p,
                "top_k": 50
            })
        
        # Inference
        with torch.no_grad():
            outputs = model.generate(**inputs, **gen_kwargs)
        
        # Decode
        new_tokens = outputs[0][inputs['input_ids'].shape[-1]:]
        generated_text = tokenizer.decode(new_tokens, skip_special_tokens=True)
        
        latency_ms = (time.time() - start_time) * 1000
        
        return GenerateResponse(
            generated_text=generated_text.strip(),
            latency_ms=latency_ms
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# Run: uvicorn main:app --host 0.0.0.0 --port 8000

# ============================================================================
# FILE: Dockerfile
# ============================================================================

FROM nvidia/cuda:11.8.0-runtime-ubuntu22.04

# Python kurulumu
RUN apt-get update && apt-get install -y python3-pip

WORKDIR /app

# Dependencies
COPY requirements.txt .
RUN pip3 install --no-cache-dir -r requirements.txt

# Model ve kod kopyala
COPY ./fine_tuned_model /app/fine_tuned_model
COPY main.py .

# Expose port
EXPOSE 8000

# Start server
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]

# Build: docker build -t llm-inference:v1 .
# Run: docker run -p 8000:8000 --gpus all llm-inference:v1

# ============================================================================
# FILE: docker-compose.yml
# ============================================================================

version: '3.8'

services:
  llm-inference:
    build: .
    ports:
      - "8000:8000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    environment:
      - MODEL_PATH=/app/fine_tuned_model
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

# Run: docker-compose up
'''
    
    print(deployment_code)
    
    print("\n" + "=" * 80)
    print("âœ… Model deployment demonstrasyonu tamamlandÄ±!")
    print("=" * 80)


# ============================================================================
# ANA PROGRAM (MAIN)
# ============================================================================

if __name__ == "__main__":
    """
    Ana Demonstrasyon ProgramÄ±
    ===========================
    
    Bu program, inference ve kiÅŸiselleÅŸtirme konusundaki tÃ¼m
    teknikleri demonstre eder.
    
    PROGRAM AKIÅI:
    --------------
    1. Inference optimizasyon teknikleri
    2. KiÅŸiselleÅŸtirilmiÅŸ chatbot sistemi
    3. Generation configuration parametreleri
    4. Performance benchmarking
    5. Inference pipeline oluÅŸturma
    6. Model deployment stratejileri
    7. Pratik kullanÄ±m Ã¶rnekleri
    """
    
    # ========================================================================
    # PROGRAM BAÅLANGIÃ‡
    # ========================================================================
    print("\n" + "â–ˆ" * 80)
    print("â–ˆ" + " " * 78 + "â–ˆ")
    print("â–ˆ" + " " * 15 + "INFERENCE VE KÄ°ÅÄ°SELLEÅTÄ°RME BOOTCAMP" + " " * 25 + "â–ˆ")
    print("â–ˆ" + " " * 78 + "â–ˆ")
    print("â–ˆ" + " " * 20 + "Production-Ready LLM Deployment" + " " * 27 + "â–ˆ")
    print("â–ˆ" + " " * 78 + "â–ˆ")
    print("â–ˆ" * 80)
    
    print("\nğŸ“š KONU BAÅLIKLARI:")
    print("-" * 80)
    topics = [
        "1ï¸âƒ£ Inference Optimizasyon Teknikleri",
        "2ï¸âƒ£ KiÅŸiselleÅŸtirilmiÅŸ Chatbot Sistemleri",
        "3ï¸âƒ£ Generation Configuration Parametreleri",
        "4ï¸âƒ£ Performance Benchmarking ve Analiz",
        "5ï¸âƒ£ Production Inference Pipeline",
        "6ï¸âƒ£ Model Deployment Stratejileri",
        "7ï¸âƒ£ Pratik KullanÄ±m Ã–rnekleri"
    ]
    
    for topic in topics:
        print(f"   {topic}")
    
    print("\n" + "=" * 80)
    print("ğŸš€ DEMONSTRASYONLARIgeliÅŸtirilmiÅŸ BAÅLIYOR...")
    print("=" * 80)
    
    # ========================================================================
    # DEMONSTRASYONLARI Ã‡ALIÅTIR
    # ========================================================================
    
    try:
        # 1. Inference Optimizasyon Teknikleri
        demonstrate_inference_optimization()
        
        # 2. KiÅŸiselleÅŸtirilmiÅŸ Chatbot
        create_personalized_chatbot()
        
        # 3. Generation Configuration
        demonstrate_generation_config()
        
        # 4. Performance Benchmarking
        benchmark_inference_speed()
        
        # 5. Inference Pipeline
        create_inference_pipeline()
        
        # 6. Model Deployment
        demonstrate_model_deployment()
        
    except Exception as e:
        print(f"\nâŒ HATA: {e}")
        import traceback
        traceback.print_exc()
    
    # ========================================================================
    # PRATÄ°K KULLANIM Ã–RNEKLERÄ°
    # ========================================================================
    
    print("\n" + "â–ˆ" * 80)
    print("â–ˆ" + " " * 78 + "â–ˆ")
    print("â–ˆ" + " " * 25 + "PRATÄ°K KULLANIM Ã–RNEKLERÄ°" + " " * 29 + "â–ˆ")
    print("â–ˆ" + " " * 78 + "â–ˆ")
    print("â–ˆ" * 80)
    
    print("\n" + "=" * 80)
    print("ğŸ’» KOD Ã–RNEKLERÄ°")
    print("=" * 80)
    
    example_usage = '''
# ============================================================================
# Ã–RNEK 1: DETERMÄ°NÄ°STÄ°K METÄ°N ÃœRETÄ°MÄ° (FaktÃ¼el/Teknik Ä°Ã§erik)
# ============================================================================

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# Model ve tokenizer yÃ¼kleme
model_path = "./fine_tuned_model"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    device_map="auto",
    torch_dtype=torch.float16  # FP16 for efficiency
)

if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

model.eval()

# Deterministik Ã¼retim (her zaman aynÄ± sonuÃ§)
prompt = "Python programlama dilinin temel Ã¶zellikleri nelerdir?"

inputs = tokenizer(prompt, return_tensors="pt", padding=True)
if torch.cuda.is_available():
    inputs = {k: v.cuda() for k, v in inputs.items()}

with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=100,
        do_sample=False,        # Sampling KAPALI
        temperature=0.0,        # Deterministik
        pad_token_id=tokenizer.eos_token_id
    )

# Sadece yeni tokenlarÄ± decode et
new_tokens = outputs[0][inputs['input_ids'].shape[-1]:]
result = tokenizer.decode(new_tokens, skip_special_tokens=True)

print("=" * 70)
print("SORU:", prompt)
print("-" * 70)
print("YANIT:", result.strip())
print("=" * 70)

# ============================================================================
# Ã–RNEK 2: YARATICI METÄ°N ÃœRETÄ°MÄ° (Hikaye/Blog)
# ============================================================================

# YaratÄ±cÄ± Ã¼retim (her Ã§alÄ±ÅŸtÄ±rmada farklÄ± sonuÃ§)
creative_prompt = "Bir zamanlar, teknolojinin insanlÄ±ÄŸÄ± deÄŸiÅŸtirdiÄŸi bir dÃ¼nyada..."

inputs = tokenizer(creative_prompt, return_tensors="pt", padding=True)
if torch.cuda.is_available():
    inputs = {k: v.cuda() for k, v in inputs.items()}

with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=200,
        do_sample=True,          # Sampling AÃ‡IK
        temperature=0.8,         # YÃ¼ksek yaratÄ±cÄ±lÄ±k
        top_p=0.9,              # Nucleus sampling
        top_k=50,               # Top-k filtering
        repetition_penalty=1.1,  # Tekrar Ã¶nleme
        pad_token_id=tokenizer.eos_token_id
    )

new_tokens = outputs[0][inputs['input_ids'].shape[-1]:]
creative_result = tokenizer.decode(new_tokens, skip_special_tokens=True)

print("\\n" + "=" * 70)
print("PROMPT:", creative_prompt)
print("-" * 70)
print("HÄ°KAYE:", creative_result.strip())
print("=" * 70)

# ============================================================================
# Ã–RNEK 3: SINIFLANDIRMA (Sentiment Analysis)
# ============================================================================

from transformers import AutoModelForSequenceClassification

# SÄ±nÄ±flandÄ±rma modeli yÃ¼kleme
classifier_model = AutoModelForSequenceClassification.from_pretrained(
    "./sentiment_model",
    device_map="auto"
)

classifier_model.eval()

# SÄ±nÄ±flandÄ±rma
text = "Bu Ã¼rÃ¼n gerÃ§ekten harika, kesinlikle tavsiye ederim!"
label_names = ["Negatif", "NÃ¶tr", "Pozitif"]

inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
if torch.cuda.is_available():
    inputs = {k: v.cuda() for k, v in inputs.items()}

with torch.no_grad():
    outputs = classifier_model(**inputs)
    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
    probabilities = predictions.cpu().numpy()[0]
    predicted_class = probabilities.argmax()

print("\\n" + "=" * 70)
print("METÄ°N:", text)
print("-" * 70)
print(f"TAHMÄ°N: {label_names[predicted_class]}")
print(f"GÃœVEN SKORU: {probabilities[predicted_class]:.2%}")
print("\\nTÃœM OLASLIKLAR:")
for i, (label, prob) in enumerate(zip(label_names, probabilities)):
    bar = "â–ˆ" * int(prob * 50)
    print(f"  {label:<10} [{prob:>6.2%}] {bar}")
print("=" * 70)

# ============================================================================
# Ã–RNEK 4: BATCH PROCESSING (Toplu Ä°ÅŸleme)
# ============================================================================

# Birden fazla prompt'u aynÄ± anda iÅŸleme
batch_prompts = [
    "Yapay zeka nedir?",
    "Machine learning nasÄ±l Ã§alÄ±ÅŸÄ±r?",
    "Python neden popÃ¼lerdir?"
]

# Batch tokenization
batch_inputs = tokenizer(
    batch_prompts,
    return_tensors="pt",
    padding=True,
    truncation=True
)

if torch.cuda.is_available():
    batch_inputs = {k: v.cuda() for k, v in batch_inputs.items()}

# Batch generation
with torch.no_grad():
    batch_outputs = model.generate(
        **batch_inputs,
        max_new_tokens=80,
        do_sample=True,
        temperature=0.7,
        pad_token_id=tokenizer.eos_token_id
    )

# Her sonucu ayrÄ± decode et
print("\\n" + "=" * 70)
print("BATCH PROCESSING SONUÃ‡LARI")
print("=" * 70)

for i, (prompt, output) in enumerate(zip(batch_prompts, batch_outputs)):
    input_length = len(tokenizer.encode(prompt))
    new_tokens = output[input_length:]
    result = tokenizer.decode(new_tokens, skip_special_tokens=True)
    
    print(f"\\n{i+1}. SORU: {prompt}")
    print(f"   YANIT: {result.strip()}")

print("=" * 70)

# ============================================================================
# Ã–RNEK 5: LORA MODEL KULLANIMI
# ============================================================================

from peft import PeftModel

# Base model ve LoRA adapter yÃ¼kleme
base_model_path = "meta-llama/Llama-2-7b-hf"
lora_adapter_path = "./lora_adapters"

print("\\n" + "=" * 70)
print("LORA MODEL YÃœKLEME")
print("=" * 70)

base_tokenizer = AutoTokenizer.from_pretrained(base_model_path)
base_model = AutoModelForCausalLM.from_pretrained(
    base_model_path,
    device_map="auto",
    torch_dtype=torch.float16
)

# LoRA adapter ekle
lora_model = PeftModel.from_pretrained(base_model, lora_adapter_path)
lora_model.eval()

print("âœ… LoRA model yÃ¼klendi")

# LoRA model ile inference
lora_prompt = "Fine-tuning yapÄ±lmÄ±ÅŸ konuda soru..."
lora_inputs = base_tokenizer(lora_prompt, return_tensors="pt")

if torch.cuda.is_available():
    lora_inputs = {k: v.cuda() for k, v in lora_inputs.items()}

with torch.no_grad():
    lora_outputs = lora_model.generate(
        **lora_inputs,
        max_new_tokens=100,
        temperature=0.7
    )

result = base_tokenizer.decode(lora_outputs[0], skip_special_tokens=True)
print(f"\\nLoRA Model YanÄ±tÄ±: {result}")
print("=" * 70)

# ============================================================================
# Ã–RNEK 6: QUANTIZED MODEL KULLANIMI
# ============================================================================

print("\\n" + "=" * 70)
print("QUANTIZED MODEL (INT8) KULLANIMI")
print("=" * 70)

# INT8 quantized model yÃ¼kleme
quantized_model = AutoModelForCausalLM.from_pretrained(
    model_path,
    load_in_8bit=True,
    device_map="auto"
)

quantized_model.eval()

print("âœ… INT8 quantized model yÃ¼klendi")
print("ğŸ’¾ Bellek tasarrufu: ~75% (FP32'ye gÃ¶re)")

# Quantized model ile inference
# (KullanÄ±mÄ± normal model ile aynÄ±)

print("=" * 70)
'''
    
    print(example_usage)
    
    # ========================================================================
    # GERÃ‡EKmodel INFERENCE Ã–RNEÄÄ° (Opsiyonel)
    # ========================================================================
    
    print("\n" + "=" * 80)
    print("ğŸ¯ GERÃ‡EKgerÃ§ek INFERENCE Ã–RNEÄÄ°")
    print("=" * 80)
    print("""
    GerÃ§ek bir model ile inference yapmak iÃ§in aÅŸaÄŸÄ±daki kodu uncomment edin:
    
    NOT: Bu kod Ã§alÄ±ÅŸtÄ±rÄ±labilmesi iÃ§in:
    â€¢ Fine-tune edilmiÅŸ bir model gereklidir
    â€¢ Model path'i doÄŸru ayarlanmalÄ±dÄ±r
    â€¢ Gerekli kÃ¼tÃ¼phaneler yÃ¼klenmiÅŸ olmalÄ±dÄ±r
    """)
    
    real_inference_code = '''
    # GerÃ§ek model inference (uncomment to run)
    try:
        model_path = "./fine_tuned_model"  # Model path'inizi buraya yazÄ±n
        
        print(f"\\nğŸ“¦ Model yÃ¼kleniyor: {model_path}")
        inference_engine = PersonalizedInference(model_path, model_type="causal_lm")
        
        # Test 1: Deterministik Ã¼retim
        print("\\nğŸ”¬ Test 1: Deterministik Ãœretim")
        print("-" * 70)
        factual_result = inference_engine.generate_text(
            "Python programlama dilinde liste (list) nedir?",
            max_new_tokens=50,
            deterministic=True
        )
        print(f"SonuÃ§: {factual_result}")
        
        # Test 2: YaratÄ±cÄ± Ã¼retim
        print("\\nğŸ¨ Test 2: YaratÄ±cÄ± Ãœretim")
        print("-" * 70)
        creative_result = inference_engine.generate_text(
            "Bir zamanlar uzak bir galakside...",
            max_new_tokens=150,
            temperature=0.9,
            deterministic=False
        )
        print(f"Hikaye: {creative_result}")
        
        print("\\nâœ… GerÃ§ek inference testleri tamamlandÄ±!")
        
    except FileNotFoundError:
        print("\\nâš ï¸ Model bulunamadÄ±. Ã–nce bir model fine-tune etmelisiniz.")
    except Exception as e:
        print(f"\\nâŒ Hata: {e}")
    '''
    
    print(real_inference_code)
    
    # ========================================================================
    # PROGRAM SONU
    # ========================================================================
    
    print("\n" + "â–ˆ" * 80)
    print("â–ˆ" + " " * 78 + "â–ˆ")
    print("â–ˆ" + " " * 25 + "âœ… DEMONSTRASYON TAMAMLANDI!" + " " * 24 + "â–ˆ")
    print("â–ˆ" + " " * 78 + "â–ˆ")
    print("â–ˆ" * 80)
    
    print("\nğŸ“š Ã–ZET:")
    print("-" * 80)
    
    summary = """
    Bu demonstrasyonda Ã¶ÄŸrendikleriniz:
    
    âœ“ Inference optimizasyon teknikleri (Quantization, Batching, KV-Cache)
    âœ“ KiÅŸiselleÅŸtirilmiÅŸ chatbot sistemi tasarÄ±mÄ±
    âœ“ Generation parametreleri ve kullanÄ±m senaryolarÄ±
    âœ“ Performance benchmarking ve analiz
    âœ“ Production-ready inference pipeline oluÅŸturma
    âœ“ Model deployment stratejileri ve best practices
    âœ“ Pratik kod Ã¶rnekleri ve kullanÄ±m senaryolarÄ±
    
    ğŸ¯ BÄ°R SONRAKÄ° ADIMLAR:
    ----------------------
    1. Kendi modelinizi fine-tune edin (1_peft_lora.py)
    2. Bu dosyadaki teknikleri kendi modelinizle deneyin
    3. Production deployment iÃ§in bir strateji seÃ§in
    4. Performance testleri yapÄ±n ve optimize edin
    5. Monitoring ve logging sistemleri kurun
    
    ğŸ“– Ä°LAVE KAYNAKLAR:
    ------------------
    â€¢ Hugging Face Documentation: https://huggingface.co/docs
    â€¢ PEFT Library: https://github.com/huggingface/peft
    â€¢ FastAPI: https://fastapi.tiangolo.com
    â€¢ Docker: https://docs.docker.com
    â€¢ Kubernetes: https://kubernetes.io/docs
    
    ğŸ’¬ SORULARINIZ Ä°Ã‡Ä°N:
    -------------------
    Kairu AI - Build with LLMs Bootcamp
    """
    
    print(summary)
    
    print("\n" + "=" * 80)
    print("ğŸ“ Ä°yi Ã‡alÄ±ÅŸmalar!")
    print("=" * 80)
    print()