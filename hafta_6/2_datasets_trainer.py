"""
================================================================================
HUGGING FACE DATASETS VE TRAINER API KULLANIMI
================================================================================

Bu script, modern NLP model eÄŸitiminin temel taÅŸlarÄ±ndan olan Hugging Face 
Datasets ve Trainer API'sini kapsamlÄ± bir ÅŸekilde gÃ¶sterir.

KONU BAÅLIKLARI:
1. Datasets KÃ¼tÃ¼phanesi: Veri yÃ¶netimi ve dÃ¶nÃ¼ÅŸÃ¼mleri
2. Trainer API: Otomatik eÄŸitim ve deÄŸerlendirme
3. Training Arguments: EÄŸitim parametreleri
4. Metrics ve Evaluation: Model performans Ã¶lÃ§Ã¼mÃ¼
5. Best Practices: GeliÅŸmiÅŸ optimizasyonlar

Ã–ÄRENME HEDEFLERÄ°:
- Datasets kÃ¼tÃ¼phanesi ile veri yÃ¼kleme ve hazÄ±rlama
- Trainer API ile otomatik eÄŸitim pipeline'Ä± oluÅŸturma
- Ã–zel metrik hesaplama ve model deÄŸerlendirme
- Production-ready model eÄŸitim sÃ¼reÃ§leri tasarlama

Yazar: Kairu AI Bootcamp - Hafta 6
Tarih: KasÄ±m 2025
================================================================================
"""

# =============================================================================
# KÃœTÃœPHANE Ä°Ã‡E AKTARMALARI
# =============================================================================

import torch
from transformers import (
    AutoTokenizer,                          # Otomatik tokenizer yÃ¼kleyici
    AutoModelForSequenceClassification,     # SÄ±nÄ±flandÄ±rma modelleri
    TrainingArguments,                      # EÄŸitim parametreleri
    Trainer,                                # Ana eÄŸitim sÄ±nÄ±fÄ±
    DataCollatorWithPadding                 # Dinamik padding iÃ§in
)
from datasets import Dataset, load_dataset  # Veri yÃ¶netimi
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support


# =============================================================================
# VERÄ° YÃœKLEME VE HAZIRLAMA FONKSÄ°YONLARI
# =============================================================================

def load_and_prepare_dataset():
    """
    Dataset YÃ¼kleme ve HazÄ±rlama
    
    Bu fonksiyon, model eÄŸitimi iÃ§in gerekli veri setini yÃ¼kler ve hazÄ±rlar.
    
    DATASETS KÃœTÃœPHANESÄ° HAKKINDA:
    - Hugging Face Datasets: BÃ¼yÃ¼k veri setlerini verimli yÃ¶netir
    - Memory mapping: RAM'i optimize eder, bÃ¼yÃ¼k veri setleri iÃ§in idealdir
    - Arrow format: HÄ±zlÄ± okuma/yazma iÃ§in Apache Arrow kullanÄ±r
    - Lazy loading: Sadece gerekli veriyi yÃ¼kler
    
    GERÃ‡EK DÃœNYA KULLANIMI:
    - load_dataset("imdb"): Film yorumlarÄ± sentiment analizi
    - load_dataset("squad"): Question answering veri seti
    - load_dataset("glue", "sst2"): Benchmark veri setleri
    
    Returns:
        tuple: (train_dataset, eval_dataset) - EÄŸitim ve deÄŸerlendirme setleri
    """
    print("ğŸ“š Dataset yÃ¼kleniyor...")
    
    # Ã–rnek sentiment analysis dataset'i
    # Production'da: load_dataset("imdb") veya kendi veri setiniz
    sample_data = {
        "text": [
            "Bu film gerÃ§ekten harikaydÄ±! Ã‡ok beÄŸendim.",
            "Berbat bir film, hiÃ§ beÄŸenmedim.",
            "Ortalama bir yapÄ±m, fena deÄŸil.",
            "MuhteÅŸem oyunculuk ve senaryo!",
            "SÄ±kÄ±cÄ± ve anlamsÄ±z bir film.",
            "GÃ¼zel bir aile filmi, tavsiye ederim.",
            "Vakit kaybÄ±, izlemeyin.",
            "Etkileyici gÃ¶rsel efektler ve mÃ¼zik.",
            "Hayal kÄ±rÄ±klÄ±ÄŸÄ± yaratan bir devam filmi.",
            "MÃ¼kemmel yÃ¶netim ve karakterler."
        ],
        "label": [1, 0, 1, 1, 0, 1, 0, 1, 0, 1]  # 1: pozitif, 0: negatif
    }
    
    # Dataset.from_dict(): Dictionary'den Dataset objesi oluÅŸturur
    # Alternatifler:
    # - Dataset.from_pandas(): Pandas DataFrame'den
    # - Dataset.from_csv(): CSV dosyasÄ±ndan
    # - Dataset.from_json(): JSON dosyasÄ±ndan
    dataset = Dataset.from_dict(sample_data)
    
    # VERÄ° BÃ–LÃœMLEME (Train/Validation Split)
    # Best Practice: %80 train, %20 validation oranÄ±
    # Burada kÃ¼Ã§Ã¼k veri seti iÃ§in 8-2 split kullanÄ±yoruz
    train_dataset = dataset.select(range(8))
    eval_dataset = dataset.select(range(8, 10))
    
    print(f"âœ“ Train Ã¶rnekleri: {len(train_dataset)}")
    print(f"âœ“ Validation Ã¶rnekleri: {len(eval_dataset)}")
    
    return train_dataset, eval_dataset


# =============================================================================
# MODEL VE TOKENIZER KURULUM FONKSÄ°YONLARI
# =============================================================================

def setup_tokenizer_and_model(model_name="distilbert-base-uncased"):
    """
    Model ve Tokenizer Kurulumu
    
    Bu fonksiyon, pre-trained bir model ve ilgili tokenizer'Ä± yÃ¼kler.
    
    AUTOTOKENIZER VE AUTOMODEL HAKKINDA:
    - Auto* sÄ±nÄ±flarÄ±: Model ismine gÃ¶re otomatik olarak doÄŸru sÄ±nÄ±fÄ± seÃ§er
    - Pre-trained: Model, bÃ¼yÃ¼k veri setleri Ã¼zerinde Ã¶nceden eÄŸitilmiÅŸtir
    - Fine-tuning: Mevcut aÄŸÄ±rlÄ±klarÄ± kullanarak spesifik gÃ¶rev iÃ§in uyarlama
    
    MODEL SEÃ‡Ä°MLERÄ°:
    - distilbert-base-uncased: HÄ±zlÄ± ve hafif (66M parametre)
    - bert-base-uncased: Standart BERT (110M parametre)
    - roberta-base: GeliÅŸtirilmiÅŸ BERT varyantÄ± (125M parametre)
    - xlm-roberta-base: Ã‡ok dilli destek (270M parametre)
    
    TOKENÄ°ZER GÃ–REVÄ°:
    1. Metni kÃ¼Ã§Ã¼k parÃ§alara (token) bÃ¶ler
    2. Her tokeni sayÄ±sal ID'ye Ã§evirir
    3. Ã–zel tokenler ekler ([CLS], [SEP], [PAD])
    4. Attention mask oluÅŸturur
    
    Args:
        model_name (str): Hugging Face Hub'dan yÃ¼klenecek model adÄ±
        
    Returns:
        tuple: (tokenizer, model) - Tokenizer ve model nesneleri
    """
    print(f"ğŸ¤– Model yÃ¼kleniyor: {model_name}")
    
    # TOKENIZER YÃœKLEME
    # from_pretrained(): Hub'dan veya local'den model yÃ¼kler
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    
    # MODEL YÃœKLEME
    # AutoModelForSequenceClassification: Metin sÄ±nÄ±flandÄ±rma iÃ§in
    # num_labels: Ã‡Ä±kÄ±ÅŸ sÄ±nÄ±f sayÄ±sÄ± (binary classification iÃ§in 2)
    model = AutoModelForSequenceClassification.from_pretrained(
        model_name, 
        num_labels=2,  # Binary classification: pozitif/negatif
        # Ä°steÄŸe baÄŸlÄ± parametreler:
        # hidden_dropout_prob=0.1,    # Dropout oranÄ±
        # attention_probs_dropout_prob=0.1,  # Attention dropout
        # classifier_dropout=0.1      # SÄ±nÄ±flandÄ±rÄ±cÄ± dropout
    )
    
    print(f"âœ“ Model yÃ¼klendi: {model.config.model_type}")
    print(f"âœ“ Parametre sayÄ±sÄ±: {model.num_parameters():,}")
    
    return tokenizer, model


# =============================================================================
# TOKENÄ°ZASYON FONKSÄ°YONLARI
# =============================================================================

def tokenize_dataset(dataset, tokenizer, max_length=128):
    """
    Dataset Tokenization (Metni SayÄ±sal Veriye DÃ¶nÃ¼ÅŸtÃ¼rme)
    
    Bu fonksiyon, tÃ¼m veri setini tokenize eder ve model iÃ§in hazÄ±r hale getirir.
    
    TOKENÄ°ZASYON SÃœRECÄ°:
    1. Metin â†’ Token'lara bÃ¶lme (kelime/alt-kelime parÃ§alarÄ±)
    2. Token â†’ ID dÃ¶nÃ¼ÅŸÃ¼mÃ¼ (vocabulary kullanarak)
    3. Padding ve Truncation uygulama
    4. Attention mask oluÅŸturma
    
    Ã–RNEK TOKENÄ°ZASYON:
    Metin: "Bu harika bir film!"
    Token'lar: ["bu", "harika", "bir", "film", "!"]
    ID'ler: [101, 2023, 5919, 2019, 2143, 999, 102]
    
    PADDING STRATEJÄ°LERÄ°:
    - padding=False: Batch sÄ±rasÄ±nda dinamik padding (Ã¶nerilen)
    - padding=True: Her Ã¶rneÄŸi hemen max_length'e getir
    - padding="max_length": Sabit uzunluk padding
    
    TRUNCATION:
    - truncation=True: max_length'i aÅŸan metinleri kes
    - truncation=False: Uzun metinler hata verir
    
    Args:
        dataset: Tokenize edilecek Dataset objesi
        tokenizer: KullanÄ±lacak tokenizer
        max_length (int): Maksimum token uzunluÄŸu
        
    Returns:
        Dataset: Tokenize edilmiÅŸ dataset
    """
    
    def tokenize_function(examples):
        """
        Batch tokenization iÃ§in yardÄ±mcÄ± fonksiyon
        
        Bu fonksiyon, dataset.map() tarafÄ±ndan her batch iÃ§in Ã§aÄŸrÄ±lÄ±r.
        """
        # Tokenization
        tokenized = tokenizer(
            examples["text"],           # Tokenize edilecek metinler
            truncation=True,            # Uzun metinleri kes
            padding=False,              # Dynamic padding kullanacaÄŸÄ±z (DataCollator ile)
            max_length=max_length,      # Maksimum uzunluk
            # Ä°steÄŸe baÄŸlÄ± parametreler:
            # add_special_tokens=True,  # [CLS], [SEP] tokenlerini ekle
            # return_attention_mask=True,  # Attention mask dÃ¶ndÃ¼r
            # return_token_type_ids=True,  # Segment ID'leri (BERT iÃ§in)
        )
        # Label'Ä± labels olarak adlandÄ±r (Trainer'Ä±n beklediÄŸi format)
        tokenized["labels"] = examples["label"]
        return tokenized
    
    # BATCH TOKENIZATION
    # map(): Her Ã¶rneÄŸe veya batch'e fonksiyon uygular
    # batched=True: VerimliliÄŸi artÄ±rÄ±r, tÃ¼m Ã¶rnekleri tek tek iÅŸlemekten daha hÄ±zlÄ±
    # remove_columns: Tokenization sonrasÄ± orijinal sÃ¼tunlarÄ± kaldÄ±r
    tokenized_dataset = dataset.map(
        tokenize_function,
        batched=True,               # Batch processing (daha hÄ±zlÄ±)
        remove_columns=["text", "label"],  # Orijinal text ve label sÃ¼tunlarÄ±nÄ± kaldÄ±r
        desc="Tokenizing dataset"   # Progress bar aÃ§Ä±klamasÄ±
    )
    
    print(f"âœ“ Dataset tokenize edildi: {len(tokenized_dataset)} Ã¶rnek")
    
    return tokenized_dataset


# =============================================================================
# METRÄ°K HESAPLAMA FONKSÄ°YONLARI
# =============================================================================

def compute_metrics(eval_pred):
    """
    Evaluation Metrics Hesaplama
    
    Bu fonksiyon, model performansÄ±nÄ± deÄŸerlendirmek iÃ§in Ã§eÅŸitli metrikler hesaplar.
    Trainer tarafÄ±ndan her evaluation adÄ±mÄ±nda otomatik olarak Ã§aÄŸrÄ±lÄ±r.
    
    METRÄ°KLER HAKKINDA:
    
    1. ACCURACY (DoÄŸruluk):
       - DoÄŸru tahmin edilen Ã¶rneklerin oranÄ±
       - FormÃ¼l: (TP + TN) / Toplam
       - Ä°yi: Dengeli veri setleri iÃ§in
       - KÃ¶tÃ¼: Dengesiz veri setleri iÃ§in yanÄ±ltÄ±cÄ± olabilir
    
    2. PRECISION (Kesinlik):
       - Pozitif dediÄŸimizin gerÃ§ekten pozitif olma oranÄ±
       - FormÃ¼l: TP / (TP + FP)
       - Ã–rnek: 100 spam emailden 90'Ä± gerÃ§ekten spam
    
    3. RECALL (DuyarlÄ±lÄ±k / HatÄ±rlama):
       - TÃ¼m pozitiflerin ne kadarÄ±nÄ± yakaladÄ±k
       - FormÃ¼l: TP / (TP + FN)
       - Ã–rnek: TÃ¼m spam emaillerden %95'ini yakaladÄ±k
    
    4. F1-SCORE:
       - Precision ve Recall'un harmonik ortalamasÄ±
       - FormÃ¼l: 2 * (Precision * Recall) / (Precision + Recall)
       - Dengeli bir metrik
    
    WEIGHTED AVERAGE:
    - Her sÄ±nÄ±fÄ±n metriÄŸi, o sÄ±nÄ±ftaki Ã¶rnek sayÄ±sÄ±na gÃ¶re aÄŸÄ±rlÄ±klandÄ±rÄ±lÄ±r
    - Dengesiz veri setleri iÃ§in daha adil bir deÄŸerlendirme
    
    Args:
        eval_pred: (predictions, labels) tuple'Ä±
        
    Returns:
        dict: Hesaplanan metrikler
    """
    predictions, labels = eval_pred
    
    # SOFTMAX Ã‡IKIÅINDAN TAHMÄ°N ALMA
    # Model Ã§Ä±kÄ±ÅŸÄ±: [batch_size, num_classes] ÅŸeklinde logit deÄŸerleri
    # argmax ile en yÃ¼ksek olasÄ±lÄ±klÄ± sÄ±nÄ±fÄ± seÃ§iyoruz
    predictions = np.argmax(predictions, axis=1)
    
    # PRECISION, RECALL, F1 HESAPLAMA
    # average='weighted': SÄ±nÄ±f dengesizliÄŸini dikkate al
    # Alternatifler:
    # - 'micro': TÃ¼m Ã¶rnekleri eÅŸit aÄŸÄ±rlÄ±kta say
    # - 'macro': Her sÄ±nÄ±fa eÅŸit aÄŸÄ±rlÄ±k ver
    # - None: Her sÄ±nÄ±f iÃ§in ayrÄ± metrik dÃ¶ndÃ¼r
    precision, recall, f1, _ = precision_recall_fscore_support(
        labels, 
        predictions, 
        average='weighted',
        zero_division=0  # BÃ¶lme hatalarÄ±nÄ± Ã¶nle
    )
    
    # ACCURACY HESAPLAMA
    accuracy = accuracy_score(labels, predictions)
    
    # METRÄ°KLERÄ° DICTIONARY OLARAK DÃ–NDÃœR
    # Bu metrikler Trainer tarafÄ±ndan loglanÄ±r ve gÃ¶rselleÅŸtirilir
    return {
        'accuracy': accuracy,      # Genel doÄŸruluk
        'f1': f1,                  # F1 skoru
        'precision': precision,    # Kesinlik
        'recall': recall          # DuyarlÄ±lÄ±k
    }


# =============================================================================
# EÄÄ°TÄ°M PARAMETRELERI KONFIGÃœRASYONU
# =============================================================================

def setup_training_arguments():
    """
    Training Arguments KonfigÃ¼rasyonu
    
    TrainingArguments, Trainer'Ä±n tÃ¼m eÄŸitim davranÄ±ÅŸÄ±nÄ± kontrol eder.
    Bu parametreler model performansÄ±nÄ± ve eÄŸitim sÃ¼resini doÄŸrudan etkiler.
    
    Ã–NEMLÄ° PARAMETRELER VE AÃ‡IKLAMALARI:
    
    1. Ã‡IKTI VE KAYIT:
       - output_dir: Model checkpoint'lerinin kaydedileceÄŸi klasÃ¶r
       - logging_dir: TensorBoard/WandB loglarÄ±nÄ±n kaydedileceÄŸi yer
       - logging_steps: KaÃ§ adÄ±mda bir log kaydedilecek
    
    2. EÄÄ°TÄ°M DÃ–NGÃœSÃœ:
       - num_train_epochs: KaÃ§ epoch eÄŸitim yapÄ±lacak
       - per_device_train_batch_size: Her GPU/CPU'da batch boyutu
       - gradient_accumulation_steps: Efektif batch boyutunu artÄ±rÄ±r
    
    3. LEARNING RATE:
       - learning_rate: VarsayÄ±lan 5e-5 (Ã§oÄŸu BERT modeli iÃ§in uygun)
       - warmup_steps: BaÅŸlangÄ±Ã§ta LR'yi kademeli artÄ±r (stabilite iÃ§in)
       - lr_scheduler_type: LR azaltma stratejisi (linear, cosine, etc.)
    
    4. REGÃœLARÄ°ZASYON:
       - weight_decay: L2 regularization (overfitting Ã¶nler)
       - max_grad_norm: Gradient clipping (training stability)
    
    5. EVALUATION STRATEJÄ°SÄ°:
       - eval_strategy: Ne zaman evaluate edileceÄŸi
         * "no": HiÃ§ evaluate etme
         * "steps": Her N adÄ±mda bir
         * "epoch": Her epoch sonunda
       - eval_steps: steps stratejisinde kaÃ§ adÄ±mda bir
    
    6. KAYIT STRATEJÄ°SÄ°:
       - save_strategy: Model kaydedme stratejisi
       - save_steps: KaÃ§ adÄ±mda bir kaydet
       - save_total_limit: Maksimum checkpoint sayÄ±sÄ±
       - load_best_model_at_end: En iyi modeli eÄŸitim sonunda yÃ¼kle
    
    7. OPTÄ°MÄ°ZASYON:
       - fp16: 16-bit floating point (2x hÄ±zlanma, %50 RAM tasarrufu)
       - dataloader_num_workers: Paralel veri yÃ¼kleme
       - gradient_checkpointing: RAM tasarrufu (ama yavaÅŸlatÄ±r)
    
    Returns:
        TrainingArguments: EÄŸitim parametreleri objesi
    """
    training_args = TrainingArguments(
        # Ã‡IKTI AYARLARI
        output_dir='./results',              # Checkpoint kayÄ±t klasÃ¶rÃ¼
        logging_dir='./logs',                # Log dosyalarÄ± klasÃ¶rÃ¼
        logging_steps=10,                    # Her 10 adÄ±mda log kaydet
        
        # EÄÄ°TÄ°M HÄ°PERPARAMETRELERÄ°
        num_train_epochs=3,                  # 3 epoch eÄŸitim
        per_device_train_batch_size=8,       # Her cihazda 8 Ã¶rnek/batch
        per_device_eval_batch_size=8,        # Evaluation iÃ§in 8 Ã¶rnek/batch
        
        # LEARNING RATE VE OPTÄ°MÄ°ZASYON
        learning_rate=5e-5,                  # VarsayÄ±lan BERT learning rate
        warmup_steps=50,                     # Ä°lk 50 adÄ±mda warmup
        weight_decay=0.01,                   # L2 regularization katsayÄ±sÄ±
        
        # EVALUATION VE KAYIT STRATEJÄ°SÄ°
        eval_strategy="epoch",               # Her epoch sonunda evaluate et
        save_strategy="epoch",               # Her epoch sonunda kaydet
        load_best_model_at_end=True,         # En iyi modeli eÄŸitim sonunda yÃ¼kle
        metric_for_best_model="accuracy",    # Hangi metrik iÃ§in "en iyi"
        greater_is_better=True,              # YÃ¼ksek accuracy daha iyi
        
        # DÄ°ÄER AYARLAR
        report_to=None,                      # WandB/TensorBoard kapalÄ±
        seed=42,                             # Reproducibility iÃ§in sabit seed
        
        # GELÄ°ÅMÄ°Å OPTÄ°MÄ°ZASYON (isteÄŸe baÄŸlÄ±)
        # fp16=True,                         # Mixed precision training (GPU varsa)
        # gradient_accumulation_steps=2,     # Efektif batch size = 8 * 2 = 16
        # max_grad_norm=1.0,                 # Gradient clipping
        # dataloader_num_workers=4,          # Paralel veri yÃ¼kleme
    )
    
    print("âœ“ Training arguments hazÄ±rlandÄ±")
    
    return training_args


# =============================================================================
# MODEL EÄÄ°TÄ°M FONKSÄ°YONU - ANA PÄ°PELINE
# =============================================================================

def train_model_with_trainer():
    """
    Trainer API Kullanarak Model EÄŸitimi
    
    Bu fonksiyon, modern NLP model eÄŸitiminin tÃ¼m adÄ±mlarÄ±nÄ± iÃ§erir.
    Trainer API, karmaÅŸÄ±k eÄŸitim dÃ¶ngÃ¼lerini otomatikleÅŸtirir.
    
    TRAINER API'NÄ°N AVANTAJLARI:
    - Otomatik gradient hesaplama ve backpropagation
    - GPU/TPU/CPU otomatik yÃ¶netimi
    - Checkpoint kaydetme ve yÃ¼kleme
    - Early stopping desteÄŸi
    - Distributed training (Ã§oklu GPU) desteÄŸi
    - Mixed precision training
    - Gradient accumulation
    - Logging ve monitoring entegrasyonu
    
    EÄÄ°TÄ°M PÄ°PELINE ADIMLARI:
    1. Dataset yÃ¼kleme ve hazÄ±rlama
    2. Model ve tokenizer kurulumu
    3. Dataset tokenization
    4. Data collator oluÅŸturma (dynamic padding iÃ§in)
    5. Training arguments tanÄ±mlama
    6. Trainer objesi oluÅŸturma
    7. Model eÄŸitimi
    8. Model deÄŸerlendirme
    9. Model kaydetme
    
    Returns:
        tuple: (trainer, model, tokenizer)
    """
    print("\n" + "="*80)
    print("ğŸš€ MODEL EÄÄ°TÄ°MÄ° BAÅLIYOR")
    print("="*80 + "\n")
    
    # -------------------------------------------------------------------------
    # ADIM 1: DATASET HAZIRLAMA
    # -------------------------------------------------------------------------
    print("ğŸ“Š ADIM 1: Dataset HazÄ±rlama")
    train_dataset, eval_dataset = load_and_prepare_dataset()
    print(f"   â””â”€ Train dataset: {len(train_dataset)} Ã¶rnek")
    print(f"   â””â”€ Eval dataset: {len(eval_dataset)} Ã¶rnek\n")
    
    # -------------------------------------------------------------------------
    # ADIM 2: MODEL VE TOKENIZER YÃœKLEME
    # -------------------------------------------------------------------------
    print("ğŸ¤– ADIM 2: Model ve Tokenizer YÃ¼kleme")
    tokenizer, model = setup_tokenizer_and_model()
    print()
    
    # -------------------------------------------------------------------------
    # ADIM 3: DATASET TOKENÄ°ZASYONU
    # -------------------------------------------------------------------------
    print("ğŸ”¤ ADIM 3: Dataset Tokenization")
    train_tokenized = tokenize_dataset(train_dataset, tokenizer)
    eval_tokenized = tokenize_dataset(eval_dataset, tokenizer)
    print()
    
    # -------------------------------------------------------------------------
    # ADIM 4: DATA COLLATOR OLUÅTURMA
    # -------------------------------------------------------------------------
    print("ğŸ“¦ ADIM 4: Data Collator HazÄ±rlama")
    # DataCollatorWithPadding: Batch oluÅŸtururken dinamik padding uygular
    # Avantaj: Her batch'teki en uzun Ã¶rneÄŸe gÃ¶re padding yapar (RAM tasarrufu)
    # Alternatif: DataCollatorForLanguageModeling (LM iÃ§in)
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
    print("   â””â”€ Dynamic padding aktif\n")
    
    # -------------------------------------------------------------------------
    # ADIM 5: TRAINING ARGUMENTS
    # -------------------------------------------------------------------------
    print("âš™ï¸  ADIM 5: Training Arguments KonfigÃ¼rasyonu")
    training_args = setup_training_arguments()
    print()
    
    # -------------------------------------------------------------------------
    # ADIM 6: TRAINER OLUÅTURMA
    # -------------------------------------------------------------------------
    print("ğŸ¯ ADIM 6: Trainer OluÅŸturma")
    trainer = Trainer(
        model=model,                          # EÄŸitilecek model
        args=training_args,                   # EÄŸitim parametreleri
        train_dataset=train_tokenized,        # EÄŸitim veri seti
        eval_dataset=eval_tokenized,          # DeÄŸerlendirme veri seti
        tokenizer=tokenizer,                  # Tokenizer (kaydetme iÃ§in gerekli)
        data_collator=data_collator,          # Batch oluÅŸturucu
        compute_metrics=compute_metrics,      # Metrik hesaplama fonksiyonu
        
        # Ä°steÄŸe baÄŸlÄ± callback'ler:
        # callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],
        # optimizers=(optimizer, scheduler),  # Custom optimizer/scheduler
    )
    print("   â””â”€ Trainer hazÄ±r\n")
    
    # -------------------------------------------------------------------------
    # ADIM 7: MODEL EÄÄ°TÄ°MÄ°
    # -------------------------------------------------------------------------
    print("ğŸ‹ï¸  ADIM 7: Model EÄŸitimi BaÅŸlÄ±yor...")
    print("-" * 80)
    
    # train() methodu tÃ¼m eÄŸitim dÃ¶ngÃ¼sÃ¼nÃ¼ yÃ¶netir:
    # - Forward pass
    # - Loss hesaplama
    # - Backward pass (gradient hesaplama)
    # - Optimizer step (aÄŸÄ±rlÄ±k gÃ¼ncelleme)
    # - Logging
    # - Evaluation (belirtilen aralÄ±klarla)
    # - Checkpoint kaydetme
    train_result = trainer.train()
    
    print("-" * 80)
    print("âœ“ EÄŸitim tamamlandÄ±!\n")
    
    # -------------------------------------------------------------------------
    # ADIM 8: FINAL EVALUATION
    # -------------------------------------------------------------------------
    print("ğŸ“ˆ ADIM 8: Final Evaluation")
    eval_results = trainer.evaluate()
    
    print("\nğŸ“Š Evaluation SonuÃ§larÄ±:")
    print("-" * 40)
    for metric, value in eval_results.items():
        print(f"   {metric:20s}: {value:.4f}")
    print("-" * 40 + "\n")
    
    # -------------------------------------------------------------------------
    # ADIM 9: MODEL KAYDETME
    # -------------------------------------------------------------------------
    print("ğŸ’¾ ADIM 9: Model Kaydetme")
    save_path = "./fine_tuned_model"
    
    # Model ve tokenizer'Ä± kaydet
    trainer.save_model(save_path)
    tokenizer.save_pretrained(save_path)
    
    print(f"   â””â”€ Model kaydedildi: {save_path}")
    print(f"   â””â”€ Dosyalar: config.json, pytorch_model.bin, tokenizer.json\n")
    
    print("="*80)
    print("âœ… TÃœM Ä°ÅLEMLER BAÅARIYLA TAMAMLANDI!")
    print("="*80 + "\n")
    
    return trainer, model, tokenizer


# =============================================================================
# DATASET OPERASYONLARI DEMO FONKSÄ°YONU
# =============================================================================

def demonstrate_dataset_operations():
    """
    Datasets KÃ¼tÃ¼phanesi - GeliÅŸmiÅŸ Operasyonlar
    
    Bu fonksiyon, Hugging Face Datasets kÃ¼tÃ¼phanesinin gÃ¼Ã§lÃ¼ Ã¶zelliklerini gÃ¶sterir.
    
    DATASETS KÃœTÃœPHANESÄ°NÄ°N Ã–ZELLÄ°KLERÄ°:
    
    1. VERÄ°MLÄ° VERÄ° YÃ–NETÄ°MÄ°:
       - Apache Arrow formatÄ±: HÄ±zlÄ± okuma/yazma
       - Memory mapping: RAM'e sÄ±ÄŸmayan bÃ¼yÃ¼k veri setleri
       - Lazy loading: Ä°htiyaÃ§ duyulan veri yÃ¼klenir
    
    2. DÃ–NÃœÅÃœM OPERASYONLARÄ°:
       - map(): Her Ã¶rneÄŸe veya batch'e fonksiyon uygula
       - filter(): Belirli koÅŸullara gÃ¶re filtrele
       - select(): Index ile seÃ§im yap
       - sort(): SÄ±ralama
       - shuffle(): KarÄ±ÅŸtÄ±rma
    
    3. VERÄ° BÄ°RLEÅTÄ°RME:
       - concatenate(): Veri setlerini birleÅŸtir
       - interleave(): Veri setlerini iÃ§ iÃ§e geÃ§ir
    
    4. BATCH PROCESSING:
       - batched=True: VerimliliÄŸi artÄ±rÄ±r
       - num_proc: Paralel iÅŸleme
    
    5. CACHING:
       - Otomatik caching: Ä°ÅŸlenen veri cache'lenir
       - load_from_cache_file: Cache'den yÃ¼kle
    """
    print("\n" + "="*80)
    print("ğŸ“š DATASETS KÃœTÃœPHANESÄ° - GELÄ°ÅMÄ°Å OPERASYONLAR")
    print("="*80 + "\n")
    
    # Dataset yÃ¼kleme
    train_dataset, eval_dataset = load_and_prepare_dataset()
    
    # -------------------------------------------------------------------------
    # DATASET Ã–ZELLÄ°KLERÄ°
    # -------------------------------------------------------------------------
    print("ğŸ” Dataset Ã–zellikleri:")
    print(f"   â””â”€ Columns: {train_dataset.column_names}")
    print(f"   â””â”€ Features: {train_dataset.features}")
    print(f"   â””â”€ Toplam Ã¶rnek sayÄ±sÄ±: {len(train_dataset)}")
    print(f"   â””â”€ Ä°lk Ã¶rnek: {train_dataset[0]}\n")
    
    # -------------------------------------------------------------------------
    # FILTER OPERASYONU
    # -------------------------------------------------------------------------
    print("ğŸ” Filter Operasyonu - Pozitif Ã–rnekleri SeÃ§:")
    # filter(): Lambda fonksiyonu ile veri filtreleme
    # Her Ã¶rnek iÃ§in True/False dÃ¶ndÃ¼rÃ¼r, True olanlarÄ± tutar
    positive_samples = train_dataset.filter(lambda x: x["label"] == 1)
    negative_samples = train_dataset.filter(lambda x: x["label"] == 0)
    
    print(f"   â””â”€ Pozitif Ã¶rnekler: {len(positive_samples)}")
    print(f"   â””â”€ Negatif Ã¶rnekler: {len(negative_samples)}\n")
    
    # -------------------------------------------------------------------------
    # MAP OPERASYONU - TEK Ã–RNEK
    # -------------------------------------------------------------------------
    print("ğŸ”§ Map Operasyonu - Yeni SÃ¼tun Ekleme:")
    
    def add_length(example):
        """Her Ã¶rneÄŸe metin uzunluÄŸu ekle"""
        example["text_length"] = len(example["text"])
        return example
    
    # map() ile her Ã¶rneÄŸe fonksiyon uygula
    dataset_with_length = train_dataset.map(add_length)
    print(f"   â””â”€ Yeni sÃ¼tun eklendi: 'text_length'")
    print(f"   â””â”€ Ã–rnek: {dataset_with_length[0]}\n")
    
    # -------------------------------------------------------------------------
    # MAP OPERASYONU - BATCH PROCESSÄ°NG
    # -------------------------------------------------------------------------
    print("âš¡ Batch Processing - Toplu Ä°ÅŸleme:")
    
    def uppercase_text(batch):
        """TÃ¼m metinleri bÃ¼yÃ¼k harfe Ã§evir (batch modunda)"""
        # batch: dictionary, her key bir liste iÃ§erir
        batch["text"] = [text.upper() for text in batch["text"]]
        return batch
    
    # batched=True: Tek tek deÄŸil, batch halinde iÅŸler (Ã§ok daha hÄ±zlÄ±!)
    uppercase_dataset = train_dataset.map(uppercase_text, batched=True)
    print(f"   â””â”€ Metinler bÃ¼yÃ¼k harfe Ã§evrildi")
    print(f"   â””â”€ Ã–rnek: {uppercase_dataset[0]['text']}\n")
    
    # -------------------------------------------------------------------------
    # SELECT ve SHUffle OPERASYONLARI
    # -------------------------------------------------------------------------
    print("ğŸ² Select ve Shuffle OperasyonlarÄ±:")
    
    # shuffle(): Veriyi karÄ±ÅŸtÄ±r
    shuffled_dataset = train_dataset.shuffle(seed=42)
    print(f"   â””â”€ Dataset karÄ±ÅŸtÄ±rÄ±ldÄ± (reproducibility iÃ§in seed=42)")
    
    # select(): Index ile seÃ§im
    subset = train_dataset.select([0, 2, 4, 6])
    print(f"   â””â”€ Alt kÃ¼me seÃ§ildi: {len(subset)} Ã¶rnek\n")
    
    # -------------------------------------------------------------------------
    # GELIÅMIÅ Ã–ZELLIKLER
    # -------------------------------------------------------------------------
    print("ğŸš€ GeliÅŸmiÅŸ Ã–zellikler:")
    print("   â”œâ”€ num_proc: Paralel iÅŸleme (Ã§oklu CPU core)")
    print("   â”œâ”€ remove_columns: Ä°stenmeyen sÃ¼tunlarÄ± kaldÄ±r")
    print("   â”œâ”€ load_from_cache_file: Cache kullanÄ±mÄ± (hÄ±z iÃ§in)")
    print("   â””â”€ keep_in_memory: TÃ¼m veriyi RAM'de tut\n")
    
    # Paralel iÅŸleme Ã¶rneÄŸi (uncomment to use)
    # dataset_parallel = train_dataset.map(
    #     add_length, 
    #     num_proc=4,  # 4 core kullan
    #     desc="Processing with 4 cores"
    # )
    
    print("="*80 + "\n")


# =============================================================================
# EÄÄ°TÄ°M STRATEJÄ°LERÄ° DEMO FONKSÄ°YONU
# =============================================================================

def demonstrate_training_strategies():
    """
    Training Strategies - EÄŸitim Stratejileri
    
    Bu fonksiyon, farklÄ± eÄŸitim stratejilerini ve TrainingArguments'in
    Ã¶nemli parametrelerini aÃ§Ä±klar.
    
    EÄÄ°TÄ°M STRATEJÄ°LERÄ°:
    
    1. EVALUATION STRATEGY:
       - Modelin ne sÄ±klÄ±kla deÄŸerlendirileceÄŸini belirler
       - Overfitting'i erken tespit etmek iÃ§in Ã¶nemli
    
    2. SAVE STRATEGY:
       - Checkpoint kaydetme sÄ±klÄ±ÄŸÄ±
       - Disk alanÄ± ve eÄŸitim sÃ¼resi dengesi
    
    3. EARLY STOPPING:
       - Performans artmayÄ± bÄ±rakÄ±nca eÄŸitimi durdur
       - Zaman ve kaynak tasarrufu
    
    4. LEARNING RATE SCHEDULING:
       - EÄŸitim boyunca LR'yi ayarla
       - Daha iyi konverjans iÃ§in
    """
    print("\n" + "="*80)
    print("ğŸ“Š EÄÄ°TÄ°M STRATEJÄ°LERÄ° VE BEST PRACTICES")
    print("="*80 + "\n")
    
    # -------------------------------------------------------------------------
    # EVALUATION STRATEJÄ°LERÄ°
    # -------------------------------------------------------------------------
    print("ğŸ¯ Evaluation Stratejileri:")
    print("-" * 40)
    
    strategies = {
        "no": "DeÄŸerlendirme yapÄ±lmaz (sadece eÄŸitim)",
        "epoch": "Her epoch sonunda deÄŸerlendir (Ã¶nerilen)",
        "steps": "Belirli step sayÄ±sÄ±nda deÄŸerlendir (bÃ¼yÃ¼k veri setleri iÃ§in)"
    }
    
    for strategy, description in strategies.items():
        print(f"   â””â”€ '{strategy}': {description}")
    
    print("\n   ğŸ’¡ Ã–neri: KÃ¼Ã§Ã¼k veri setleri iÃ§in 'epoch', bÃ¼yÃ¼k veri iÃ§in 'steps'")
    print()
    
    # -------------------------------------------------------------------------
    # SAVE STRATEJÄ°LERÄ°
    # -------------------------------------------------------------------------
    print("ğŸ’¾ Save Stratejileri:")
    print("-" * 40)
    print("   â”œâ”€ save_strategy='no': Checkpoint kaydetme")
    print("   â”œâ”€ save_strategy='epoch': Her epoch'ta kaydet")
    print("   â”œâ”€ save_strategy='steps': Her N adÄ±mda kaydet")
    print("   â””â”€ save_total_limit: Maksimum checkpoint sayÄ±sÄ± (disk tasarrufu)")
    print()
    
    # -------------------------------------------------------------------------
    # EARLY STOPPING
    # -------------------------------------------------------------------------
    print("â¹ï¸  Early Stopping:")
    print("-" * 40)
    print("   KullanÄ±mÄ±:")
    print("   ```python")
    print("   from transformers import EarlyStoppingCallback")
    print("   ")
    print("   trainer = Trainer(")
    print("       ...,")
    print("       callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]")
    print("   )")
    print("   ```")
    print("   â””â”€ patience=3: 3 epoch boyunca geliÅŸme yoksa dur")
    print()
    
    # -------------------------------------------------------------------------
    # TRAININGARGUMENTS Ã–NEMLÄ° PARAMETRELER
    # -------------------------------------------------------------------------
    print("âš™ï¸  TrainingArguments - Ã–nemli Parametreler:")
    print("="*80)
    
    params = [
        ("learning_rate", "5e-5", "Ã–ÄŸrenme oranÄ± (BERT iÃ§in Ã¶nerilen)"),
        ("weight_decay", "0.01", "L2 regularization (overfitting Ã¶nler)"),
        ("warmup_steps", "500", "LR warmup (baÅŸlangÄ±Ã§ stabilitesi)"),
        ("warmup_ratio", "0.1", "Warmup oranÄ± (steps yerine kullanÄ±labilir)"),
        ("gradient_accumulation_steps", "2", "Efektif batch size artÄ±rma"),
        ("fp16", "True", "16-bit precision (2x hÄ±zlanma, GPU gerekir)"),
        ("max_grad_norm", "1.0", "Gradient clipping (training stability)"),
        ("lr_scheduler_type", "'linear'", "LR azaltma stratejisi"),
        ("dataloader_num_workers", "4", "Paralel veri yÃ¼kleme"),
        ("logging_first_step", "True", "Ä°lk adÄ±mÄ± da logla"),
        ("load_best_model_at_end", "True", "En iyi modeli eÄŸitim sonunda yÃ¼kle"),
        ("metric_for_best_model", "'eval_loss'", "En iyi model iÃ§in metrik"),
    ]
    
    for param, default, desc in params:
        print(f"   {param:30s} = {default:15s}  # {desc}")
    
    print("="*80 + "\n")
    
    # -------------------------------------------------------------------------
    # LR SCHEDULER TÄ°PLERÄ°
    # -------------------------------------------------------------------------
    print("ğŸ“ˆ Learning Rate Scheduler Tipleri:")
    print("-" * 40)
    
    schedulers = {
        "linear": "Lineer azalma (varsayÄ±lan, Ã§oÄŸu durumda iyi)",
        "cosine": "Cosine azalma (yumuÅŸak azalma)",
        "cosine_with_restarts": "Periyodik cosine (uzun eÄŸitimler iÃ§in)",
        "polynomial": "Polinom azalma",
        "constant": "Sabit LR (warmup sonrasÄ±)",
        "constant_with_warmup": "Warmup sonrasÄ± sabit"
    }
    
    for scheduler, description in schedulers.items():
        print(f"   â””â”€ {scheduler:25s}: {description}")
    
    print()
    
    # -------------------------------------------------------------------------
    # BATCH SIZE OPTÄ°MÄ°ZASYONU
    # -------------------------------------------------------------------------
    print("ğŸ“¦ Batch Size Optimizasyonu:")
    print("-" * 40)
    print("   Gradient Accumulation ile efektif batch size artÄ±rma:")
    print()
    print("   per_device_train_batch_size = 8")
    print("   gradient_accumulation_steps = 4")
    print("   num_gpus = 2")
    print("   â†’ Efektif batch size = 8 Ã— 4 Ã— 2 = 64")
    print()
    print("   ğŸ’¡ Avantaj: GPU memory'ye sÄ±ÄŸmayan bÃ¼yÃ¼k batch'ler kullanabilirsiniz")
    print()
    
    # -------------------------------------------------------------------------
    # MIXED PRECISION TRAINING
    # -------------------------------------------------------------------------
    print("âš¡ Mixed Precision Training (FP16/BF16):")
    print("-" * 40)
    print("   FP16 (Float16):")
    print("   â”œâ”€ 2x hÄ±zlanma")
    print("   â”œâ”€ %50 RAM tasarrufu")
    print("   â””â”€ Modern GPU gerekir (V100, A100, RTX series)")
    print()
    print("   BF16 (BFloat16):")
    print("   â”œâ”€ FP16'ya benzer performans")
    print("   â”œâ”€ Daha geniÅŸ dinamik aralÄ±k")
    print("   â””â”€ A100, TPU v3/v4 iÃ§in Ã¶nerilen")
    print()
    
    # -------------------------------------------------------------------------
    # BEST PRACTICES
    # -------------------------------------------------------------------------
    print("âœ¨ Best Practices - En Ä°yi Uygulamalar:")
    print("="*80)
    print("   1. KÃ¼Ã§Ã¼k learning rate ile baÅŸla (5e-5 veya 3e-5)")
    print("   2. Warmup kullan (total_steps'in %10'u)")
    print("   3. Weight decay uygula (0.01)")
    print("   4. Gradient clipping kullan (max_grad_norm=1.0)")
    print("   5. Evaluation strategy ayarla (overfitting kontrolÃ¼)")
    print("   6. Best model kaydetme aktif et (load_best_model_at_end=True)")
    print("   7. Seed sabitle (reproducibility iÃ§in)")
    print("   8. FP16/BF16 kullan (GPU varsa)")
    print("   9. Logging ve monitoring ekle (WandB, TensorBoard)")
    print("   10. Early stopping kullan (gereksiz eÄŸitimi Ã¶nle)")
    print("="*80 + "\n")


# =============================================================================
# ANA PROGRAM - MAIN EXECUTION
# =============================================================================

if __name__ == "__main__":
    """
    Ana Program AkÄ±ÅŸÄ±
    
    Bu bÃ¶lÃ¼m, script'in tÃ¼m Ã¶zelliklerini sÄ±rasÄ±yla gÃ¶sterir.
    EÄŸitimi Ã§alÄ±ÅŸtÄ±rmak iÃ§in ilgili satÄ±rlarÄ±n comment'ini kaldÄ±rÄ±n.
    
    KULLANIM Ã–RNEKLERÄ°:
    
    1. Sadece demo'larÄ± Ã§alÄ±ÅŸtÄ±r:
       python 2_datasets_trainer.py
    
    2. Model eÄŸitimi iÃ§in (uncomment gerekli):
       - train_model_with_trainer() satÄ±rÄ±nÄ±n comment'ini kaldÄ±r
       - GPU varsa fp16=True ekle (hÄ±zlanma iÃ§in)
    
    3. Checkpoint'ten devam etme:
       trainer.train(resume_from_checkpoint="./results/checkpoint-xxx")
    """
    
    print("\n")
    print("â•”" + "="*78 + "â•—")
    print("â•‘" + " "*78 + "â•‘")
    print("â•‘" + "  HUGGING FACE DATASETS & TRAINER API - KAPSAMLI EÄÄ°TÄ°M  ".center(78) + "â•‘")
    print("â•‘" + " "*78 + "â•‘")
    print("â•š" + "="*78 + "â•")
    
    # -------------------------------------------------------------------------
    # BÃ–LÃœM 1: DATASET OPERASYONLARI
    # -------------------------------------------------------------------------
    demonstrate_dataset_operations()
    
    # -------------------------------------------------------------------------
    # BÃ–LÃœM 2: EÄÄ°TÄ°M STRATEJÄ°LERÄ°
    # -------------------------------------------------------------------------
    demonstrate_training_strategies()
    
    # -------------------------------------------------------------------------
    # BÃ–LÃœM 3: MODEL EÄÄ°TÄ°MÄ° (Ä°STEÄE BAÄLI)
    # -------------------------------------------------------------------------
    print("\n" + "="*80)
    print("ğŸ“ MODEL EÄÄ°TÄ°MÄ°")
    print("="*80)
    print()
    print("   Model eÄŸitimini Ã§alÄ±ÅŸtÄ±rmak iÃ§in aÅŸaÄŸÄ±daki satÄ±rÄ±n comment'ini kaldÄ±rÄ±n:")
    print("   â†’ trainer, model, tokenizer = train_model_with_trainer()")
    print()
    print("   âš ï¸  Not: EÄŸitim birkaÃ§ dakika sÃ¼rebilir ve GPU kullanÄ±mÄ±nÄ± Ã¶nerilir.")
    print()
    
    # Model eÄŸitimi (uncomment to run / Ã‡alÄ±ÅŸtÄ±rmak iÃ§in comment'i kaldÄ±r)
    trainer, model, tokenizer = train_model_with_trainer()
    
    # EÄŸitimden sonra inference Ã¶rneÄŸi
    print("\n=== Inference Ã–rneÄŸi ===")
    test_text = "Bu muhteÅŸem bir film, Ã§ok beÄŸendim!"
    inputs = tokenizer(test_text, return_tensors="pt", truncation=True, padding=True)
    outputs = model(**inputs)
    prediction = torch.argmax(outputs.logits, dim=-1)
    sentiment = "POZÄ°TÄ°F" if prediction.item() == 1 else "NEGATÄ°F"
    print(f"Metin: {test_text}")
    print(f"Tahmin: {sentiment}")
    
    # -------------------------------------------------------------------------
    # SONUÃ‡
    # -------------------------------------------------------------------------
    print("\n" + "="*80)
    print("âœ… DEMO TAMAMLANDI!")
    print("="*80)
    print()
    print("ğŸ“š Ã–ÄRENÄ°LEN KONULAR:")
    print("   âœ“ Hugging Face Datasets kÃ¼tÃ¼phanesi kullanÄ±mÄ±")
    print("   âœ“ Dataset yÃ¼kleme, hazÄ±rlama ve dÃ¶nÃ¼ÅŸÃ¼mleri")
    print("   âœ“ Tokenization ve data collation")
    print("   âœ“ Trainer API ile otomatik eÄŸitim")
    print("   âœ“ Training arguments ve hiperparametre ayarlama")
    print("   âœ“ Metric hesaplama ve model deÄŸerlendirme")
    print("   âœ“ Model kaydetme ve yÃ¼kleme")
    print()
    print("ğŸš€ SONRAKI ADIMLAR:")
    print("   1. Kendi veri setiniz ile deneyin")
    print("   2. FarklÄ± modeller test edin (BERT, RoBERTa, etc.)")
    print("   3. Hiperparametre optimizasyonu yapÄ±n")
    print("   4. WandB/TensorBoard ile monitoring ekleyin")
    print("   5. Production deployment iÃ§in model optimize edin")
    print()
    print("="*80 + "\n")


# =============================================================================
# EK NOTLAR VE KAYNAKLAR
# =============================================================================

"""
FAYDALÎ™ KAYNAKLAR:

1. Hugging Face Datasets DokÃ¼mantasyonu:
   https://huggingface.co/docs/datasets/

2. Transformers Trainer Guide:
   https://huggingface.co/docs/transformers/main_classes/trainer

3. Training Arguments Full List:
   https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments

4. Model Hub:
   https://huggingface.co/models

5. Dataset Hub:
   https://huggingface.co/datasets


SÎ™KÃ‡A SORULAN SORULAR (FAQ):

Q: GPU olmadan eÄŸitim yapabilir miyim?
A: Evet, ancak CPU'da Ã§ok daha yavaÅŸ olacaktÄ±r. KÃ¼Ã§Ã¼k modeller ve veri setleri 
   ile baÅŸlayÄ±n.

Q: Out of memory hatasÄ± alÄ±yorum, ne yapmalÄ±yÄ±m?
A: 1) Batch size'Ä± kÃ¼Ã§Ã¼lt
   2) Gradient accumulation kullan
   3) FP16 kullan
   4) Daha kÃ¼Ã§Ã¼k model seÃ§

Q: En iyi hiperparametreleri nasÄ±l bulurum?
A: Hyperparameter search kullanÄ±n (Optuna, Ray Tune ile entegre)
   https://huggingface.co/docs/transformers/hpo_train

Q: Ã‡oklu GPU'da eÄŸitim nasÄ±l yapÄ±lÄ±r?
A: `python -m torch.distributed.launch --nproc_per_node=NUM_GPUS script.py`
   veya `accelerate launch script.py`

Q: Custom loss function nasÄ±l kullanÄ±rÄ±m?
A: Trainer'Ä± subclass yapÄ±n ve compute_loss() methodunu override edin.


PERFORMANS OPTÄ°MÄ°ZASYON Ä°PUÃ‡LARI:

1. Veri YÃ¼kleme:
   - dataloader_num_workers > 0 (paralel yÃ¼kleme)
   - pin_memory=True (GPU iÃ§in)
   - prefetch_factor > 1 (Ã¶nceden yÃ¼kleme)

2. Model Optimizasyonu:
   - gradient_checkpointing: RAM tasarrufu
   - fp16/bf16: HÄ±zlanma
   - torch.compile(): PyTorch 2.0+ hÄ±zlanma

3. Batch Size:
   - MÃ¼mkÃ¼n olan en bÃ¼yÃ¼k batch size kullanÄ±n
   - Gradient accumulation ile efektif size artÄ±rÄ±n

4. Learning Rate:
   - Batch size ile birlikte scale edin
   - Linear warmup kullanÄ±n
   - Learning rate finder kullanÄ±n

5. Distributed Training:
   - DDP (DistributedDataParallel)
   - FSDP (Fully Sharded Data Parallel) bÃ¼yÃ¼k modeller iÃ§in
   - DeepSpeed entegrasyonu


PRODUCTION DEPLOYMENT:

1. Model Kaydetme:
   - trainer.save_model() veya model.save_pretrained()
   - Tokenizer'Ä± da kaydetmeyi unutmayÄ±n

2. Model Boyutu KÃ¼Ã§Ã¼ltme:
   - Quantization (int8, int4)
   - Pruning (aÄŸÄ±rlÄ±k budama)
   - Distillation (Ã¶ÄŸretmen-Ã¶ÄŸrenci)

3. Inference Optimizasyonu:
   - ONNX export
   - TensorRT
   - TorchScript

4. API Servisi:
   - FastAPI ile REST API
   - Triton Inference Server
   - HuggingFace Inference API


GÃœVENLÄ°K VE ETÄ°K HUSUSLAR:

- Veri gizliliÄŸi: Hassas veriyi model eÄŸitiminde kullanmayÄ±n
- Bias kontrolÃ¼: Model Ã§Ä±ktÄ±larÄ±nÄ± adalet aÃ§Ä±sÄ±ndan deÄŸerlendirin
- Model kÃ¶tÃ¼ye kullanÄ±mÄ±: Responsible AI prensiplerini takip edin
- Lisanslama: Model ve veri lisanslarÄ±na dikkat edin


Son GÃ¼ncelleme: KasÄ±m 2025
Kairu AI Bootcamp - Hafta 6
"""