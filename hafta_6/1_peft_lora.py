"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                 PEFT (Parameter Efficient Fine-Tuning) ve LoRA              â•‘
â•‘                          DetaylÄ± EÄŸitim Scripti                             â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

GENEL BAKIÅ:
-----------
Bu script, modern dil modellerini verimli bir ÅŸekilde fine-tune etmek iÃ§in 
PEFT (Parameter Efficient Fine-Tuning) yaklaÅŸÄ±mÄ±nÄ± ve Ã¶zellikle LoRA 
(Low-Rank Adaptation) tekniÄŸini kullanmayÄ± gÃ¶sterir.

TEMEL KAVRAMLAR:
---------------
1. PEFT (Parameter Efficient Fine-Tuning):
   - BÃ¼yÃ¼k modellerin tÃ¼m parametrelerini eÄŸitmek yerine, sadece kÃ¼Ã§Ã¼k bir 
     kÄ±smÄ±nÄ± veya ek adapter katmanlarÄ±nÄ± eÄŸitme yaklaÅŸÄ±mÄ±
   - Avantajlar: Daha az bellek, daha hÄ±zlÄ± eÄŸitim, daha az disk kullanÄ±mÄ±

2. LoRA (Low-Rank Adaptation):
   - PEFT'in en popÃ¼ler tekniklerinden biri
   - Orijinal model aÄŸÄ±rlÄ±klarÄ±nÄ± dondurup, dÃ¼ÅŸÃ¼k rankli (low-rank) 
     matrisler ekleyerek adaptasyon saÄŸlar
   - Matematiksel formÃ¼l: W = Wâ‚€ + Î”W = Wâ‚€ + BA
     * Wâ‚€: Orijinal aÄŸÄ±rlÄ±k matrisi (dondurulmuÅŸ)
     * B, A: DÃ¼ÅŸÃ¼k rankli matrisler (eÄŸitilebilir)
     * rank(BA) << rank(Wâ‚€)

KULLANIM ALANLARI:
-----------------
- Chatbot Ã¶zelleÅŸtirme
- Domain-specific dil modelleri (tÄ±bbi, hukuki, teknik vb.)
- Ã‡ok gÃ¶revli Ã¶ÄŸrenme (multi-task learning)
- KiÅŸiselleÅŸtirilmiÅŸ asistanlar
- DÃ¼ÅŸÃ¼k kaynaklÄ± ortamlarda model eÄŸitimi

DOSYA YAPISI:
------------
1. KÃ¼tÃ¼phane Ä°mportlarÄ± ve YapÄ±landÄ±rma
2. Model Kurulum FonksiyonlarÄ±
3. Veri HazÄ±rlama FonksiyonlarÄ±
4. EÄŸitim FonksiyonlarÄ±
5. Analiz ve KarÅŸÄ±laÅŸtÄ±rma FonksiyonlarÄ±
6. Ana Ã‡alÄ±ÅŸtÄ±rma BloÄŸu

Yazar: Kairu LLM Bootcamp - Hafta 6
Tarih: 2025
Versiyon: 2.0 (GeliÅŸtirilmiÅŸ ve TÃ¼rkÃ§eleÅŸtirilmiÅŸ)
"""

# ============================================================================
# BÃ–LÃœM 1: KÃœTÃœPHANE Ä°MPORTLARI
# ============================================================================

import torch  # PyTorch - Derin Ã¶ÄŸrenme framework'Ã¼
from transformers import (
    AutoTokenizer,           # Otomatik tokenizer yÃ¼kleyici
    AutoModelForCausalLM,    # Causal Language Model (GPT-tarzÄ±) otomatik yÃ¼kleyici
    TrainingArguments,        # EÄŸitim parametrelerini yapÄ±landÄ±rma sÄ±nÄ±fÄ±
    Trainer,                  # Hugging Face'in yÃ¼ksek seviye eÄŸitim API'si
    DataCollatorForLanguageModeling  # Language modeling iÃ§in data collator
)
from peft import (
    LoraConfig,      # LoRA yapÄ±landÄ±rma sÄ±nÄ±fÄ±
    get_peft_model,  # Standart modeli PEFT modeline dÃ¶nÃ¼ÅŸtÃ¼ren fonksiyon
    TaskType         # GÃ¶rev tiplerini tanÄ±mlayan enum
)
from datasets import Dataset  # Hugging Face Datasets kÃ¼tÃ¼phanesi


# ============================================================================
# BÃ–LÃœM 2: GLOBAL YAPILANDIRMA VE SABITLER
# ============================================================================

# Model YapÄ±landÄ±rmasÄ±
DEFAULT_MODEL_NAME = "microsoft/DialoGPT-medium"  # VarsayÄ±lan kullanÄ±lacak model
MAX_SEQUENCE_LENGTH = 512                         # Maksimum token uzunluÄŸu

# LoRA Hiperparametreleri
LORA_RANK = 16              # Low-rank matrisin rank'Ä± (r parametresi)
                            # Daha yÃ¼ksek r = daha fazla kapasite ama daha fazla parametre
                            # Tipik deÄŸerler: 4, 8, 16, 32, 64
                            
LORA_ALPHA = 32             # LoRA scaling faktÃ¶rÃ¼ (Î± parametresi)
                            # Genellikle r'nin 1-2 katÄ± olarak ayarlanÄ±r
                            # FormÃ¼l: scaling = Î± / r
                            
LORA_DROPOUT = 0.1          # LoRA katmanlarÄ±nda dropout oranÄ±
                            # Overfitting'i Ã¶nlemek iÃ§in kullanÄ±lÄ±r
                            
# DialoGPT iÃ§in hedef modÃ¼ller
# Bu modÃ¼ller, transformer'Ä±n attention ve projection katmanlarÄ±dÄ±r
LORA_TARGET_MODULES = ["c_attn", "c_proj"]

# EÄŸitim YapÄ±landÄ±rmasÄ±
OUTPUT_DIR = "./lora_results"           # Checkpoint'lerin kaydedileceÄŸi klasÃ¶r
FINAL_MODEL_DIR = "./lora_model"        # Final modelin kaydedileceÄŸi klasÃ¶r
NUM_TRAIN_EPOCHS = 3                    # Toplam eÄŸitim epoch sayÄ±sÄ±
BATCH_SIZE = 2                          # Her cihaz iÃ§in batch size
GRADIENT_ACCUMULATION_STEPS = 4         # Gradient accumulation adÄ±mlarÄ±
WARMUP_STEPS = 100                      # Learning rate warmup adÄ±mlarÄ±
LOGGING_STEPS = 10                      # Her kaÃ§ adÄ±mda log yazÄ±lacaÄŸÄ±


# ============================================================================
# BÃ–LÃœM 3: MODEL KURULUM FONKSÄ°YONLARI
# ============================================================================

def setup_lora_model(model_name: str = DEFAULT_MODEL_NAME) -> tuple:
    """
    LoRA konfigÃ¼rasyonu ile dil modelini hazÄ±rlar ve yÃ¼kler.
    
    Bu fonksiyon ÅŸu adÄ±mlarÄ± gerÃ§ekleÅŸtirir:
    1. Pre-trained model ve tokenizer'Ä± indirir/yÃ¼kler
    2. Tokenizer yapÄ±landÄ±rmasÄ±nÄ± optimize eder
    3. LoRA konfigÃ¼rasyonunu oluÅŸturur
    4. Standart modeli PEFT modeline dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r
    5. EÄŸitilebilir parametreleri analiz eder ve raporlar
    
    LoRA NEDÄ°R?
    -----------
    LoRA (Low-Rank Adaptation), bÃ¼yÃ¼k dil modellerini eÄŸitmek iÃ§in 
    memory-efficient bir yÃ¶ntemdir. Temel prensibi:
    
    - Orijinal model aÄŸÄ±rlÄ±klarÄ± Wâ‚€ dondurulur (freeze edilir)
    - Bunun yerine, dÃ¼ÅŸÃ¼k rankli iki matris (A ve B) eÄŸitilir
    - Final Ã§Ä±ktÄ±: W = Wâ‚€ + BA ÅŸeklinde hesaplanÄ±r
    - rank(BA) << rank(Wâ‚€) olduÄŸu iÃ§in Ã§ok daha az parametre eÄŸitilir
    
    Ã–rnek: 
    - Orijinal aÄŸÄ±rlÄ±k matrisi: 1024x1024 = 1,048,576 parametre
    - LoRA ile (r=16): (1024x16) + (16x1024) = 32,768 parametre
    - %96.9 parametre tasarrufu!
    
    Parametreler:
    ------------
    model_name : str
        Hugging Face model hub'dan yÃ¼klenecek model adÄ±.
        VarsayÄ±lan: "microsoft/DialoGPT-medium"
        DiÄŸer Ã¶rnekler: "gpt2", "gpt2-large", "EleutherAI/gpt-neo-1.3B"
    
    DÃ¶ndÃ¼rÃ¼r:
    --------
    tuple : (model, tokenizer)
        model : PeftModel
            LoRA adapter'larÄ± eklenmiÅŸ PEFT modeli
        tokenizer : PreTrainedTokenizer
            Model ile uyumlu tokenizer
    
    Ã–rnek KullanÄ±m:
    --------------
    >>> model, tokenizer = setup_lora_model("gpt2")
    >>> model.print_trainable_parameters()
    trainable params: 294,912 || all params: 124,734,720 || trainable%: 0.2364
    """
    
    print(f"\n{'='*80}")
    print(f"ğŸ”§ MODEL KURULUMU BAÅLIYOR")
    print(f"{'='*80}")
    print(f"ğŸ“¦ Model: {model_name}")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # AdÄ±m 1: Tokenizer'Ä± yÃ¼kle
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print(f"\n[1/5] Tokenizer yÃ¼kleniyor...")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    
    # Pad token kontrolÃ¼ ve ayarlamasÄ±
    # BazÄ± modellerde pad_token tanÄ±mlÄ± deÄŸildir, bu durumda eos_token kullanÄ±lÄ±r
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        print(f"   â„¹ï¸  Pad token ayarlandÄ±: '{tokenizer.eos_token}'")
    
    print(f"   âœ… Tokenizer baÅŸarÄ±yla yÃ¼klendi")
    print(f"   ğŸ“Š Vocabulary boyutu: {len(tokenizer):,} token")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # AdÄ±m 2: Pre-trained modeli yÃ¼kle
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print(f"\n[2/5] Pre-trained model yÃ¼kleniyor...")
    model = AutoModelForCausalLM.from_pretrained(model_name)
    
    # Model bilgilerini gÃ¶ster
    total_params_original = sum(p.numel() for p in model.parameters())
    print(f"   âœ… Model baÅŸarÄ±yla yÃ¼klendi")
    print(f"   ğŸ“Š Toplam parametre sayÄ±sÄ±: {total_params_original:,}")
    print(f"   ğŸ’¾ Tahmini boyut: {total_params_original * 4 / (1024**3):.2f} GB (float32)")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # AdÄ±m 3: LoRA konfigÃ¼rasyonunu oluÅŸtur
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print(f"\n[3/5] LoRA konfigÃ¼rasyonu oluÅŸturuluyor...")
    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,        # GÃ¶rev tipi: Causal Language Modeling
        r=LORA_RANK,                          # Low-rank decomposition rank deÄŸeri
        lora_alpha=LORA_ALPHA,                # LoRA scaling faktÃ¶rÃ¼
        lora_dropout=LORA_DROPOUT,            # Dropout oranÄ± (regularization)
        target_modules=LORA_TARGET_MODULES,   # LoRA'nÄ±n uygulanacaÄŸÄ± modÃ¼ller
        bias="none",                          # Bias parametrelerini eÄŸitme ("none", "all", "lora_only")
        inference_mode=False,                 # EÄŸitim modu (False = eÄŸitim, True = inference)
    )
    
    print(f"   âœ… LoRA konfigÃ¼rasyonu oluÅŸturuldu")
    print(f"   ğŸ“ Rank (r): {LORA_RANK}")
    print(f"   ğŸšï¸  Alpha (Î±): {LORA_ALPHA}")
    print(f"   ğŸ“‰ Scaling faktÃ¶rÃ¼ (Î±/r): {LORA_ALPHA/LORA_RANK}")
    print(f"   ğŸ’§ Dropout: {LORA_DROPOUT}")
    print(f"   ğŸ¯ Hedef modÃ¼ller: {', '.join(LORA_TARGET_MODULES)}")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # AdÄ±m 4: PEFT modelini oluÅŸtur
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print(f"\n[4/5] PEFT modeli oluÅŸturuluyor...")
    model = get_peft_model(model, lora_config)
    
    print(f"   âœ… Model PEFT modeline dÃ¶nÃ¼ÅŸtÃ¼rÃ¼ldÃ¼")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # AdÄ±m 5: EÄŸitilebilir parametreleri analiz et
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print(f"\n[5/5] Parametre analizi yapÄ±lÄ±yor...")
    print(f"\n{'â”€'*80}")
    
    # Hugging Face PEFT'in built-in fonksiyonu ile Ã¶zet bilgi
    model.print_trainable_parameters()
    
    # DetaylÄ± analiz
    print(f"\nğŸ“‹ DETAYLI PARAMETRE ANALÄ°ZÄ°:")
    print(f"{'â”€'*80}")
    
    # EÄŸitilebilir parametreler
    print(f"\nâœ… EÄÄ°TÄ°LEBÄ°LÄ°R PARAMETRELER (LoRA Adapter'larÄ±):")
    print(f"{'â”€'*80}")
    trainable_count = 0
    for name, param in model.named_parameters():
        if param.requires_grad:
            print(f"   â€¢ {name:50s} â†’ {str(param.shape):20s} ({param.numel():,} params)")
            trainable_count += 1
    
    print(f"\n   ğŸ“Š Toplam eÄŸitilebilir katman sayÄ±sÄ±: {trainable_count}")
    
    # DondurulmuÅŸ parametreler (Ã¶rnekleme)
    print(f"\nâ„ï¸  DONDURULMUÅ PARAMETRELER (Orijinal Model):")
    print(f"{'â”€'*80}")
    frozen_count = 0
    total_frozen = 0
    for name, param in model.named_parameters():
        if not param.requires_grad:
            total_frozen += 1
            if frozen_count < 5:  # Ä°lk 5 Ã¶rneÄŸi gÃ¶ster
                print(f"   â€¢ {name:50s} â†’ {str(param.shape):20s} ({param.numel():,} params)")
                frozen_count += 1
    
    if total_frozen > 5:
        print(f"   ... ve {total_frozen - 5:,} adet daha dondurulmuÅŸ parametre")
    
    print(f"\n   ğŸ“Š Toplam dondurulmuÅŸ katman sayÄ±sÄ±: {total_frozen}")
    
    # Ã–zet istatistikler
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"\n{'='*80}")
    print(f"ğŸ“Š Ã–ZET Ä°STATÄ°STÄ°KLER:")
    print(f"{'='*80}")
    print(f"   Toplam parametreler      : {total_params:,}")
    print(f"   EÄŸitilebilir parametreler: {trainable_params:,}")
    print(f"   DondurulmuÅŸ parametreler : {total_params - trainable_params:,}")
    print(f"   EÄŸitilebilir oran        : {100 * trainable_params / total_params:.4f}%")
    print(f"   Bellek tasarrufu         : ~{100 * (1 - trainable_params / total_params):.2f}%")
    print(f"{'='*80}\n")
    
    return model, tokenizer


# ============================================================================
# BÃ–LÃœM 4: VERÄ° HAZIRLAMA FONKSÄ°YONLARI
# ============================================================================

def prepare_dataset(tokenizer, max_length: int = MAX_SEQUENCE_LENGTH) -> Dataset:
    """
    Model eÄŸitimi iÃ§in basit bir konuÅŸma dataseti oluÅŸturur ve tokenize eder.
    
    Bu fonksiyon ÅŸu iÅŸlemleri gerÃ§ekleÅŸtirir:
    1. Ã–rnek konuÅŸma metinlerini oluÅŸturur
    2. Metinleri tokenize eder (sayÄ±sal forma Ã§evirir)
    3. Padding ve truncation uygular
    4. Hugging Face Dataset formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r
    
    TOKENIZATION NEDÄ°R?
    ------------------
    Tokenization, metinleri model tarafÄ±ndan iÅŸlenebilir sayÄ±sal forma
    dÃ¶nÃ¼ÅŸtÃ¼rme iÅŸlemidir. Ã–rnek:
    
    Metin: "Merhaba dÃ¼nya!"
    Tokenlar: ["Mer", "haba", " dÃ¼n", "ya", "!"]
    Token IDs: [5673, 8901, 2134, 6789, 0]
    
    PADDING VE TRUNCATION:
    ---------------------
    - Padding: KÄ±sa cÃ¼mleleri sabit uzunluÄŸa getirmek iÃ§in token ekleme
    - Truncation: Uzun cÃ¼mleleri max_length'e gÃ¶re kesme
    
    Parametreler:
    ------------
    tokenizer : PreTrainedTokenizer
        Metinleri tokenize etmek iÃ§in kullanÄ±lacak tokenizer
    max_length : int
        Maksimum sequence uzunluÄŸu (varsayÄ±lan: 512)
        Daha uzun sequenceler kesilir, daha kÄ±sa olanlar padding ile doldurulur
    
    DÃ¶ndÃ¼rÃ¼r:
    --------
    Dataset
        Tokenize edilmiÅŸ Hugging Face Dataset objesi
        Her Ã¶rnek ÅŸu sÃ¼tunlarÄ± iÃ§erir:
        - input_ids: Token ID'leri
        - attention_mask: Hangi tokenlerin gerÃ§ek, hangilerinin padding olduÄŸunu gÃ¶sterir
        - labels: EÄŸitim iÃ§in hedef token'lar (input_ids ile aynÄ±)
    
    Ã–rnek KullanÄ±m:
    --------------
    >>> tokenizer = AutoTokenizer.from_pretrained("gpt2")
    >>> dataset = prepare_dataset(tokenizer, max_length=128)
    >>> print(dataset)
    Dataset({
        features: ['input_ids', 'attention_mask', 'labels'],
        num_rows: 10
    })
    """
    
    print(f"\n{'='*80}")
    print(f"ğŸ“š VERÄ° SETÄ° HAZIRLANIYOR")
    print(f"{'='*80}")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # AdÄ±m 1: Ã–rnek konuÅŸma verilerini tanÄ±mla
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # NOT: GerÃ§ek bir projede bu veriler bir dosyadan veya veritabanÄ±ndan
    # yÃ¼klenirdi. Bu Ã¶rnekte gÃ¶sterim amaÃ§lÄ± basit veriler kullanÄ±yoruz.
    
    conversations = [
        # Temel selamlaÅŸma
        "KullanÄ±cÄ±: Merhaba! Asistan: Merhaba! Size nasÄ±l yardÄ±mcÄ± olabilirim?",
        
        # Programlama sorularÄ±
        "KullanÄ±cÄ±: Python Ã¶ÄŸrenmek istiyorum. Asistan: Harika! Python'a nereden baÅŸlamak istiyorsunuz? Temel sÃ¶zdizimi mi yoksa belirli bir proje mi?",
        "KullanÄ±cÄ±: Makine Ã¶ÄŸrenmesi nedir? Asistan: Makine Ã¶ÄŸrenmesi, bilgisayarlarÄ±n verilerden Ã¶ÄŸrenmesini ve deneyimle performanslarÄ±nÄ± geliÅŸtirmelerini saÄŸlayan bir tekniktir.",
        
        # Teknik sorular
        "KullanÄ±cÄ±: Derin Ã¶ÄŸrenme ile makine Ã¶ÄŸrenmesi arasÄ±ndaki fark nedir? Asistan: Derin Ã¶ÄŸrenme, makine Ã¶ÄŸrenmesinin bir alt dalÄ±dÄ±r ve yapay sinir aÄŸlarÄ± kullanÄ±r.",
        "KullanÄ±cÄ±: Neural network nasÄ±l Ã§alÄ±ÅŸÄ±r? Asistan: Neural network, birbirine baÄŸlÄ± nÃ¶ronlardan oluÅŸan katmanlar halinde Ã§alÄ±ÅŸÄ±r ve veriyi iÅŸleyerek Ã¶ÄŸrenir.",
        
        # Pratik Ã¶rnekler
        "KullanÄ±cÄ±: Bir chatbot nasÄ±l yapabilirim? Asistan: Chatbot yapmak iÃ§in Ã¶nce bir dil modeli seÃ§meli, sonra kendi verinizle fine-tune etmelisiniz.",
        "KullanÄ±cÄ±: Fine-tuning nedir? Asistan: Fine-tuning, Ã¶nceden eÄŸitilmiÅŸ bir modeli kendi Ã¶zel gÃ¶reviniz iÃ§in uyarlama sÃ¼recidir.",
        
        # Ä°leri seviye
        "KullanÄ±cÄ±: LoRA ne iÅŸe yarar? Asistan: LoRA, bÃ¼yÃ¼k modelleri verimli ÅŸekilde fine-tune etmek iÃ§in kullanÄ±lan bir tekniktir ve Ã§ok az parametre gÃ¼ncelleyerek iyi sonuÃ§lar verir.",
        "KullanÄ±cÄ±: Transformer mimarisi nedir? Asistan: Transformer, attention mekanizmasÄ± kullanan ve modern NLP'nin temelini oluÅŸturan bir sinir aÄŸÄ± mimarisidir.",
        "KullanÄ±cÄ±: GPT modelleri nasÄ±l Ã§alÄ±ÅŸÄ±r? Asistan: GPT modelleri, transformer decoder kullanarak Ã¶nceki tokenlere bakarak sonraki tokenÄ± tahmin eder.",
    ]
    
    print(f"   ğŸ“ Toplam konuÅŸma Ã¶rneÄŸi: {len(conversations)}")
    print(f"   ğŸ“ Maksimum sequence uzunluÄŸu: {max_length}")
    
    # Ã–rnek bir konuÅŸmayÄ± gÃ¶ster
    print(f"\n   ğŸ’¬ Ã–rnek konuÅŸma:")
    print(f"   {'-'*76}")
    print(f"   {conversations[0]}")
    print(f"   {'-'*76}")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # AdÄ±m 2: Tokenization fonksiyonunu tanÄ±mla
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def tokenize_function(examples):
        """
        Batch iÃ§indeki metinleri tokenize eder.
        
        Bu iÃ§ fonksiyon, dataset.map() tarafÄ±ndan Ã§aÄŸrÄ±lÄ±r ve her batch
        iÃ§in tokenization iÅŸlemini gerÃ§ekleÅŸtirir.
        """
        result = tokenizer(
            examples["text"],                  # Tokenize edilecek metinler
            truncation=True,                   # Uzun metinleri max_length'e gÃ¶re kes
            padding="max_length",              # TÃ¼m sequenceleri aynÄ± uzunluÄŸa getir
            max_length=max_length,             # Maksimum sequence uzunluÄŸu
            return_tensors=None,               # Dataset map iÃ§in None kullan (batching iÃ§in)
        )
        # Causal LM iÃ§in labels ekliyoruz (input_ids'in kopyasÄ±)
        result["labels"] = result["input_ids"].copy()
        return result
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # AdÄ±m 3: Dataset oluÅŸtur ve tokenize et
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print(f"\n   ğŸ”„ Tokenization iÅŸlemi baÅŸlÄ±yor...")
    
    # Ham dataseti oluÅŸtur
    dataset = Dataset.from_dict({"text": conversations})
    
    # Tokenization uygula (batched=True daha hÄ±zlÄ± iÅŸlem saÄŸlar)
    tokenized_dataset = dataset.map(
        tokenize_function, 
        batched=True,
        remove_columns=["text"],  # Orijinal text kolonunu kaldÄ±r (artÄ±k gerekli deÄŸil)
        desc="Tokenizing"         # Progress bar iÃ§in aÃ§Ä±klama
    )
    
    print(f"   âœ… Tokenization tamamlandÄ±")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # AdÄ±m 4: Dataset istatistiklerini gÃ¶ster
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print(f"\n   ğŸ“Š Dataset Ä°statistikleri:")
    print(f"   {'-'*76}")
    print(f"   â€¢ Toplam Ã¶rnek sayÄ±sÄ±: {len(tokenized_dataset)}")
    print(f"   â€¢ Ã–zellikler: {list(tokenized_dataset.features.keys())}")
    print(f"   â€¢ Her sequence uzunluÄŸu: {max_length} token")
    
    # Ä°lk Ã¶rneÄŸin detaylarÄ±nÄ± gÃ¶ster
    if len(tokenized_dataset) > 0:
        first_example = tokenized_dataset[0]
        print(f"\n   ğŸ” Ä°lk Ã–rnek DetayÄ±:")
        print(f"   {'-'*76}")
        print(f"   â€¢ Input IDs sayÄ±sÄ±: {len(first_example['input_ids'])}")
        print(f"   â€¢ Attention mask sayÄ±sÄ±: {len(first_example['attention_mask'])}")
        print(f"   â€¢ Labels sayÄ±sÄ±: {len(first_example['labels'])}")
        print(f"   â€¢ GerÃ§ek token sayÄ±sÄ±: {sum(first_example['attention_mask'])}")
        print(f"   â€¢ Padding token sayÄ±sÄ±: {len(first_example['attention_mask']) - sum(first_example['attention_mask'])}")
    
    print(f"{'='*80}\n")
    
    return tokenized_dataset


# ============================================================================
# BÃ–LÃœM 5: EÄÄ°TÄ°M FONKSÄ°YONLARI
# ============================================================================

def train_lora_model() -> tuple:
    """
    LoRA kullanarak dil modelini fine-tune eder.
    
    Bu fonksiyon, complete bir eÄŸitim pipeline'Ä±nÄ± orchestrate eder:
    1. Model ve tokenizer kurulumu
    2. Dataset hazÄ±rlÄ±ÄŸÄ± ve preprocessing
    3. Training arguments yapÄ±landÄ±rmasÄ±
    4. Hugging Face Trainer ile eÄŸitim
    5. Model ve tokenizer kaydetme
    
    EÄÄ°TÄ°M SÃœRECÄ°:
    --------------
    EÄŸitim ÅŸu aÅŸamalardan oluÅŸur:
    
    1. Forward Pass: Input verisi modelden geÃ§irilir, Ã§Ä±ktÄ± alÄ±nÄ±r
    2. Loss Hesaplama: Tahmin ile gerÃ§ek deÄŸer arasÄ±ndaki fark hesaplanÄ±r
    3. Backward Pass: Gradient'lar hesaplanÄ±r (backpropagation)
    4. Optimizer Step: Parametreler gÃ¼ncellenir
    5. Tekrar (epoch ve batch sayÄ±sÄ±na gÃ¶re)
    
    GRADIENT ACCUMULATION:
    ---------------------
    BÃ¼yÃ¼k batch size kullanamÄ±yorsak, gradient accumulation ile
    birden fazla kÃ¼Ã§Ã¼k batch'in gradient'larÄ±nÄ± biriktirip tek seferde
    update yapabiliriz.
    
    Ã–rnek:
    - GerÃ§ek batch size = batch_size Ã— gradient_accumulation_steps
    - batch_size=2, accumulation=4 â†’ Efektif batch size = 8
    
    WARMUP:
    -------
    EÄŸitimin baÅŸÄ±nda learning rate'i dÃ¼ÅŸÃ¼k tutup yavaÅŸÃ§a artÄ±rma stratejisi.
    Ani bÃ¼yÃ¼k gÃ¼ncellemelerin modeli bozmasÄ±nÄ± Ã¶nler.
    
    DÃ¶ndÃ¼rÃ¼r:
    --------
    tuple : (model, tokenizer)
        model : PeftModel
            EÄŸitilmiÅŸ LoRA modeli
        tokenizer : PreTrainedTokenizer
            Model ile uyumlu tokenizer
    
    Ã–rnek KullanÄ±m:
    --------------
    >>> model, tokenizer = train_lora_model()
    ğŸ”§ MODEL KURULUMU BAÅLIYOR...
    ğŸ“š VERÄ° SETÄ° HAZIRLANIYOR...
    ğŸš€ EÄÄ°TÄ°M BAÅLIYOR...
    âœ… EÄŸitim tamamlandÄ±!
    """
    
    print(f"\n{'#'*80}")
    print(f"{'#'*80}")
    print(f"##{' '*76}##")
    print(f"##{'LoRA MODEL EÄÄ°TÄ°M PÄ°PELINE':^76}##")
    print(f"##{' '*76}##")
    print(f"{'#'*80}")
    print(f"{'#'*80}\n")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # AdÄ±m 1: Model ve Tokenizer Kurulumu
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print(f"[ADIM 1/4] Model Kurulumu")
    model, tokenizer = setup_lora_model()
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # AdÄ±m 2: Dataset HazÄ±rlÄ±ÄŸÄ±
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print(f"[ADIM 2/4] Dataset HazÄ±rlÄ±ÄŸÄ±")
    train_dataset = prepare_dataset(tokenizer)
    
    # Data Collator oluÅŸtur
    # Bu, batch'leri dinamik olarak padding yapar ve labels'Ä± dÃ¼zenler
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,  # Causal LM iÃ§in False (Masked LM iÃ§in True olur)
    )
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # AdÄ±m 3: Training Arguments YapÄ±landÄ±rmasÄ±
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print(f"[ADIM 3/4] Training Arguments YapÄ±landÄ±rmasÄ±")
    print(f"{'='*80}")
    
    training_args = TrainingArguments(
        # â”€â”€â”€ Ã‡Ä±ktÄ± ve Loglama â”€â”€â”€
        output_dir=OUTPUT_DIR,                      # Checkpoint'lerin kaydedileceÄŸi klasÃ¶r
        logging_dir=f"{OUTPUT_DIR}/logs",           # TensorBoard loglarÄ±
        logging_steps=LOGGING_STEPS,                # Her X step'te log kaydÄ±
        
        # â”€â”€â”€ EÄŸitim Hiperparametreleri â”€â”€â”€
        num_train_epochs=NUM_TRAIN_EPOCHS,          # KaÃ§ epoch eÄŸitim yapÄ±lacak
        per_device_train_batch_size=BATCH_SIZE,     # Her GPU/CPU iÃ§in batch size
        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,  # Gradient biriktirme adÄ±mlarÄ±
        
        # â”€â”€â”€ Optimizer ve Learning Rate â”€â”€â”€
        learning_rate=5e-5,                         # BaÅŸlangÄ±Ã§ learning rate
        warmup_steps=WARMUP_STEPS,                  # LR warmup adÄ±m sayÄ±sÄ±
        lr_scheduler_type="linear",                 # LR scheduler tipi (linear, cosine, etc.)
        weight_decay=0.01,                          # L2 regularization (overfitting Ã¶nler)
        
        # â”€â”€â”€ Checkpoint ve Kaydetme â”€â”€â”€
        save_strategy="epoch",                      # Her epoch'ta kaydet
        save_steps=500,                             # Alternatif: Her X step'te kaydet
        save_total_limit=2,                         # En fazla kaÃ§ checkpoint sakla
        load_best_model_at_end=False,               # EÄŸitim sonunda en iyi modeli yÃ¼kle
        
        # â”€â”€â”€ Performans OptimizasyonlarÄ± â”€â”€â”€
        fp16=False,                                 # 16-bit floating point (GPU'da hÄ±zlandÄ±rma)
        # bf16=False,                               # Brain float 16 (yeni GPU'larda)
        gradient_checkpointing=False,               # Bellek tasarrufu (daha yavaÅŸ ama daha az bellek)
        
        # â”€â”€â”€ DiÄŸer Ayarlar â”€â”€â”€
        report_to="none",                           # Loglama servisi (wandb, tensorboard, etc.)
        disable_tqdm=False,                         # Progress bar'Ä± kapat (False = aÃ§Ä±k)
        remove_unused_columns=True,                 # KullanÄ±lmayan kolonlarÄ± kaldÄ±r
        dataloader_num_workers=0,                   # Data loading iÃ§in worker sayÄ±sÄ±
        
        # â”€â”€â”€ Seed (Tekrarlanabilirlik iÃ§in) â”€â”€â”€
        seed=42,                                    # Random seed
    )
    
    print(f"\n   âš™ï¸  EÄÄ°TÄ°M AYARLARI:")
    print(f"   {'-'*76}")
    print(f"   ğŸ“ Output directory        : {OUTPUT_DIR}")
    print(f"   ğŸ”„ Epoch sayÄ±sÄ±            : {NUM_TRAIN_EPOCHS}")
    print(f"   ğŸ“¦ Batch size              : {BATCH_SIZE}")
    print(f"   ğŸ”¢ Gradient accumulation   : {GRADIENT_ACCUMULATION_STEPS}")
    print(f"   ğŸ“Š Efektif batch size      : {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}")
    print(f"   ğŸ“ˆ Learning rate           : {training_args.learning_rate}")
    print(f"   ğŸŒ¡ï¸  Warmup steps           : {WARMUP_STEPS}")
    print(f"   ğŸ’¾ Checkpoint stratejisi   : {training_args.save_strategy}")
    print(f"   ğŸ² Random seed             : {training_args.seed}")
    print(f"{'='*80}\n")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # AdÄ±m 4: Trainer OluÅŸturma ve EÄŸitimi BaÅŸlatma
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print(f"[ADIM 4/4] Trainer OluÅŸturma ve EÄŸitim")
    print(f"{'='*80}")
    
    # Trainer objesi oluÅŸtur
    # Trainer, Hugging Face'in high-level eÄŸitim API'sidir
    # Training loop, logging, checkpointing gibi tÃ¼m detaylarÄ± halleder
    trainer = Trainer(
        model=model,                    # EÄŸitilecek model (LoRA eklenmiÅŸ)
        args=training_args,             # Training ayarlarÄ±
        train_dataset=train_dataset,    # EÄŸitim dataseti
        tokenizer=tokenizer,            # Tokenizer (kaydetme iÃ§in gerekli)
        data_collator=data_collator,    # Data collator (batch hazÄ±rlama)
        # compute_metrics=...,          # Metrik hesaplama fonksiyonu (opsiyonel)
    )
    
    print(f"\n   âœ… Trainer baÅŸarÄ±yla oluÅŸturuldu")
    print(f"\n{'='*80}")
    print(f"ğŸš€ EÄÄ°TÄ°M BAÅLIYOR...")
    print(f"{'='*80}\n")
    
    # EÄÄ°TÄ°MÄ° BAÅLAT!
    # Bu satÄ±r tÃ¼m eÄŸitim sÃ¼recini Ã§alÄ±ÅŸtÄ±rÄ±r
    train_result = trainer.train()
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # EÄŸitim SonrasÄ± Ä°ÅŸlemler
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print(f"\n{'='*80}")
    print(f"âœ… EÄÄ°TÄ°M TAMAMLANDI!")
    print(f"{'='*80}")
    
    # EÄŸitim metriklerini gÃ¶ster
    print(f"\nğŸ“Š EÄÄ°TÄ°M METRÄ°KLERÄ°:")
    print(f"{'-'*80}")
    print(f"   â€¢ Training Loss     : {train_result.training_loss:.4f}")
    print(f"   â€¢ Toplam AdÄ±m       : {train_result.global_step}")
    print(f"   â€¢ Epoch SayÄ±sÄ±      : {NUM_TRAIN_EPOCHS}")
    
    # Model ve tokenizer'Ä± kaydet
    print(f"\nğŸ’¾ MODEL KAYDEDÄ°LÄ°YOR...")
    print(f"{'-'*80}")
    
    # LoRA adapter'larÄ±nÄ± kaydet
    trainer.save_model(FINAL_MODEL_DIR)
    print(f"   âœ… LoRA adapter'larÄ± kaydedildi: {FINAL_MODEL_DIR}")
    
    # Tokenizer'Ä± kaydet
    tokenizer.save_pretrained(FINAL_MODEL_DIR)
    print(f"   âœ… Tokenizer kaydedildi: {FINAL_MODEL_DIR}")
    
    # Training arguments'Ä± kaydet (referans iÃ§in)
    training_args_path = f"{FINAL_MODEL_DIR}/training_args.txt"
    with open(training_args_path, "w", encoding="utf-8") as f:
        f.write(training_args.to_json_string())
    print(f"   âœ… Training arguments kaydedildi: {training_args_path}")
    
    print(f"\n{'='*80}")
    print(f"ğŸ‰ TÃœM Ä°ÅLEMLER BAÅARIYLA TAMAMLANDI!")
    print(f"{'='*80}")
    print(f"\nğŸ“‚ Kaydedilen dosyalar:")
    print(f"   â€¢ LoRA Adapter     : {FINAL_MODEL_DIR}/adapter_model.bin")
    print(f"   â€¢ Adapter Config   : {FINAL_MODEL_DIR}/adapter_config.json")
    print(f"   â€¢ Tokenizer        : {FINAL_MODEL_DIR}/tokenizer.json")
    print(f"   â€¢ Training Args    : {FINAL_MODEL_DIR}/training_args.txt")
    print(f"\nğŸ’¡ Modeli yÃ¼klemek iÃ§in:")
    print(f'   >>> from peft import PeftModel')
    print(f'   >>> base_model = AutoModelForCausalLM.from_pretrained("{DEFAULT_MODEL_NAME}")')
    print(f'   >>> model = PeftModel.from_pretrained(base_model, "{FINAL_MODEL_DIR}")')
    print(f"{'='*80}\n")
    
    return model, tokenizer


# ============================================================================
# BÃ–LÃœM 6: ANALÄ°Z VE KARÅILAÅTIRMA FONKSÄ°YONLARI
# ============================================================================

def demonstrate_lora_benefits():
    """
    LoRA'nÄ±n saÄŸladÄ±ÄŸÄ± faydalarÄ± detaylÄ± bir ÅŸekilde gÃ¶sterir ve analiz eder.
    
    Bu fonksiyon ÅŸunlarÄ± yapar:
    1. LoRA'nÄ±n avantajlarÄ±nÄ± aÃ§Ä±klar
    2. GerÃ§ek bir model Ã¼zerinde parametre analizi yapar
    3. Bellek ve hesaplama tasarrufunu hesaplar
    4. Traditional fine-tuning ile karÅŸÄ±laÅŸtÄ±rma yapar
    
    LoRA AVANTAJLARI:
    ----------------
    
    1. BELLEK VERÄ°MLÄ°LÄ°ÄÄ°:
       - Sadece adapter katmanlarÄ± GPU belleÄŸinde tutulur
       - Base model aÄŸÄ±rlÄ±klarÄ± donuk olduÄŸu iÃ§in gradientleri saklanmaz
       - Ã–rnek: GPT-3 175B â†’ Sadece ~100MB adapter
    
    2. HIZLI EÄÄ°TÄ°M:
       - Daha az parametre = daha az backpropagation
       - Daha az optimizer state (Adam iÃ§in Ã¶nemli)
       - 2-3x daha hÄ±zlÄ± eÄŸitim sÃ¼resi
    
    3. MODÃœLER YAPILAR:
       - FarklÄ± gÃ¶revler iÃ§in farklÄ± adapter'lar
       - Base model tek, gÃ¶rev baÅŸÄ±na adapter
       - Kolay geÃ§iÅŸ: Adapter'Ä± deÄŸiÅŸtir, gÃ¶rev deÄŸiÅŸti!
    
    4. DÃœÅÃœK DÄ°SK KULLANIMI:
       - Base model: 500MB-5GB
       - Her adapter: ~1-10MB
       - 100 gÃ¶rev iÃ§in: 1 base + 100 adapter â‰ˆ 1GB total
    
    5. DEPLOYMENT KOLAYLIÄI:
       - Base model bir kere deploy edilir
       - Her kullanÄ±cÄ±/gÃ¶rev iÃ§in sadece adapter yÃ¼klenir
       - Multi-tenant senaryolar iÃ§in ideal
    
    DÃ¶ndÃ¼rÃ¼r:
    --------
    None
        Fonksiyon sonuÃ§ dÃ¶ndÃ¼rmez, sadece detaylÄ± analiz Ã§Ä±ktÄ±larÄ± verir
    
    Ã–rnek KullanÄ±m:
    --------------
    >>> demonstrate_lora_benefits()
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                    LoRA AVANTAJLARI ANALÄ°ZÄ°                      â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    ...
    """
    
    print(f"\n{'â•”' + 'â•'*78 + 'â•—'}")
    print(f"â•‘{'LoRA AVANTAJLARI - DETAYLI ANALÄ°Z':^78}â•‘")
    print(f"{'â•š' + 'â•'*78 + 'â•'}\n")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # BÃ¶lÃ¼m 1: Teorik Avantajlar
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print(f"{'='*80}")
    print(f"ğŸ“š 1. TEORÄ°K AVANTAJLAR")
    print(f"{'='*80}\n")
    
    advantages = [
        {
            "icon": "ğŸ’¾",
            "title": "Bellek VerimliliÄŸi",
            "description": "Sadece adapter katmanlarÄ± eÄŸitilir, base model dondurulur",
            "benefit": "GPU belleÄŸi %70-90 oranÄ±nda azalÄ±r"
        },
        {
            "icon": "âš¡",
            "title": "HÄ±zlÄ± EÄŸitim",
            "description": "Daha az parametre gÃ¼ncellenir, daha az gradient hesaplanÄ±r",
            "benefit": "EÄŸitim sÃ¼resi 2-3x daha kÄ±sa"
        },
        {
            "icon": "ğŸ”§",
            "title": "ModÃ¼lerlik",
            "description": "Her gÃ¶rev iÃ§in ayrÄ± adapter, tek base model",
            "benefit": "Multi-task learning kolaylaÅŸÄ±r"
        },
        {
            "icon": "ğŸ’¿",
            "title": "DÃ¼ÅŸÃ¼k Disk KullanÄ±mÄ±",
            "description": "Sadece adapter aÄŸÄ±rlÄ±klarÄ± kaydedilir (~1-10 MB)",
            "benefit": "100 gÃ¶rev iÃ§in bile minimal disk kullanÄ±mÄ±"
        },
        {
            "icon": "ğŸš€",
            "title": "Kolay Deployment",
            "description": "Base model bir kere, adapter'lar ihtiyaÃ§ anÄ±nda yÃ¼klenir",
            "benefit": "Multi-tenant sistemler iÃ§in ideal"
        },
        {
            "icon": "ğŸ¯",
            "title": "DÃ¼ÅŸÃ¼k Catastrophic Forgetting",
            "description": "Base model deÄŸiÅŸmediÄŸi iÃ§in orijinal yetenekler korunur",
            "benefit": "Genel yetenekler + Ã¶zel gÃ¶rev becerileri"
        }
    ]
    
    for i, adv in enumerate(advantages, 1):
        print(f"{adv['icon']} {adv['title']}")
        print(f"   â””â”€ AÃ§Ä±klama : {adv['description']}")
        print(f"   â””â”€ Fayda    : {adv['benefit']}")
        print()
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # BÃ¶lÃ¼m 2: Pratik Demonstrasyon
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print(f"{'='*80}")
    print(f"ğŸ”¬ 2. PRATÄ°K DEMONSTRASYON - GERÃ‡EK MODEL ANALÄ°ZÄ°")
    print(f"{'='*80}\n")
    
    print(f"Model yÃ¼kleniyor (lÃ¼tfen bekleyin)...")
    model, tokenizer = setup_lora_model()
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # BÃ¶lÃ¼m 3: DetaylÄ± Parametre Analizi
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print(f"\n{'='*80}")
    print(f"ğŸ“Š 3. DETAYLI PARAMETRE ANALÄ°ZÄ°")
    print(f"{'='*80}\n")
    
    # Parametre istatistiklerini hesapla
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    frozen_params = total_params - trainable_params
    trainable_percentage = 100 * trainable_params / total_params
    
    # Bellek hesaplamalarÄ± (float32 iÃ§in 4 byte/param)
    bytes_per_param = 4  # float32
    total_memory_mb = (total_params * bytes_per_param) / (1024**2)
    trainable_memory_mb = (trainable_params * bytes_per_param) / (1024**2)
    frozen_memory_mb = (frozen_params * bytes_per_param) / (1024**2)
    
    # Gradient belleÄŸi (sadece trainable parametreler iÃ§in)
    # Adam optimizer: params + momentum + variance = 3x
    gradient_memory_mb = trainable_memory_mb * 3
    total_training_memory_mb = frozen_memory_mb + trainable_memory_mb + gradient_memory_mb
    
    # Traditional fine-tuning ile karÅŸÄ±laÅŸtÄ±rma
    traditional_gradient_memory_mb = total_memory_mb * 3
    traditional_total_memory_mb = total_memory_mb + traditional_gradient_memory_mb
    memory_saving_percentage = 100 * (1 - total_training_memory_mb / traditional_total_memory_mb)
    
    # SonuÃ§larÄ± gÃ¶ster
    print(f"â”Œ{'â”€'*78}â”")
    print(f"â”‚ {'PARAMETRE Ä°STATÄ°STÄ°KLERÄ°':^76} â”‚")
    print(f"â”œ{'â”€'*78}â”¤")
    print(f"â”‚ Toplam Parametreler          â”‚ {total_params:>15,} params â”‚ {total_memory_mb:>8.2f} MB â”‚")
    print(f"â”‚ EÄŸitilebilir Parametreler    â”‚ {trainable_params:>15,} params â”‚ {trainable_memory_mb:>8.2f} MB â”‚")
    print(f"â”‚ DondurulmuÅŸ Parametreler     â”‚ {frozen_params:>15,} params â”‚ {frozen_memory_mb:>8.2f} MB â”‚")
    print(f"â”œ{'â”€'*78}â”¤")
    print(f"â”‚ EÄŸitilebilir Oran            â”‚ {trainable_percentage:>15.4f} %      â”‚            â”‚")
    print(f"â””{'â”€'*78}â”˜")
    
    print(f"\nâ”Œ{'â”€'*78}â”")
    print(f"â”‚ {'BELLEK KULLANIMI ANALÄ°ZÄ° (Adam Optimizer ile)':^76} â”‚")
    print(f"â”œ{'â”€'*78}â”¤")
    print(f"â”‚ Model AÄŸÄ±rlÄ±klarÄ± (Frozen)   â”‚                         â”‚ {frozen_memory_mb:>8.2f} MB â”‚")
    print(f"â”‚ Model AÄŸÄ±rlÄ±klarÄ± (Trainable)â”‚                         â”‚ {trainable_memory_mb:>8.2f} MB â”‚")
    print(f"â”‚ Gradient + Optimizer States  â”‚                         â”‚ {gradient_memory_mb:>8.2f} MB â”‚")
    print(f"â”œ{'â”€'*78}â”¤")
    print(f"â”‚ LoRA Toplam EÄŸitim BelleÄŸi   â”‚                         â”‚ {total_training_memory_mb:>8.2f} MB â”‚")
    print(f"â””{'â”€'*78}â”˜")
    
    print(f"\nâ”Œ{'â”€'*78}â”")
    print(f"â”‚ {'TRADITIONAL FINE-TUNING KARÅILAÅTIRMASI':^76} â”‚")
    print(f"â”œ{'â”€'*78}â”¤")
    print(f"â”‚ Traditional Model BelleÄŸi    â”‚                         â”‚ {total_memory_mb:>8.2f} MB â”‚")
    print(f"â”‚ Traditional Gradient BelleÄŸi â”‚                         â”‚ {traditional_gradient_memory_mb:>8.2f} MB â”‚")
    print(f"â”‚ Traditional Toplam           â”‚                         â”‚ {traditional_total_memory_mb:>8.2f} MB â”‚")
    print(f"â”œ{'â”€'*78}â”¤")
    print(f"â”‚ LoRA ile Bellek Tasarrufu    â”‚                         â”‚ {memory_saving_percentage:>7.2f} %  â”‚")
    print(f"â””{'â”€'*78}â”˜")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # BÃ¶lÃ¼m 4: Disk KullanÄ±mÄ± Projeksiyonu
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print(f"\n{'='*80}")
    print(f"ğŸ’¿ 4. DÄ°SK KULLANIMI PROJEKSÄ°YONU")
    print(f"{'='*80}\n")
    
    # Adapter boyutu (sadece trainable parametreler)
    adapter_size_mb = trainable_memory_mb
    
    # FarklÄ± senaryo Ã¶rnekleri
    scenarios = [
        ("1 gÃ¶rev", 1),
        ("10 gÃ¶rev", 10),
        ("50 gÃ¶rev", 50),
        ("100 gÃ¶rev", 100),
    ]
    
    print(f"â”Œ{'â”€'*78}â”")
    print(f"â”‚ {'Senaryo':^20} â”‚ {'Traditional':^15} â”‚ {'LoRA':^15} â”‚ {'Tasarruf':^15} â”‚")
    print(f"â”œ{'â”€'*78}â”¤")
    
    for scenario_name, num_tasks in scenarios:
        traditional_total = total_memory_mb * num_tasks
        lora_total = total_memory_mb + (adapter_size_mb * num_tasks)
        saving = traditional_total - lora_total
        saving_pct = 100 * saving / traditional_total
        
        print(f"â”‚ {scenario_name:^20} â”‚ {traditional_total:>12.1f} MB â”‚ {lora_total:>12.1f} MB â”‚ {saving_pct:>12.1f} % â”‚")
    
    print(f"â””{'â”€'*78}â”˜")
    
    print(f"\nğŸ’¡ AÃ§Ä±klama:")
    print(f"   â€¢ Traditional: Her gÃ¶rev iÃ§in tam model kopyasÄ±")
    print(f"   â€¢ LoRA: 1 base model + her gÃ¶rev iÃ§in adapter")
    print(f"   â€¢ Adapter boyutu: ~{adapter_size_mb:.2f} MB")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # BÃ¶lÃ¼m 5: EÄŸitim HÄ±zÄ± Tahmini
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print(f"\n{'='*80}")
    print(f"âš¡ 5. EÄÄ°TÄ°M HIZI TAHMÄ°NÄ°")
    print(f"{'='*80}\n")
    
    # Forward pass: Her iki yÃ¶ntemde de aynÄ±
    # Backward pass: Sadece trainable parametrelere gÃ¶re
    backward_speedup = total_params / trainable_params
    
    print(f"â”Œ{'â”€'*78}â”")
    print(f"â”‚ {'Metrik':^35} â”‚ {'Traditional':^15} â”‚ {'LoRA':^15} â”‚")
    print(f"â”œ{'â”€'*78}â”¤")
    print(f"â”‚ {'Forward Pass (gÃ¶receli)':^35} â”‚ {'1.0x':>15} â”‚ {'1.0x':>15} â”‚")
    print(f"â”‚ {'Backward Pass (gÃ¶receli)':^35} â”‚ {'1.0x':>15} â”‚ {f'{1/backward_speedup:.3f}x':>15} â”‚")
    print(f"â”‚ {'Optimizer Step (gÃ¶receli)':^35} â”‚ {'1.0x':>15} â”‚ {f'{1/backward_speedup:.3f}x':>15} â”‚")
    print(f"â”œ{'â”€'*78}â”¤")
    print(f"â”‚ {'Tahmini HÄ±zlanma':^35} â”‚ {'1.0x':>15} â”‚ {f'{backward_speedup/3:.2f}x':>15} â”‚")
    print(f"â””{'â”€'*78}â”˜")
    
    print(f"\nğŸ’¡ Not:")
    print(f"   â€¢ GerÃ§ek hÄ±zlanma, model boyutu ve donanÄ±ma gÃ¶re deÄŸiÅŸir")
    print(f"   â€¢ Genellikle 2-3x hÄ±zlanma gÃ¶zlemlenir")
    print(f"   â€¢ Daha bÃ¼yÃ¼k modellerde fark daha belirgin olur")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Ã–zet ve Ã–neriler
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print(f"\n{'='*80}")
    print(f"âœ¨ Ã–ZET VE Ã–NERÄ°LER")
    print(f"{'='*80}\n")
    
    print(f"ğŸ¯ LoRA kullanmanÄ±z Ã¶nerilen durumlar:")
    print(f"   âœ“ SÄ±nÄ±rlÄ± GPU belleÄŸiniz varsa")
    print(f"   âœ“ HÄ±zlÄ± iterasyon yapmanÄ±z gerekiyorsa")
    print(f"   âœ“ Ã‡oklu gÃ¶rev/domain iÃ§in model Ã¶zelleÅŸtiriyorsanÄ±z")
    print(f"   âœ“ Production'da Ã§oklu kullanÄ±cÄ±ya Ã¶zel modeller sunacaksanÄ±z")
    print(f"   âœ“ Maliyeti dÃ¼ÅŸÃ¼k tutmak istiyorsanÄ±z")
    
    print(f"\nâš ï¸  Traditional fine-tuning tercih edilebilecek durumlar:")
    print(f"   â€¢ Ã‡ok spesifik/dar bir domain'de maksimum performans gerekiyorsa")
    print(f"   â€¢ Modelin tÃ¼m katmanlarÄ±nÄ± ciddi ÅŸekilde deÄŸiÅŸtirmek gerekiyorsa")
    print(f"   â€¢ Bellek ve hesaplama kaynaÄŸÄ± bol ise")
    
    print(f"\n{'='*80}")
    print(f"ğŸ“ LoRA Hiperparametre Ã–nerileri:")
    print(f"{'='*80}\n")
    
    print(f"   r (rank) seÃ§imi:")
    print(f"   â€¢ KÃ¼Ã§Ã¼k gÃ¶revler: r=4 veya r=8")
    print(f"   â€¢ Orta gÃ¶revler: r=16 veya r=32")
    print(f"   â€¢ BÃ¼yÃ¼k/kompleks gÃ¶revler: r=64 veya r=128")
    print(f"   â€¢ Åu anki deÄŸer: r={LORA_RANK}")
    
    print(f"\n   Î± (alpha) seÃ§imi:")
    print(f"   â€¢ Genellikle Î± = r veya Î± = 2Ã—r")
    print(f"   â€¢ Scaling = Î±/r â†’ Tipik deÄŸer: 1 veya 2")
    print(f"   â€¢ Åu anki deÄŸer: Î±={LORA_ALPHA} (scaling={LORA_ALPHA/LORA_RANK})")
    
    print(f"\n   Hedef modÃ¼ller:")
    print(f"   â€¢ Attention: Query, Key, Value, Output projection")
    print(f"   â€¢ Feed-forward: Intermediate ve output layers")
    print(f"   â€¢ Åu anki hedefler: {', '.join(LORA_TARGET_MODULES)}")
    
    print(f"\n{'='*80}\n")


# ============================================================================
# BÃ–LÃœM 7: YARDIMCI FONKSÄ°YONLAR
# ============================================================================

def print_welcome_message():
    """
    Program baÅŸlangÄ±cÄ±nda hoÅŸ geldiniz mesajÄ± ve bilgilendirme yapar.
    """
    print(f"\n")
    print(f"{'â•”' + 'â•'*78 + 'â•—'}")
    print(f"â•‘{' '*78}â•‘")
    print(f"â•‘{'PEFT (Parameter Efficient Fine-Tuning) ve LoRA':^78}â•‘")
    print(f"â•‘{'DetaylÄ± EÄŸitim ve Demonstrasyon Scripti':^78}â•‘")
    print(f"â•‘{' '*78}â•‘")
    print(f"{'â•š' + 'â•'*78 + 'â•'}")
    
    print(f"\nğŸ“– Bu script ÅŸunlarÄ± iÃ§erir:")
    print(f"   1. LoRA model kurulumu ve yapÄ±landÄ±rmasÄ±")
    print(f"   2. Veri hazÄ±rlama ve tokenization")
    print(f"   3. Model eÄŸitimi (training pipeline)")
    print(f"   4. DetaylÄ± avantaj analizi ve karÅŸÄ±laÅŸtÄ±rmalar")
    
    print(f"\nâš™ï¸  YapÄ±landÄ±rma:")
    print(f"   â€¢ Model: {DEFAULT_MODEL_NAME}")
    print(f"   â€¢ LoRA Rank: {LORA_RANK}")
    print(f"   â€¢ LoRA Alpha: {LORA_ALPHA}")
    print(f"   â€¢ Epoch: {NUM_TRAIN_EPOCHS}")
    print(f"   â€¢ Batch Size: {BATCH_SIZE}")
    
    print(f"\n{'='*80}\n")


# ============================================================================
# BÃ–LÃœM 8: ANA Ã‡ALIÅTIRMA BLOÄU
# ============================================================================

if __name__ == "__main__":
    """
    Ana Ã§alÄ±ÅŸtÄ±rma bloÄŸu.
    
    Bu bÃ¶lÃ¼m, scriptin doÄŸrudan Ã§alÄ±ÅŸtÄ±rÄ±lmasÄ± durumunda (import edilmediÄŸinde)
    Ã§alÄ±ÅŸÄ±r ve tÃ¼m demonstrasyonu orchestrate eder.
    
    KullanÄ±m:
    --------
    1. Sadece demonstrasyon (varsayÄ±lan):
       python 1_peft_lora.py
    
    2. Tam eÄŸitim iÃ§in, aÅŸaÄŸÄ±daki satÄ±rÄ±n yorumunu kaldÄ±rÄ±n:
       model, tokenizer = train_lora_model()
    """
    
    # HoÅŸ geldiniz mesajÄ±
    print_welcome_message()
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Demonstrasyon Modu (VarsayÄ±lan)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # LoRA'nÄ±n avantajlarÄ±nÄ± ve parametre analizini gÃ¶ster
    demonstrate_lora_benefits()
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # EÄŸitim Modu (Manuel Aktivasyon Gerekli)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # EÄŸitim yapmak iÃ§in aÅŸaÄŸÄ±daki satÄ±rlarÄ±n yorumunu kaldÄ±rÄ±n:
    
    print(f"\n{'='*80}")
    print(f"âš ï¸  DÄ°KKAT: EÄŸitim modu aktif!")
    print(f"{'='*80}\n")
    
    model, tokenizer = train_lora_model()
    
    # EÄŸitim sonrasÄ± basit bir inference Ã¶rneÄŸi
    print(f"\n{'='*80}")
    print(f"ğŸ¤– BASÄ°T INFERENCE Ã–RNEÄÄ°")
    print(f"{'='*80}\n")
    
    prompt = "KullanÄ±cÄ±: Merhaba! Asistan:"
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(**inputs, max_length=50, num_return_sequences=1)
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    print(f"Prompt: {prompt}")
    print(f"Ã‡Ä±ktÄ±: {generated_text}")
    print(f"\n{'='*80}\n")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Bilgilendirme MesajÄ±
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print(f"\nğŸ’¡ Ä°PUCU:")
    print(f"   â€¢ EÄŸitim modu Ã§alÄ±ÅŸtÄ±rÄ±ldÄ±!")
    print(f"   â€¢ Model baÅŸarÄ±yla eÄŸitildi ve kaydedildi")
    print(f"   â€¢ Kaydedilen konum: {FINAL_MODEL_DIR}")
    
    print(f"\nğŸ“š Ã–ÄŸrenim KaynaklarÄ±:")
    print(f"   â€¢ LoRA Paper: https://arxiv.org/abs/2106.09685")
    print(f"   â€¢ Hugging Face PEFT: https://huggingface.co/docs/peft")
    print(f"   â€¢ Transformers Docs: https://huggingface.co/docs/transformers")
    
    print(f"\n{'='*80}")
    print(f"âœ… PROGRAM BAÅARIYLA TAMAMLANDI")
    print(f"{'='*80}\n")