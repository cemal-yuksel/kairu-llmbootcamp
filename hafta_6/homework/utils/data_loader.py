"""
================================================================================
DATA LOADER - Intelligent Review Summarizer
================================================================================

IMDB ve diÄŸer veri setlerini yÃ¼klemek ve iÅŸlemek iÃ§in yardÄ±mcÄ± fonksiyonlar.

Yazar: Kairu AI - Build with LLMs Bootcamp
Tarih: 2 KasÄ±m 2025
================================================================================
"""

import json
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union
from datasets import load_dataset, Dataset, DatasetDict
from loguru import logger
import pandas as pd
from tqdm import tqdm


class DataLoader:
    """
    Veri seti yÃ¼kleme ve yÃ¶netim sÄ±nÄ±fÄ±
    
    IMDB ve custom dataset'leri yÃ¼kler, filtreleyip iÅŸler.
    """
    
    def __init__(self, config):
        """
        Args:
            config: Configuration objesi
        """
        self.config = config
        self.dataset: Optional[DatasetDict] = None
        
    def load_imdb(
        self, 
        max_train_samples: Optional[int] = None,
        max_test_samples: Optional[int] = None,
        cache_dir: Optional[Path] = None
    ) -> DatasetDict:
        """
        IMDB dataset'ini yÃ¼kle
        
        IMDB Dataset:
        - 50,000 film yorumu (25k train, 25k test)
        - Binary sentiment (pos/neg)
        - Her split dengeli (12.5k pos, 12.5k neg)
        
        Args:
            max_train_samples: Maksimum train Ã¶rnekleri (None = hepsi)
            max_test_samples: Maksimum test Ã¶rnekleri
            cache_dir: Cache directory
            
        Returns:
            DatasetDict: {"train": Dataset, "test": Dataset}
        """
        logger.info(f"ğŸ“¥ IMDB dataset yÃ¼kleniyor...")
        
        try:
            # Dataset'i Hugging Face'den yÃ¼kle
            dataset = load_dataset(
                self.config.DATASET_NAME,
                cache_dir=str(cache_dir) if cache_dir else None,
                trust_remote_code=True
            )
            
            # Sampling uygula
            if max_train_samples:
                logger.info(f"  ğŸ”¹ Train set {max_train_samples} ile sÄ±nÄ±rlanÄ±yor...")
                dataset["train"] = dataset["train"].select(range(max_train_samples))
                
            if max_test_samples:
                logger.info(f"  ğŸ”¹ Test set {max_test_samples} ile sÄ±nÄ±rlanÄ±yor...")
                dataset["test"] = dataset["test"].select(range(max_test_samples))
            
            # Ä°statistikler
            logger.info(f"âœ… IMDB yÃ¼klendi:")
            logger.info(f"  ğŸ“Š Train samples: {len(dataset['train']):,}")
            logger.info(f"  ğŸ“Š Test samples: {len(dataset['test']):,}")
            
            self.dataset = dataset
            return dataset
            
        except Exception as e:
            logger.error(f"âŒ IMDB yÃ¼kleme hatasÄ±: {e}")
            raise
    
    def filter_reviews(
        self,
        dataset: Dataset,
        min_length: int = 50,
        max_length: int = 2000,
        sentiment: Optional[int] = None
    ) -> Dataset:
        """
        Review'larÄ± filtrele
        
        Args:
            dataset: Filtrelenecek dataset
            min_length: Minimum karakter sayÄ±sÄ±
            max_length: Maximum karakter sayÄ±sÄ±
            sentiment: Sadece belirli sentiment (0=neg, 1=pos)
            
        Returns:
            FiltrelenmiÅŸ Dataset
        """
        logger.info(f"ğŸ” Review'lar filtreleniyor...")
        original_len = len(dataset)
        
        def filter_function(example):
            text = example["text"]
            # Uzunluk kontrolÃ¼
            if len(text) < min_length or len(text) > max_length:
                return False
            # Sentiment kontrolÃ¼
            if sentiment is not None and example["label"] != sentiment:
                return False
            return True
        
        filtered = dataset.filter(filter_function)
        filtered_len = len(filtered)
        
        logger.info(f"  âœ… {original_len:,} â†’ {filtered_len:,} review kaldÄ±")
        logger.info(f"  ğŸ“‰ {original_len - filtered_len:,} review filtrelendi")
        
        return filtered
    
    def balance_dataset(
        self,
        dataset: Dataset,
        label_column: str = "label",
        samples_per_class: Optional[int] = None
    ) -> Dataset:
        """
        Dataset'i dengele (her sÄ±nÄ±ftan eÅŸit sayÄ±da Ã¶rnek)
        
        Args:
            dataset: Dengelenecek dataset
            label_column: Label sÃ¼tunu adÄ±
            samples_per_class: Her sÄ±nÄ±ftan kaÃ§ Ã¶rnek (None = minimum)
            
        Returns:
            DengelenmiÅŸ Dataset
        """
        logger.info("âš–ï¸  Dataset dengeleniyor...")
        
        # Her sÄ±nÄ±fÄ±n Ã¶rneklerini ayÄ±r
        labels = dataset[label_column]
        unique_labels = set(labels)
        
        class_datasets = {}
        for label in unique_labels:
            class_datasets[label] = dataset.filter(
                lambda x: x[label_column] == label
            )
            logger.info(f"  ğŸ“Š Class {label}: {len(class_datasets[label]):,} samples")
        
        # KaÃ§ Ã¶rnek alÄ±nacaÄŸÄ±nÄ± belirle
        if samples_per_class is None:
            samples_per_class = min(len(ds) for ds in class_datasets.values())
        
        logger.info(f"  ğŸ¯ Hedef: {samples_per_class:,} sample per class")
        
        # Her sÄ±nÄ±ftan eÅŸit sayÄ±da al
        balanced_datasets = []
        for label, ds in class_datasets.items():
            if len(ds) > samples_per_class:
                ds = ds.shuffle(seed=42).select(range(samples_per_class))
            balanced_datasets.append(ds)
        
        # BirleÅŸtir
        from datasets import concatenate_datasets
        balanced = concatenate_datasets(balanced_datasets)
        
        logger.info(f"  âœ… DengelenmiÅŸ dataset: {len(balanced):,} samples")
        
        return balanced.shuffle(seed=42)
    
    def get_dataset_stats(self, dataset: Dataset) -> Dict:
        """
        Dataset istatistikleri
        
        Args:
            dataset: Analiz edilecek dataset
            
        Returns:
            Ä°statistik dictionary'si
        """
        stats = {
            "total_samples": len(dataset),
            "columns": dataset.column_names,
        }
        
        # Text uzunluklarÄ±
        if "text" in dataset.column_names:
            texts = dataset["text"]
            lengths = [len(t) for t in texts]
            stats["text_stats"] = {
                "min_length": min(lengths),
                "max_length": max(lengths),
                "avg_length": sum(lengths) / len(lengths),
                "total_chars": sum(lengths)
            }
        
        # Label daÄŸÄ±lÄ±mÄ±
        if "label" in dataset.column_names:
            labels = dataset["label"]
            from collections import Counter
            label_counts = Counter(labels)
            stats["label_distribution"] = dict(label_counts)
        
        return stats
    
    def save_to_json(
        self,
        dataset: Dataset,
        output_path: Union[str, Path],
        include_labels: bool = True
    ):
        """
        Dataset'i JSON formatÄ±nda kaydet
        
        Args:
            dataset: Kaydedilecek dataset
            output_path: Ã‡Ä±ktÄ± dosya yolu
            include_labels: Label'larÄ± dahil et
        """
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"ğŸ’¾ Dataset kaydediliyor: {output_path}")
        
        data = []
        for item in tqdm(dataset, desc="Converting to JSON"):
            entry = {"text": item["text"]}
            if include_labels and "label" in item:
                entry["label"] = item["label"]
            data.append(entry)
        
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"  âœ… {len(data):,} samples kaydedildi")
    
    def load_from_json(
        self,
        input_path: Union[str, Path]
    ) -> Dataset:
        """
        JSON'dan dataset yÃ¼kle
        
        Args:
            input_path: JSON dosya yolu
            
        Returns:
            Dataset objesi
        """
        input_path = Path(input_path)
        logger.info(f"ğŸ“¥ JSON yÃ¼kleniyor: {input_path}")
        
        with open(input_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        dataset = Dataset.from_list(data)
        logger.info(f"  âœ… {len(dataset):,} samples yÃ¼klendi")
        
        return dataset
    
    def create_review_chunks(
        self,
        dataset: Dataset,
        chunk_size: int = 512,
        overlap: int = 50,
        text_column: str = "text"
    ) -> List[Dict]:
        """
        Uzun review'larÄ± chunk'lara bÃ¶l
        
        RAG iÃ§in review'larÄ± daha kÃ¼Ã§Ã¼k parÃ§alara bÃ¶ler.
        
        Args:
            dataset: BÃ¶lÃ¼necek dataset
            chunk_size: Her chunk'Ä±n max token sayÄ±sÄ±
            overlap: Chunk'lar arasÄ± overlap
            text_column: Text sÃ¼tunu adÄ±
            
        Returns:
            Chunk'lar listesi [{"text": ..., "metadata": ...}]
        """
        logger.info(f"âœ‚ï¸  Review'lar chunk'lara bÃ¶lÃ¼nÃ¼yor...")
        logger.info(f"  ğŸ“ Chunk size: {chunk_size}, Overlap: {overlap}")
        
        chunks = []
        
        for idx, item in enumerate(tqdm(dataset, desc="Chunking")):
            text = item[text_column]
            
            # Basit word-based chunking
            words = text.split()
            
            for i in range(0, len(words), chunk_size - overlap):
                chunk_words = words[i:i + chunk_size]
                chunk_text = " ".join(chunk_words)
                
                chunks.append({
                    "text": chunk_text,
                    "metadata": {
                        "review_id": idx,
                        "chunk_id": i // (chunk_size - overlap),
                        "label": item.get("label", None),
                        "word_count": len(chunk_words)
                    }
                })
        
        logger.info(f"  âœ… {len(chunks):,} chunk oluÅŸturuldu")
        logger.info(f"  ğŸ“Š Ortalama: {len(chunks) / len(dataset):.1f} chunk/review")
        
        return chunks


# ============================================================================
# QUICK TEST
# ============================================================================
if __name__ == "__main__":
    from config import config
    
    logger.info("ğŸ§ª DataLoader test baÅŸlÄ±yor...")
    
    # DataLoader oluÅŸtur
    loader = DataLoader(config)
    
    # IMDB yÃ¼kle (kÃ¼Ã§Ã¼k sample)
    dataset = loader.load_imdb(max_train_samples=100, max_test_samples=50)
    
    # Ä°statistikler
    stats = loader.get_dataset_stats(dataset["train"])
    logger.info(f"\nğŸ“Š Dataset Stats:\n{json.dumps(stats, indent=2)}")
    
    # Filtreleme testi
    filtered = loader.filter_reviews(
        dataset["train"],
        min_length=100,
        max_length=1000
    )
    
    # Chunk testi
    chunks = loader.create_review_chunks(
        filtered,
        chunk_size=100,
        overlap=20
    )
    
    logger.info("\nâœ… TÃ¼m testler baÅŸarÄ±lÄ±!")
