"""
================================================================================
METRICS - Intelligent Review Summarizer
================================================================================

Model deÄŸerlendirme metrikleri: ROUGE, BLEU, BERTScore ve custom metrics.

Yazar: Kairu AI - Build with LLMs Bootcamp
Tarih: 2 KasÄ±m 2025
================================================================================
"""

from typing import List, Dict, Optional, Tuple, Union
import numpy as np
from loguru import logger
from collections import defaultdict
import json


class MetricsCalculator:
    """
    Model performans metriklerini hesaplayan sÄ±nÄ±f
    
    Summarization ve Q&A iÃ§in Ã§eÅŸitli metrikler saÄŸlar.
    """
    
    def __init__(self, config):
        """
        Args:
            config: Configuration objesi
        """
        self.config = config
        self._initialize_scorers()
    
    def _initialize_scorers(self):
        """Metric scorer'larÄ±nÄ± baÅŸlat"""
        try:
            from rouge_score import rouge_scorer
            self.rouge_scorer = rouge_scorer.RougeScorer(
                self.config.ROUGE_TYPES,
                use_stemmer=self.config.USE_STEMMER
            )
            logger.info("âœ… ROUGE scorer hazÄ±r")
        except ImportError:
            logger.warning("âš ï¸  rouge-score yÃ¼klenemedi, ROUGE metrikleri kullanÄ±lamayacak")
            self.rouge_scorer = None
        
        # BERTScore (optional, yavaÅŸ)
        self.bertscore_scorer = None
        if self.config.CALCULATE_BERTSCORE:
            try:
                import bert_score
                self.bertscore_scorer = bert_score
                logger.info("âœ… BERTScore scorer hazÄ±r")
            except ImportError:
                logger.warning("âš ï¸  bert-score yÃ¼klenemedi")
    
    # ========================================================================
    # ROUGE METRICS (Summarization)
    # ========================================================================
    
    def calculate_rouge(
        self,
        predictions: List[str],
        references: List[str]
    ) -> Dict[str, float]:
        """
        ROUGE skorlarÄ±nÄ± hesapla
        
        ROUGE (Recall-Oriented Understudy for Gisting Evaluation):
        - Ã–zetleme kalitesi iÃ§in standart metrik
        - N-gram overlap'e bakar
        
        ROUGE Tipleri:
        - ROUGE-1: Unigram overlap
        - ROUGE-2: Bigram overlap
        - ROUGE-L: Longest Common Subsequence
        - ROUGE-Lsum: Multi-sentence summary iÃ§in ROUGE-L
        
        Args:
            predictions: Model'in Ã¼rettiÄŸi Ã¶zetler
            references: GerÃ§ek (gold) Ã¶zetler
            
        Returns:
            ROUGE skorlarÄ± dictionary
        """
        if not self.rouge_scorer:
            logger.error("âŒ ROUGE scorer yok!")
            return {}
        
        if len(predictions) != len(references):
            raise ValueError(f"Prediction ve reference sayÄ±sÄ± eÅŸleÅŸmiyor: {len(predictions)} vs {len(references)}")
        
        logger.info(f"ðŸ“Š ROUGE hesaplanÄ±yor... ({len(predictions)} sample)")
        
        # Her sample iÃ§in ROUGE hesapla
        all_scores = defaultdict(list)
        
        for pred, ref in zip(predictions, references):
            scores = self.rouge_scorer.score(ref, pred)
            
            for rouge_type, score in scores.items():
                all_scores[f"{rouge_type}_precision"].append(score.precision)
                all_scores[f"{rouge_type}_recall"].append(score.recall)
                all_scores[f"{rouge_type}_fmeasure"].append(score.fmeasure)
        
        # Ortalama al
        avg_scores = {}
        for key, values in all_scores.items():
            avg_scores[key] = np.mean(values)
        
        # Sadece F1 skorlarÄ±nÄ± logla
        logger.info("âœ… ROUGE Scores (F1):")
        for rouge_type in self.config.ROUGE_TYPES:
            f1_key = f"{rouge_type}_fmeasure"
            if f1_key in avg_scores:
                logger.info(f"  â€¢ {rouge_type}: {avg_scores[f1_key]:.4f}")
        
        return avg_scores
    
    # ========================================================================
    # BLEU SCORE (Translation/Generation)
    # ========================================================================
    
    def calculate_bleu(
        self,
        predictions: List[str],
        references: List[List[str]]  # Her prediction iÃ§in multiple reference olabilir
    ) -> Dict[str, float]:
        """
        BLEU skorunu hesapla
        
        BLEU (Bilingual Evaluation Understudy):
        - Makine Ã§evirisi iÃ§in geliÅŸtirilmiÅŸ
        - Generation quality iÃ§in de kullanÄ±lÄ±r
        - N-gram precision'a odaklanÄ±r
        
        Args:
            predictions: Model Ã§Ä±ktÄ±larÄ±
            references: Reference'lar (her biri liste)
            
        Returns:
            BLEU skorlarÄ±
        """
        try:
            from sacrebleu import corpus_bleu
            
            # Her prediction iÃ§in tek reference varsa, nested list yap
            if references and not isinstance(references[0], list):
                references = [[ref] for ref in references]
            
            # sacrebleu references'Ä± transpose ister
            references_transposed = list(zip(*references))
            
            bleu = corpus_bleu(predictions, references_transposed)
            
            return {
                "bleu": bleu.score,
                "bleu_precisions": bleu.precisions,
                "bleu_bp": bleu.bp,
                "bleu_ratio": bleu.sys_len / bleu.ref_len
            }
        
        except ImportError:
            logger.warning("âš ï¸  sacrebleu yÃ¼klenemedi")
            return {}
    
    # ========================================================================
    # BERTSCORE (Semantic Similarity)
    # ========================================================================
    
    def calculate_bertscore(
        self,
        predictions: List[str],
        references: List[str],
        lang: str = "en"
    ) -> Dict[str, float]:
        """
        BERTScore hesapla
        
        BERTScore:
        - BERT embeddings kullanarak semantic similarity Ã¶lÃ§er
        - N-gram'lardan daha sofistike
        - Daha yavaÅŸ ama daha kaliteli
        
        Args:
            predictions: Model Ã§Ä±ktÄ±larÄ±
            references: Gold references
            lang: Dil kodu
            
        Returns:
            BERTScore metrikleri
        """
        if not self.bertscore_scorer:
            logger.warning("âš ï¸  BERTScore hesaplanamÄ±yor")
            return {}
        
        logger.info("ðŸ“Š BERTScore hesaplanÄ±yor... (bu biraz sÃ¼rebilir)")
        
        P, R, F1 = self.bertscore_scorer.score(
            predictions,
            references,
            lang=lang,
            verbose=False
        )
        
        return {
            "bertscore_precision": P.mean().item(),
            "bertscore_recall": R.mean().item(),
            "bertscore_f1": F1.mean().item()
        }
    
    # ========================================================================
    # Q&A METRICS
    # ========================================================================
    
    def calculate_exact_match(
        self,
        predictions: List[str],
        references: List[str],
        normalize: bool = True
    ) -> float:
        """
        Exact Match (EM) hesapla
        
        Prediction ve reference'Ä±n tam olarak eÅŸleÅŸip eÅŸleÅŸmediÄŸini kontrol eder.
        
        Args:
            predictions: Model cevaplarÄ±
            references: DoÄŸru cevaplar
            normalize: Lowercase ve whitespace normalizasyonu
            
        Returns:
            EM skoru (0-1 arasÄ±)
        """
        def normalize_answer(s):
            """Answer normalizasyonu"""
            import re
            import string
            
            # Lowercase
            s = s.lower()
            
            # Punctuation kaldÄ±r
            s = s.translate(str.maketrans('', '', string.punctuation))
            
            # Fazla whitespace
            s = re.sub(r'\s+', ' ', s).strip()
            
            return s
        
        matches = 0
        for pred, ref in zip(predictions, references):
            if normalize:
                pred = normalize_answer(pred)
                ref = normalize_answer(ref)
            
            if pred == ref:
                matches += 1
        
        em = matches / len(predictions) if predictions else 0
        logger.info(f"  ðŸ“Š Exact Match: {em:.4f} ({matches}/{len(predictions)})")
        
        return em
    
    def calculate_f1_score(
        self,
        predictions: List[str],
        references: List[str]
    ) -> float:
        """
        Token-level F1 score hesapla (Q&A iÃ§in)
        
        Args:
            predictions: Model cevaplarÄ±
            references: DoÄŸru cevaplar
            
        Returns:
            Ortalama F1 skoru
        """
        def compute_f1(pred_tokens, ref_tokens):
            """Tek bir Ã¶rnek iÃ§in F1"""
            common = set(pred_tokens) & set(ref_tokens)
            
            if len(common) == 0:
                return 0.0
            
            precision = len(common) / len(pred_tokens) if pred_tokens else 0
            recall = len(common) / len(ref_tokens) if ref_tokens else 0
            
            if precision + recall == 0:
                return 0.0
            
            f1 = 2 * (precision * recall) / (precision + recall)
            return f1
        
        f1_scores = []
        for pred, ref in zip(predictions, references):
            pred_tokens = pred.lower().split()
            ref_tokens = ref.lower().split()
            f1 = compute_f1(pred_tokens, ref_tokens)
            f1_scores.append(f1)
        
        avg_f1 = np.mean(f1_scores)
        logger.info(f"  ðŸ“Š F1 Score: {avg_f1:.4f}")
        
        return avg_f1
    
    # ========================================================================
    # CUSTOM METRICS
    # ========================================================================
    
    def calculate_length_stats(
        self,
        predictions: List[str],
        references: Optional[List[str]] = None
    ) -> Dict:
        """
        Uzunluk istatistikleri
        
        Args:
            predictions: Model Ã§Ä±ktÄ±larÄ±
            references: Reference'lar (optional)
            
        Returns:
            Uzunluk metrikleri
        """
        pred_lengths = [len(p.split()) for p in predictions]
        
        stats = {
            "pred_avg_length": np.mean(pred_lengths),
            "pred_min_length": np.min(pred_lengths),
            "pred_max_length": np.max(pred_lengths),
            "pred_std_length": np.std(pred_lengths)
        }
        
        if references:
            ref_lengths = [len(r.split()) for r in references]
            stats.update({
                "ref_avg_length": np.mean(ref_lengths),
                "length_ratio": np.mean(pred_lengths) / np.mean(ref_lengths)
            })
        
        return stats
    
    def calculate_diversity_metrics(
        self,
        predictions: List[str]
    ) -> Dict:
        """
        Ãœretilen text'lerin Ã§eÅŸitliliÄŸini Ã¶lÃ§
        
        Args:
            predictions: Model Ã§Ä±ktÄ±larÄ±
            
        Returns:
            Diversity metrikleri
        """
        all_tokens = []
        for pred in predictions:
            all_tokens.extend(pred.lower().split())
        
        unique_tokens = set(all_tokens)
        
        # Distinct-1, Distinct-2 (unique unigram/bigram oranÄ±)
        bigrams = []
        for pred in predictions:
            tokens = pred.lower().split()
            bigrams.extend([f"{tokens[i]} {tokens[i+1]}" for i in range(len(tokens)-1)])
        
        unique_bigrams = set(bigrams)
        
        return {
            "distinct_1": len(unique_tokens) / len(all_tokens) if all_tokens else 0,
            "distinct_2": len(unique_bigrams) / len(bigrams) if bigrams else 0,
            "vocab_size": len(unique_tokens)
        }
    
    # ========================================================================
    # COMPREHENSIVE EVALUATION
    # ========================================================================
    
    def evaluate_summarization(
        self,
        predictions: List[str],
        references: List[str],
        include_bertscore: bool = False
    ) -> Dict:
        """
        KapsamlÄ± summarization deÄŸerlendirmesi
        
        Args:
            predictions: Model Ã¶zetleri
            references: Gold Ã¶zetler
            include_bertscore: BERTScore ekle (yavaÅŸ)
            
        Returns:
            TÃ¼m metrikler
        """
        logger.info("=" * 80)
        logger.info("ðŸ“Š SUMMARIZATION EVALUATION")
        logger.info("=" * 80)
        
        results = {}
        
        # ROUGE
        if self.rouge_scorer:
            rouge_scores = self.calculate_rouge(predictions, references)
            results.update(rouge_scores)
        
        # Length stats
        length_stats = self.calculate_length_stats(predictions, references)
        results.update(length_stats)
        
        # Diversity
        diversity = self.calculate_diversity_metrics(predictions)
        results.update(diversity)
        
        # BERTScore (optional)
        if include_bertscore and self.bertscore_scorer:
            bertscore = self.calculate_bertscore(predictions, references)
            results.update(bertscore)
        
        logger.info("=" * 80)
        
        return results
    
    def evaluate_qa(
        self,
        predictions: List[str],
        references: List[str]
    ) -> Dict:
        """
        Q&A deÄŸerlendirmesi
        
        Args:
            predictions: Model cevaplarÄ±
            references: DoÄŸru cevaplar
            
        Returns:
            Q&A metrikleri
        """
        logger.info("=" * 80)
        logger.info("ðŸ“Š Q&A EVALUATION")
        logger.info("=" * 80)
        
        results = {}
        
        # Exact Match
        em = self.calculate_exact_match(predictions, references)
        results["exact_match"] = em
        
        # F1 Score
        f1 = self.calculate_f1_score(predictions, references)
        results["f1_score"] = f1
        
        logger.info("=" * 80)
        
        return results
    
    def save_results(
        self,
        results: Dict,
        output_path: str
    ):
        """
        SonuÃ§larÄ± kaydet
        
        Args:
            results: Metrik sonuÃ§larÄ±
            output_path: JSON Ã§Ä±ktÄ± yolu
        """
        import json
        from pathlib import Path
        
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ðŸ’¾ SonuÃ§lar kaydedildi: {output_path}")


# ============================================================================
# QUICK TEST
# ============================================================================
if __name__ == "__main__":
    from config import config
    
    logger.info("ðŸ§ª MetricsCalculator test baÅŸlÄ±yor...")
    
    calculator = MetricsCalculator(config)
    
    # Test data
    predictions = [
        "This is a great movie with excellent acting.",
        "The film was boring and too long."
    ]
    
    references = [
        "This movie is excellent with great performances.",
        "The movie was dull and overly lengthy."
    ]
    
    # ROUGE test
    if calculator.rouge_scorer:
        rouge = calculator.calculate_rouge(predictions, references)
        logger.info(f"\nðŸ“Š ROUGE: {rouge}")
    
    # Q&A test
    qa_preds = ["Paris", "1998"]
    qa_refs = ["Paris", "1998"]
    
    em = calculator.calculate_exact_match(qa_preds, qa_refs)
    f1 = calculator.calculate_f1_score(qa_preds, qa_refs)
    
    logger.info("\nâœ… TÃ¼m testler baÅŸarÄ±lÄ±!")
