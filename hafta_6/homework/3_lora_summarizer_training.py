"""
================================================================================
3. LORA SUMMARIZER TRAINING - Intelligent Review Summarizer
================================================================================

LoRA ile BART/T5 modelini summarization gÃ¶revi iÃ§in fine-tune eder.

Bu script:
1. Summarization data yÃ¼kler
2. BART/T5 base model yÃ¼kler
3. LoRA configuration uygular
4. Model'i eÄŸitir
5. Fine-tuned modeli kaydeder
6. Evaluation metrikleri hesaplar

KULLANIM:
---------
python 3_lora_summarizer_training.py

veya

python 3_lora_summarizer_training.py --epochs 5 --batch-size 8

Yazar: Kairu AI - Build with LLMs Bootcamp
Tarih: 2 KasÄ±m 2025
================================================================================
"""

import sys
import os
from pathlib import Path
import argparse
import json
from loguru import logger
import torch
from tqdm import tqdm

# Wandb'yi devre dÄ±ÅŸÄ± bÄ±rak
os.environ["WANDB_DISABLED"] = "true"
os.environ["WANDB_MODE"] = "disabled"

# Proje root'una path ekle
sys.path.append(str(Path(__file__).parent))

from config import config
from utils.metrics import MetricsCalculator


# ============================================================================
# LOGGING SETUP
# ============================================================================

def setup_logging():
    """Logging yapÄ±landÄ±rmasÄ±"""
    logger.remove()
    logger.add(
        sys.stderr,
        format=config.LOG_FORMAT,
        level=config.LOG_LEVEL,
        colorize=True
    )
    logger.add(
        config.LOG_FILE,
        format=config.LOG_FORMAT,
        level=config.LOG_LEVEL,
        rotation="10 MB"
    )


# ============================================================================
# DATA LOADING
# ============================================================================

def load_summarization_data():
    """
    Summarization datasÄ±nÄ± yÃ¼kle
    
    Returns:
        (train_data, test_data) tuple
    """
    logger.info("=" * 80)
    logger.info("ğŸ“¥ SUMMARIZATION DATA YÃœKLEME")
    logger.info("=" * 80)
    
    train_path = config.PROCESSED_DATA_DIR / "train.json"
    test_path = config.PROCESSED_DATA_DIR / "test.json"
    
    if not train_path.exists() or not test_path.exists():
        logger.error("âŒ Data dosyalarÄ± bulunamadÄ±!")
        logger.error("   Ã–nce 1_data_preparation.py Ã§alÄ±ÅŸtÄ±rÄ±n!")
        raise FileNotFoundError("train.json veya test.json bulunamadÄ±")
    
    logger.info(f"ğŸ“‚ Train: {train_path}")
    logger.info(f"ğŸ“‚ Test: {test_path}")
    
    with open(train_path, "r", encoding="utf-8") as f:
        train_data = json.load(f)
    
    with open(test_path, "r", encoding="utf-8") as f:
        test_data = json.load(f)
    
    logger.info(f"âœ… Veri yÃ¼klendi:")
    logger.info(f"  â€¢ Train: {len(train_data):,} samples")
    logger.info(f"  â€¢ Test: {len(test_data):,} samples")
    
    return train_data, test_data


def create_dataset_from_data(data, tokenizer, max_samples=None):
    """
    Dict data'dan Hugging Face Dataset oluÅŸtur
    
    Args:
        data: JSON data listesi
        tokenizer: Tokenizer
        max_samples: Maksimum sample sayÄ±sÄ±
        
    Returns:
        Dataset
    """
    from datasets import Dataset
    
    if max_samples:
        data = data[:max_samples]
    
    # Dataset formatÄ±na Ã§evir
    dataset_dict = {
        "text": [item["text"] for item in data],
        "summary": [item["summary"] for item in data],
        "label": [item["label"] for item in data]
    }
    
    dataset = Dataset.from_dict(dataset_dict)
    
    return dataset


# ============================================================================
# MODEL SETUP
# ============================================================================

def load_model_and_tokenizer(model_name: str = None):
    """
    Base summarization modelini yÃ¼kle
    
    Args:
        model_name: Model adÄ± (None = config'den)
        
    Returns:
        (model, tokenizer) tuple
    """
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ¤– MODEL VE TOKENIZER YÃœKLEME")
    logger.info("=" * 80)
    
    if model_name is None:
        model_name = config.BASE_MODEL_NAME
    
    logger.info(f"ğŸ“¦ Model: {model_name}")
    
    from transformers import (
        AutoTokenizer,
        AutoModelForSeq2SeqLM
    )
    
    # Tokenizer
    logger.info("ğŸ“ Tokenizer yÃ¼kleniyor...")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    logger.info(f"  âœ… Vocab size: {len(tokenizer):,}")
    
    # Model
    logger.info("ğŸ§  Model yÃ¼kleniyor...")
    
    # CPU'da float16 sorunlu olabilir, float32 kullan
    device = "cuda" if torch.cuda.is_available() else "cpu"
    use_fp16 = config.FP16 and device == "cuda"
    
    model = AutoModelForSeq2SeqLM.from_pretrained(
        model_name
    )
    
    # TÃ¼m parametreleri freeze et (LoRA sadece adapter'larÄ± train edecek)
    for param in model.parameters():
        param.requires_grad = False
    
    # Model info
    total_params = sum(p.numel() for p in model.parameters())
    logger.info(f"  âœ… Model yÃ¼klendi!")
    logger.info(f"  ğŸ“Š Total parameters: {total_params:,}")
    logger.info(f"  ğŸ’¾ Model size: ~{total_params * 2 / 1024**3:.2f} GB")
    logger.info(f"  ğŸ”§ dtype: {next(model.parameters()).dtype}")
    logger.info(f"  ğŸ–¥ï¸  device: {device}")
    
    return model, tokenizer


def apply_lora(model):
    """
    Modele LoRA uygula
    
    Args:
        model: Base model
        
    Returns:
        PEFT model
    """
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ”§ LORA UYGULAMA")
    logger.info("=" * 80)
    
    from peft import LoraConfig, get_peft_model, TaskType
    
    # LoRA config
    lora_config = LoraConfig(
        task_type=TaskType.SEQ_2_SEQ_LM,  # Sequence-to-sequence task
        r=config.LORA_R,
        lora_alpha=config.LORA_ALPHA,
        lora_dropout=config.LORA_DROPOUT,
        target_modules=config.LORA_TARGET_MODULES,  # None = auto-detect
        bias=config.LORA_BIAS,
        inference_mode=False
    )
    
    logger.info("ğŸ“ LoRA Configuration:")
    logger.info(f"  â€¢ Rank (r): {config.LORA_R}")
    logger.info(f"  â€¢ Alpha (Î±): {config.LORA_ALPHA}")
    logger.info(f"  â€¢ Dropout: {config.LORA_DROPOUT}")
    logger.info(f"  â€¢ Scaling: {config.LORA_ALPHA / config.LORA_R}")
    
    # LoRA uygula
    logger.info("\nğŸ”„ PEFT modeli oluÅŸturuluyor...")
    model = get_peft_model(model, lora_config)
    
    # Trainable parameters
    logger.info("\nğŸ“Š PARAMETRE ANALÄ°ZÄ°:")
    model.print_trainable_parameters()
    
    # Gradient kontrolÃ¼
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model.parameters())
    logger.info(f"  â€¢ Gradient-enabled params: {trainable_params:,}")
    logger.info(f"  â€¢ Total params: {total_params:,}")
    logger.info(f"  â€¢ Trainable ratio: {100 * trainable_params / total_params:.2f}%")
    
    return model


# ============================================================================
# DATA PREPROCESSING
# ============================================================================

def preprocess_function(examples, tokenizer):
    """
    Summarization iÃ§in data preprocessing
    
    Args:
        examples: Batch examples
        tokenizer: Tokenizer
        
    Returns:
        Tokenized batch
    """
    # Input (review text)
    inputs = examples["text"]
    
    # Target (summary)
    targets = examples["summary"]
    
    # Tokenize inputs
    model_inputs = tokenizer(
        inputs,
        max_length=config.CHUNK_SIZE,
        truncation=True,
        padding="max_length",
        return_tensors=None  # Dataset iÃ§in None
    )
    
    # Tokenize targets
    labels = tokenizer(
        targets,
        max_length=config.MAX_SUMMARY_LENGTH,
        truncation=True,
        padding="max_length",
        return_tensors=None
    )
    
    # Labels'Ä± model_inputs'a ekle
    model_inputs["labels"] = labels["input_ids"]
    
    return model_inputs


def prepare_datasets(train_data, test_data, tokenizer, max_train=None, max_test=None):
    """
    Dataset'leri hazÄ±rla ve tokenize et
    
    Args:
        train_data: Train data
        test_data: Test data
        tokenizer: Tokenizer
        max_train: Max train samples
        max_test: Max test samples
        
    Returns:
        (train_dataset, test_dataset) tokenized
    """
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ”„ DATASET HAZIRLAMA VE TOKENÄ°ZASYON")
    logger.info("=" * 80)
    
    # Datasets oluÅŸtur
    train_dataset = create_dataset_from_data(train_data, tokenizer, max_train)
    test_dataset = create_dataset_from_data(test_data, tokenizer, max_test)
    
    logger.info(f"ğŸ“Š Dataset boyutlarÄ±:")
    logger.info(f"  â€¢ Train: {len(train_dataset):,}")
    logger.info(f"  â€¢ Test: {len(test_dataset):,}")
    
    # Tokenize
    logger.info("\nğŸ“ Tokenization...")
    
    train_dataset = train_dataset.map(
        lambda x: preprocess_function(x, tokenizer),
        batched=True,
        remove_columns=["text", "summary", "label"],
        desc="Tokenizing train"
    )
    
    test_dataset = test_dataset.map(
        lambda x: preprocess_function(x, tokenizer),
        batched=True,
        remove_columns=["text", "summary", "label"],
        desc="Tokenizing test"
    )
    
    logger.info("âœ… Tokenization tamamlandÄ±!")
    
    return train_dataset, test_dataset


# ============================================================================
# TRAINING
# ============================================================================

def train_model(model, tokenizer, train_dataset, test_dataset, args):
    """
    Model'i eÄŸit
    
    Args:
        model: PEFT model
        tokenizer: Tokenizer
        train_dataset: Train dataset
        test_dataset: Test dataset
        args: CLI arguments
        
    Returns:
        Trained model
    """
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ“ MODEL EÄÄ°TÄ°MÄ°")
    logger.info("=" * 80)
    
    from transformers import (
        Trainer,
        TrainingArguments,
        DataCollatorForSeq2Seq
    )
    
    # Training arguments
    training_args = TrainingArguments(
        # Ã‡Ä±ktÄ±
        output_dir=str(config.LORA_MODEL_DIR),
        logging_dir=str(config.LOGS_DIR / "training"),
        
        # EÄŸitim
        num_train_epochs=args.epochs if args.epochs else config.NUM_EPOCHS,
        per_device_train_batch_size=args.batch_size if args.batch_size else config.BATCH_SIZE_TRAIN,
        per_device_eval_batch_size=config.BATCH_SIZE_EVAL,
        gradient_accumulation_steps=config.GRADIENT_ACCUMULATION_STEPS,
        
        # Optimization
        learning_rate=config.LEARNING_RATE,
        warmup_steps=config.WARMUP_STEPS,
        weight_decay=config.WEIGHT_DECAY,
        max_grad_norm=config.MAX_GRAD_NORM,
        
        # Mixed precision
        fp16=config.FP16 and torch.cuda.is_available(),
        
        # Logging ve saving
        logging_steps=config.LOGGING_STEPS,
        eval_strategy="steps",
        eval_steps=config.EVAL_STEPS,
        save_strategy="steps",
        save_steps=config.SAVE_STEPS,
        save_total_limit=config.SAVE_TOTAL_LIMIT,
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        
        # DiÄŸer
        report_to=None,  # WandB kapalÄ±
        seed=42,
        dataloader_num_workers=4,
        remove_unused_columns=False,
        
        # Gradient checkpointing kapalÄ± (LoRA ile uyumsuz olabilir)
        gradient_checkpointing=False,
    )
    
    # Data collator (dynamic padding)
    data_collator = DataCollatorForSeq2Seq(
        tokenizer=tokenizer,
        model=model,
        padding=True
    )
    
    # Trainer
    logger.info("ğŸ”§ Trainer oluÅŸturuluyor...")
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=test_dataset,
        tokenizer=tokenizer,
        data_collator=data_collator,
    )
    
    # EÄŸitim bilgisi
    logger.info("\nğŸ“‹ EÄÄ°TÄ°M PARAMETRELERÄ°:")
    logger.info(f"  â€¢ Epochs: {training_args.num_train_epochs}")
    logger.info(f"  â€¢ Batch size (per device): {training_args.per_device_train_batch_size}")
    logger.info(f"  â€¢ Gradient accumulation: {training_args.gradient_accumulation_steps}")
    logger.info(f"  â€¢ Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps * training_args.world_size}")
    logger.info(f"  â€¢ Learning rate: {training_args.learning_rate:.0e}")
    logger.info(f"  â€¢ Warmup steps: {training_args.warmup_steps}")
    logger.info(f"  â€¢ FP16: {training_args.fp16}")
    logger.info(f"  â€¢ Device: {training_args.device}")
    
    # EÄŸitim
    logger.info("\nğŸš€ EÄŸitim baÅŸlÄ±yor...\n")
    
    try:
        trainer.train()
        logger.info("\nâœ… EÄŸitim tamamlandÄ±!")
    except KeyboardInterrupt:
        logger.warning("\nâš ï¸  EÄŸitim kullanÄ±cÄ± tarafÄ±ndan durduruldu!")
        logger.info("ğŸ’¾ Mevcut checkpoint kaydediliyor...")
        trainer.save_model(str(config.LORA_MODEL_DIR / "interrupted"))
        raise
    
    return trainer.model, trainer


# ============================================================================
# EVALUATION
# ============================================================================

def evaluate_model(model, tokenizer, test_data, num_samples=100):
    """
    Model'i deÄŸerlendir ve Ã¶rnekler oluÅŸtur
    
    Args:
        model: Trained model
        tokenizer: Tokenizer
        test_data: Test data
        num_samples: KaÃ§ sample deÄŸerlendirilecek
        
    Returns:
        Evaluation results dict
    """
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ“Š MODEL DEÄERLENDÄ°RME")
    logger.info("=" * 80)
    
    device = config.DEVICE
    model = model.to(device)
    model.eval()
    
    # Sample'larÄ± al
    test_samples = test_data[:num_samples]
    
    logger.info(f"ğŸ” {len(test_samples)} sample deÄŸerlendiriliyor...")
    
    predictions = []
    references = []
    
    with torch.no_grad():
        for item in tqdm(test_samples, desc="Generating summaries"):
            text = item["text"]
            reference = item["summary"]
            
            # Tokenize input
            inputs = tokenizer(
                text,
                max_length=config.CHUNK_SIZE,
                truncation=True,
                padding="max_length",
                return_tensors="pt"
            ).to(device)
            
            # Generate
            outputs = model.generate(
                **inputs,
                max_new_tokens=config.MAX_SUMMARY_LENGTH,
                min_length=config.MIN_SUMMARY_LENGTH,
                num_beams=config.NUM_BEAMS,
                length_penalty=config.LENGTH_PENALTY,
                no_repeat_ngram_size=config.NO_REPEAT_NGRAM_SIZE,
                early_stopping=config.EARLY_STOPPING,
            )
            
            # Decode
            prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            predictions.append(prediction)
            references.append(reference)
    
    # Metrikleri hesapla
    logger.info("\nğŸ“ˆ Metrikler hesaplanÄ±yor...")
    metrics_calc = MetricsCalculator(config)
    
    results = metrics_calc.evaluate_summarization(
        predictions=predictions,
        references=references,
        include_bertscore=False  # HÄ±zlÄ± deÄŸerlendirme iÃ§in kapalÄ±
    )
    
    # SonuÃ§larÄ± gÃ¶ster
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ“Š DEÄERLENDIRME SONUÃ‡LARI")
    logger.info("=" * 80)
    
    for metric, value in results.items():
        if isinstance(value, float):
            logger.info(f"  â€¢ {metric}: {value:.4f}")
        else:
            logger.info(f"  â€¢ {metric}: {value}")
    
    # Ã–rnek Ã§Ä±ktÄ±lar gÃ¶ster
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ“ Ã–RNEK Ã‡IKTILAR")
    logger.info("=" * 80)
    
    for i in range(min(3, len(predictions))):
        logger.info(f"\n[Example {i+1}]")
        logger.info(f"Input: {test_samples[i]['text'][:200]}...")
        logger.info(f"\nReference: {references[i]}")
        logger.info(f"\nPrediction: {predictions[i]}")
        logger.info("-" * 80)
    
    return results


# ============================================================================
# SAVE MODEL
# ============================================================================

def save_final_model(model, tokenizer):
    """
    Final modeli kaydet
    
    Args:
        model: Trained model
        tokenizer: Tokenizer
    """
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ’¾ FÄ°NAL MODEL KAYDETME")
    logger.info("=" * 80)
    
    save_dir = config.LORA_MODEL_DIR / "final"
    save_dir.mkdir(parents=True, exist_ok=True)
    
    logger.info(f"ğŸ“ Save directory: {save_dir}")
    
    # Model kaydet
    logger.info("ğŸ§  Model kaydediliyor...")
    model.save_pretrained(str(save_dir))
    
    # Tokenizer kaydet
    logger.info("ğŸ“ Tokenizer kaydediliyor...")
    tokenizer.save_pretrained(str(save_dir))
    
    # Config kaydet
    logger.info("âš™ï¸  Config kaydediliyor...")
    config_info = {
        "base_model": config.BASE_MODEL_NAME,
        "lora_config": {
            "r": config.LORA_R,
            "alpha": config.LORA_ALPHA,
            "dropout": config.LORA_DROPOUT
        },
        "training": {
            "epochs": config.NUM_EPOCHS,
            "batch_size": config.BATCH_SIZE_TRAIN,
            "learning_rate": config.LEARNING_RATE
        }
    }
    
    with open(save_dir / "training_config.json", "w") as f:
        json.dump(config_info, f, indent=2)
    
    logger.info(f"âœ… Model kaydedildi: {save_dir}")
    
    # Boyut bilgisi
    total_size = sum(
        f.stat().st_size for f in save_dir.rglob("*") if f.is_file()
    )
    logger.info(f"ğŸ’¾ Total size: {total_size / 1024 / 1024:.1f} MB")


# ============================================================================
# MAIN PIPELINE
# ============================================================================

def main(args):
    """Ana eÄŸitim pipeline'Ä±"""
    
    logger.info("\n")
    logger.info("â•”" + "â•" * 78 + "â•—")
    logger.info("â•‘" + " " * 18 + "LORA SUMMARIZER TRAINING PIPELINE" + " " * 27 + "â•‘")
    logger.info("â•š" + "â•" * 78 + "â•")
    logger.info("\n")
    
    # Configuration
    config.print_config()
    
    # 1. Data yÃ¼kle
    train_data, test_data = load_summarization_data()
    
    # 2. Model ve tokenizer yÃ¼kle
    model, tokenizer = load_model_and_tokenizer(args.model)
    
    # 3. LoRA uygula
    model = apply_lora(model)
    
    # 4. Datasets hazÄ±rla
    train_dataset, test_dataset = prepare_datasets(
        train_data, test_data, tokenizer,
        max_train=args.max_train,
        max_test=args.max_test
    )
    
    # 5. EÄŸitim
    model, trainer = train_model(model, tokenizer, train_dataset, test_dataset, args)
    
    # 6. DeÄŸerlendirme
    if args.evaluate:
        results = evaluate_model(model, tokenizer, test_data, num_samples=args.eval_samples)
        
        # NumPy tiplerini Python tiplerine dÃ¶nÃ¼ÅŸtÃ¼r
        def convert_to_python_types(obj):
            """NumPy ve diÄŸer Ã¶zel tipleri Python tiplerine dÃ¶nÃ¼ÅŸtÃ¼r"""
            import numpy as np
            if isinstance(obj, dict):
                return {k: convert_to_python_types(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_to_python_types(v) for v in obj]
            elif isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            else:
                return obj
        
        results = convert_to_python_types(results)
        
        # SonuÃ§larÄ± kaydet
        results_path = config.EVAL_RESULTS_DIR / "summarization_results.json"
        with open(results_path, "w") as f:
            json.dump(results, f, indent=2)
        logger.info(f"ğŸ’¾ Results saved: {results_path}")
    
    # 7. Kaydet
    save_final_model(model, tokenizer)
    
    # Final
    logger.info("\n")
    logger.info("â•”" + "â•" * 78 + "â•—")
    logger.info("â•‘" + " " * 28 + "âœ… BAÅARILI! âœ…" + " " * 35 + "â•‘")
    logger.info("â•š" + "â•" * 78 + "â•")
    logger.info("\n")
    logger.info("ğŸ“ Model: " + str(config.LORA_MODEL_DIR / "final"))
    logger.info("\nğŸš€ Sonraki adÄ±m: python 4_rag_qa_system.py")


# ============================================================================
# CLI
# ============================================================================

if __name__ == "__main__":
    setup_logging()
    
    parser = argparse.ArgumentParser(
        description="Train LoRA summarization model"
    )
    parser.add_argument(
        "--model",
        type=str,
        default=None,
        help="Base model name (default: from config)"
    )
    parser.add_argument(
        "--epochs",
        type=int,
        default=None,
        help="Number of epochs (default: from config)"
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=None,
        help="Batch size (default: from config)"
    )
    parser.add_argument(
        "--max-train",
        type=int,
        default=None,
        help="Max train samples"
    )
    parser.add_argument(
        "--max-test",
        type=int,
        default=None,
        help="Max test samples"
    )
    parser.add_argument(
        "--evaluate",
        action="store_true",
        default=True,
        help="Run evaluation after training"
    )
    parser.add_argument(
        "--eval-samples",
        type=int,
        default=100,
        help="Number of samples for evaluation"
    )
    
    args = parser.parse_args()
    
    try:
        main(args)
    except KeyboardInterrupt:
        logger.warning("\nâš ï¸  Ä°ÅŸlem kullanÄ±cÄ± tarafÄ±ndan durduruldu!")
        sys.exit(1)
    except Exception as e:
        logger.exception(f"âŒ Hata: {e}")
        sys.exit(1)
