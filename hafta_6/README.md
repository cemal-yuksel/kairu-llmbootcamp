# ğŸ¯ **Hafta 6: Model Fine-Tuning - PEFT, LoRA ve Production Inference**
![Durum](https://img.shields.io/badge/Durum-TamamlandÄ±-brightgreen)
![Odak](https://img.shields.io/badge/Odak-PEFT%20%7C%20LoRA%20%7C%20Fine--Tuning%20%7C%20Inference-blue)
![Teknoloji](https://img.shields.io/badge/Teknoloji-Transformers%20%7C%20PEFT%20%7C%20Datasets%20%7C%20Trainer%20API-blueviolet)

---

## ğŸ“ Dosya YapÄ±sÄ±

Bu klasÃ¶rdeki ana dosyalar ve iÃ§erikleri:

| Dosya AdÄ±                           | AÃ§Ä±klama                                                                                        |
|-------------------------------------|-------------------------------------------------------------------------------------------------|
| `1_peft_lora.py`                    | PEFT ve LoRA ile parameter-efficient fine-tuning, model optimizasyonu ve eÄŸitim sÃ¼reÃ§leri      |
| `2_datasets_trainer.py`             | Hugging Face Datasets ve Trainer API kullanÄ±mÄ±, Ã¶zel metrik hesaplama ve veri yÃ¶netimi         |
| `3_inference_personalization.py`    | Production inference, kiÅŸiselleÅŸtirilmiÅŸ chatbot, quantization ve deployment stratejileri      |
| `requirements.txt`                  | Gerekli Python paketleri (transformers, PEFT, datasets, torch, accelerate, bitsandbytes)       |
| `README.md`                         | HaftanÄ±n Ã¶zeti, kullanÄ±m talimatlarÄ±, teknik aÃ§Ä±klamalar ve best practices                     |
| `fine_tuned_model/`                 | Fine-tune edilmiÅŸ model checkpointleri ve konfigÃ¼rasyon dosyalarÄ±                              |
| `lora_model/`                       | LoRA adapter modeli, tokenizer ve eÄŸitim parametreleri                                          |
| `lora_results/`                     | LoRA eÄŸitim checkpointleri (checkpoint-4, checkpoint-6)                                         |
| `results/`                          | Full fine-tuning eÄŸitim checkpointleri (checkpoint-1, 2, 3)                                     |

Her dosya, modern LLM fine-tuning teknikleri, memory-efficient eÄŸitim yÃ¶ntemleri ve production-ready inference implementasyonlarÄ±nÄ± detaylÄ± Ã¶rneklerle gÃ¶sterir.

---

## ğŸ¯ HaftanÄ±n Ã–zeti

Bu hafta, **Large Language Model (LLM)** fine-tuning sÃ¼reÃ§lerinin profesyonel dÃ¼zeyde uygulanmasÄ±nÄ± Ã¶ÄŸrendim. **PEFT (Parameter Efficient Fine-Tuning)** ve **LoRA (Low-Rank Adaptation)** teknikleri ile bÃ¼yÃ¼k modelleri minimum kaynak kullanÄ±mÄ±yla nasÄ±l Ã¶zelleÅŸtireceÄŸimi, **Hugging Face Datasets** ve **Trainer API** ile modern veri pipeline'larÄ± nasÄ±l oluÅŸturacaÄŸÄ±mÄ±, ve production ortamÄ±nda **inference optimization** ile **model deployment** stratejilerini nasÄ±l uygulayacaÄŸÄ±mÄ± Ã¶ÄŸrendim.

Her aÅŸamada, memory-efficient training teknikleri, quantization stratejileri, custom metric hesaplama, real-time inference optimizasyonu ve production deployment best practices'leri ile LLM'leri gerÃ§ek dÃ¼nya uygulamalarÄ±nda nasÄ±l kullanabileceÄŸimi pratik Ã¶rneklerle deneyimledim.

Kodlarda, detaylÄ± aÃ§Ä±klamalar ve step-by-step implementasyonlar ile hem temel kavramlarÄ± hem de enterprise-grade LLM uygulamalarÄ±nÄ±n nasÄ±l geliÅŸtirebileceÄŸini kapsamlÄ± Ã¶rneklerle sundum.

---

## ğŸš¦ Fine-Tuning ve Inference Pipeline YolculuÄŸu

<p align="center" style="font-size:1.1em;">
	<b>ğŸ¯ PEFT & LoRA â†’ Datasets & Training â†’ Inference & Deployment<br>
	<span style="color:#2980B9;">Modern LLM</span> ile <span style="color:#CA6F1E;">verimli</span>, <span style="color:#229954;">Ã¶lÃ§eklenebilir</span> ve <span style="color:#8E44AD;">production-ready</span> Ã§Ã¶zÃ¼mler!</b>
</p>

```mermaid
flowchart TD
		style A1 fill:#D6EAF8,stroke:#2980B9,stroke-width:3px
		style B1 fill:#F9E79F,stroke:#B7950B,stroke-width:3px
		style B2 fill:#D5F5E3,stroke:#229954,stroke-width:3px
		style B3 fill:#FADBD8,stroke:#C0392B,stroke-width:3px
		style B4 fill:#E8DAEF,stroke:#8E44AD,stroke-width:3px
		style B5 fill:#FDEBD0,stroke:#CA6F1E,stroke-width:3px
		style Z1 fill:#D5DBDB,stroke:#34495E,stroke-width:3px

		A1([<b>ğŸ”§<br>PEFT & LoRA Setup</b>])
		B1([<b>ğŸ“Š<br>Datasets & Training<br><i>Trainer API</i></b>])
		B2([<b>ğŸ¯<br>Fine-Tuning<br><i>Optimization</i></b>])
		B3([<b>ğŸš€<br>Inference Engine</b>])
		B4([<b>âš¡<br>Quantization<br><i>INT8/FP16</i></b>])
		B5([<b>ğŸŒ<br>Production Deploy</b>])
		Z1([<b>âœ¨<br>Enterprise-Grade LLM System</b>])

		A1 --> B1
		B1 --> B2
		B2 --> B3
		B3 --> B4
		B4 --> B5
		B5 --> Z1
		A1 -.-> B4
		B2 -.-> B5
```

<p align="center" style="font-size:1.1em; margin-top:10px;">
	<b>âœ¨ <span style="color:#229954;">PEFT & LoRA</span> ile <span style="color:#C0392B;">%96+ memory tasarrufu</span>, <span style="color:#CA6F1E;">hÄ±zlÄ± eÄŸitim</span> ve <span style="color:#8E44AD;">production-ready</span> LLM sistemleri! âœ¨</b>
</p>

---

## ğŸ“š Ä°Ã§erik

### 1. PEFT ve LoRA ile Memory-Efficient Fine-Tuning  
**Dosya:** `1_peft_lora.py`  
- **LoRA (Low-Rank Adaptation):** DÃ¼ÅŸÃ¼k rankli matris decomposition ile parameter-efficient training
- **PEFT Configuration:** Rank (r), Alpha (Î±), Dropout ve target modules optimizasyonu
- **Memory Optimization:** %96+ parametre tasarrufu ile bÃ¼yÃ¼k modelleri kÃ¼Ã§Ã¼k GPU'larda eÄŸitme
- **Adapter Training:** Orijinal modeli freeze ederek sadece adapter katmanlarÄ± eÄŸitme
- **Model Analysis:** Trainable vs frozen parameters detaylÄ± analizi
- **Checkpoint Management:** Model kaydetme, yÃ¼kleme ve versiyonlama

**Teknik Detaylar:**
```python
# LoRA Matematiksel FormÃ¼l: W = Wâ‚€ + Î”W = Wâ‚€ + BA
# Wâ‚€: Frozen original weights (1024Ã—1024 = 1,048,576 params)
# BÃ—A: Low-rank matrices (1024Ã—16 + 16Ã—1024 = 32,768 params)
# Tasarruf: %96.9 daha az eÄŸitilebilir parametre!

lora_config = LoraConfig(
    r=16,                    # Rank deÄŸeri
    lora_alpha=32,           # Scaling factor (Î±/r = 2.0)
    target_modules=["c_attn", "c_proj"],  # Attention layers
    lora_dropout=0.1,
    task_type=TaskType.CAUSAL_LM
)
```

---

### 2. Hugging Face Datasets ve Trainer API KullanÄ±mÄ±  
**Dosya:** `2_datasets_trainer.py`  
- **Dataset Management:** Veri yÃ¼kleme, preprocessing ve augmentation
- **Tokenization Pipeline:** Batch tokenization, padding ve truncation stratejileri
- **Trainer API:** High-level training interface ile otomatik eÄŸitim loop
- **Training Arguments:** Learning rate scheduling, warmup, gradient accumulation
- **Custom Metrics:** Accuracy, Precision, Recall, F1-Score hesaplama
- **Evaluation Strategy:** Training sÄ±rasÄ±nda validation ve early stopping
- **Data Collator:** Dynamic padding ile memory optimizasyonu

**Best Practices:**
```python
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=8,
    gradient_accumulation_steps=4,  # Effective batch = 32
    warmup_steps=500,
    weight_decay=0.01,
    logging_steps=10,
    evaluation_strategy="steps",
    eval_steps=100,
    save_strategy="epoch",
    load_best_model_at_end=True,
    fp16=True  # Mixed precision training
)
```

---

### 3. Production Inference ve KiÅŸiselleÅŸtirme  
**Dosya:** `3_inference_personalization.py`  
- **Inference Engine:** PersonalizedInference sÄ±nÄ±fÄ± ile production-ready inference
- **Generation Strategies:** Deterministik vs yaratÄ±cÄ± text generation
- **Quantization Techniques:** INT8, FP16, FP32 model optimizasyonu
- **Batch Processing:** Multi-sample inference ile throughput optimizasyonu
- **KV-Cache Optimization:** Memory ve speed optimizasyonu
- **Personalized Chatbot:** User profile bazlÄ± kiÅŸiselleÅŸtirilmiÅŸ yanÄ±tlar
- **Deployment Strategies:** REST API, gRPC, Docker ve Kubernetes

**Inference Optimization:**
```python
# 1. Quantization (8-bit inference)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    load_in_8bit=True,
    device_map="auto"
)

# 2. Generation Config
generation_config = {
    # Deterministik (faktÃ¼el iÃ§erik)
    "deterministic": {
        "temperature": 0.1,
        "do_sample": False,
        "num_beams": 1
    },
    # YaratÄ±cÄ± (blog, hikaye)
    "creative": {
        "temperature": 0.9,
        "top_p": 0.95,
        "top_k": 50,
        "do_sample": True
    }
}

# 3. Batch Inference
results = model.generate(
    input_ids_batch,
    max_new_tokens=100,
    use_cache=True  # KV-cache optimization
)
```

---

## ğŸ› ï¸ Kurulum ve KullanÄ±m

### ğŸ“¦ Gereksinimler

```bash
# Ana baÄŸÄ±mlÄ±lÄ±klar
torch>=2.0.0
transformers>=4.21.0
datasets>=2.14.0
peft>=0.4.0
accelerate>=0.21.0
bitsandbytes>=0.41.0

# YardÄ±mcÄ± kÃ¼tÃ¼phaneler
scikit-learn>=1.3.0
numpy>=1.24.0
tqdm>=4.65.0
```

### ğŸš€ HÄ±zlÄ± BaÅŸlangÄ±Ã§

#### 1. Sanal Ortam OluÅŸtur
```powershell
# Windows PowerShell
python -m venv venv
.\venv\Scripts\Activate.ps1

# BaÄŸÄ±mlÄ±lÄ±klarÄ± yÃ¼kle
pip install --upgrade pip
pip install -r requirements.txt
```

#### 2. PEFT & LoRA Fine-Tuning
```powershell
# LoRA ile model eÄŸitimi
python 1_peft_lora.py

# Ã‡Ä±ktÄ±lar:
# - ./lora_results/        â†’ EÄŸitim checkpointleri
# - ./lora_model/          â†’ Final LoRA adapter modeli
```

#### 3. Datasets & Trainer KullanÄ±mÄ±
```powershell
# Trainer API ile model eÄŸitimi
python 2_datasets_trainer.py

# Ã‡Ä±ktÄ±lar:
# - ./results/             â†’ Full fine-tuning checkpointleri
# - ./fine_tuned_model/    â†’ Final model
```

#### 4. Production Inference
```powershell
# Inference ve deployment Ã¶rnekleri
python 3_inference_personalization.py

# Test senaryolarÄ±:
# - Quantization techniques
# - Generation strategies
# - Personalized chatbot
# - Performance benchmarking
```

---

## ğŸŒŸ HaftanÄ±n AÅŸamalarÄ± & SÄ±kÃ§a Sorulanlar

### 1. **PEFT ve LoRA Nedir? Neden KullanÄ±lÄ±r?**

<div style="border:1px solid #2980B9; border-radius:8px; padding:12px; background:#F4F8FB; margin:10px 0;">

**Soru:** PEFT ve LoRA teknikleri ne iÅŸe yarar?

**Cevap:** PEFT (Parameter Efficient Fine-Tuning), bÃ¼yÃ¼k modellerin tÃ¼m parametrelerini eÄŸitmek yerine sadece kÃ¼Ã§Ã¼k bir kÄ±smÄ±nÄ± veya adapter katmanlarÄ±nÄ± eÄŸitme yaklaÅŸÄ±mÄ±dÄ±r. LoRA, bu yaklaÅŸÄ±mÄ±n en popÃ¼ler implementasyonudur.

**Avantajlar:**
- ğŸ’¾ **Memory Tasarrufu:** %96+ daha az eÄŸitilebilir parametre
- âš¡ **HÄ±zlÄ± EÄŸitim:** Daha az parametre = daha hÄ±zlÄ± backpropagation
- ğŸ’° **Maliyet DÃ¼ÅŸÃ¼rme:** KÃ¼Ã§Ã¼k GPU'larda bÃ¼yÃ¼k modeller eÄŸitilebilir
- ğŸ”„ **ModÃ¼lerlik:** FarklÄ± gÃ¶revler iÃ§in farklÄ± adapter'lar

**Matematiksel Temel:**
```
W = Wâ‚€ + Î”W = Wâ‚€ + BA

Wâ‚€: Orijinal aÄŸÄ±rlÄ±klar (frozen)
B, A: DÃ¼ÅŸÃ¼k rankli matrisler (trainable)
rank(BA) << rank(Wâ‚€)

Ã–rnek: 1024Ã—1024 matris â†’ 1M parametre
LoRA (r=16): (1024Ã—16) + (16Ã—1024) = 32K parametre
Tasarruf: %96.9!
```

</div>

**Kod Ã–rneÄŸi:**
```python
from peft import LoraConfig, get_peft_model

# LoRA konfigÃ¼rasyonu
lora_config = LoraConfig(
    r=16,                           # Rank (dÃ¼ÅŸÃ¼k = az parametre, yÃ¼ksek = fazla kapasite)
    lora_alpha=32,                  # Scaling factor
    target_modules=["c_attn"],      # Hangi katmanlar eÄŸitilecek
    lora_dropout=0.1,
    task_type=TaskType.CAUSAL_LM
)

# Modele LoRA adapter ekle
model = get_peft_model(base_model, lora_config)

# Parametre analizi
model.print_trainable_parameters()
# trainable params: 294,912 || all params: 124,734,720 || trainable%: 0.24%
```

---

### 2. **Datasets ve Trainer API ile EÄŸitim**

<div style="border:1px solid #229954; border-radius:8px; padding:12px; background:#F4FBF4; margin:10px 0;">

**Soru:** Hugging Face Datasets ve Trainer API neden kullanÄ±lÄ±r?

**Cevap:** Datasets kÃ¼tÃ¼phanesi, bÃ¼yÃ¼k veri setlerini memory-efficient ÅŸekilde yÃ¶netir. Trainer API ise training loop'u otomatikleÅŸtirir ve best practices'leri uygular.

**Datasets AvantajlarÄ±:**
- ğŸ“Š **Memory Mapping:** RAM'i optimize eder
- âš¡ **Apache Arrow:** HÄ±zlÄ± okuma/yazma
- ğŸ”„ **Lazy Loading:** Sadece gerekli veri yÃ¼klenir
- ğŸ› ï¸ **Preprocessing:** Batch tokenization ve transformations

**Trainer API AvantajlarÄ±:**
- ğŸ¤– **Otomatik Training Loop:** Forward, backward, optimizer steps
- ğŸ“ˆ **Built-in Logging:** TensorBoard, WandB entegrasyonu
- ğŸ’¾ **Checkpoint Management:** Otomatik kaydetme ve yÃ¼kleme
- ğŸ¯ **Evaluation:** Validation ve metrics hesaplama

</div>

**Kod Ã–rneÄŸi:**
```python
from transformers import TrainingArguments, Trainer

# Training parametreleri
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=8,
    gradient_accumulation_steps=4,    # Effective batch size = 32
    warmup_steps=500,                 # Learning rate warmup
    weight_decay=0.01,                # L2 regularization
    logging_steps=10,
    evaluation_strategy="steps",
    eval_steps=100,
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    fp16=True                         # Mixed precision training
)

# Trainer oluÅŸtur
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    compute_metrics=compute_metrics
)

# EÄŸitimi baÅŸlat
trainer.train()
```

---

### 3. **Inference Optimization Teknikleri**

<div style="border:1px solid #CA6F1E; border-radius:8px; padding:12px; background:#FDEBD0; margin:10px 0;">

**Soru:** Production ortamÄ±nda inference nasÄ±l optimize edilir?

**Cevap:** Quantization, batch processing, KV-cache ve generation config optimizasyonlarÄ± ile hem hÄ±z hem memory kullanÄ±mÄ± iyileÅŸtirilebilir.

**Optimizasyon Teknikleri:**

1. **Quantization (Niceleme)**
   - INT8: %50 bellek tasarrufu, minimal accuracy kaybÄ±
   - FP16: %50 bellek tasarrufu, hÄ±z artÄ±ÅŸÄ±
   - 4-bit: %75 bellek tasarrufu, consumer GPU'larda kullanÄ±m

2. **Batch Processing**
   - Birden fazla Ã¶rneÄŸi aynÄ± anda iÅŸleme
   - Throughput artÄ±ÅŸÄ± (%200-400)
   - GPU utilization iyileÅŸtirme

3. **KV-Cache Optimization**
   - Attention cache'i tekrar kullanma
   - Autoregressive generation hÄ±zlandÄ±rma
   - Memory-speed trade-off

4. **Generation Config**
   - Temperature, top_p, top_k ayarlarÄ±
   - Beam search vs sampling
   - Deterministik vs yaratÄ±cÄ± Ã¼retim

</div>

**Kod Ã–rneÄŸi:**
```python
# 1. INT8 Quantization
from transformers import AutoModelForCausalLM
import torch

model = AutoModelForCausalLM.from_pretrained(
    "model_path",
    load_in_8bit=True,              # INT8 quantization
    device_map="auto",               # Otomatik device mapping
    torch_dtype=torch.float16       # FP16 computation
)

# 2. Batch Inference
input_ids_batch = tokenizer(
    ["Prompt 1", "Prompt 2", "Prompt 3"],
    return_tensors="pt",
    padding=True
)

outputs = model.generate(
    **input_ids_batch,
    max_new_tokens=100,
    use_cache=True,                 # KV-cache aktif
    num_beams=1,                    # Greedy decoding (hÄ±zlÄ±)
    do_sample=False                 # Deterministik
)

# 3. Generation Config
from transformers import GenerationConfig

# FaktÃ¼el iÃ§erik iÃ§in
factual_config = GenerationConfig(
    temperature=0.1,
    do_sample=False,
    num_beams=1
)

# YaratÄ±cÄ± iÃ§erik iÃ§in
creative_config = GenerationConfig(
    temperature=0.9,
    top_p=0.95,
    top_k=50,
    do_sample=True
)

output = model.generate(
    input_ids,
    generation_config=creative_config
)
```

---

### 4. **KiÅŸiselleÅŸtirilmiÅŸ Chatbot TasarÄ±mÄ±**

<div style="border:1px solid #C0392B; border-radius:8px; padding:12px; background:#FDF2F0; margin:10px 0;">

**Soru:** KullanÄ±cÄ± profiline gÃ¶re kiÅŸiselleÅŸtirilmiÅŸ yanÄ±tlar nasÄ±l oluÅŸturulur?

**Cevap:** User profile, conversation history ve context-aware prompting ile LLM'ler her kullanÄ±cÄ±ya Ã¶zel yanÄ±tlar Ã¼retebilir.

**KiÅŸiselleÅŸtirme Stratejileri:**

1. **User Profile Management**
   - KullanÄ±cÄ± tercihlerini saklama
   - Ä°lgi alanlarÄ± ve uzmanlÄ±k seviyesi
   - KonuÅŸma tarzÄ± (formal/casual)

2. **Context-Aware Prompting**
   - GeÃ§miÅŸ konuÅŸma geÃ§miÅŸi
   - KullanÄ±cÄ± metadata'sÄ±
   - Dinamik prompt construction

3. **Adaptive Generation**
   - KullanÄ±cÄ± feedback'ine gÃ¶re ayarlama
   - A/B testing ile optimizasyon
   - Reinforcement learning from human feedback (RLHF)

</div>

**Kod Ã–rneÄŸi:**
```python
class PersonalizedChatbot:
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self.user_profiles = {}
        
    def create_user_profile(self, user_id, preferences):
        """KullanÄ±cÄ± profili oluÅŸtur"""
        self.user_profiles[user_id] = {
            "name": preferences.get("name"),
            "expertise": preferences.get("expertise", "beginner"),
            "interests": preferences.get("interests", []),
            "tone": preferences.get("tone", "professional"),
            "conversation_history": []
        }
    
    def generate_response(self, user_id, query):
        """KiÅŸiselleÅŸtirilmiÅŸ yanÄ±t Ã¼ret"""
        profile = self.user_profiles.get(user_id)
        
        # KiÅŸiselleÅŸtirilmiÅŸ prompt oluÅŸtur
        system_prompt = f"""
        KullanÄ±cÄ±: {profile['name']}
        Seviye: {profile['expertise']}
        Ä°lgi AlanlarÄ±: {', '.join(profile['interests'])}
        Ton: {profile['tone']}
        
        YukarÄ±daki profili dikkate alarak yanÄ±t ver.
        """
        
        # Conversation history ekle
        context = "\n".join(profile['conversation_history'][-5:])
        
        full_prompt = f"{system_prompt}\n\nGeÃ§miÅŸ:\n{context}\n\nSoru: {query}\n\nYanÄ±t:"
        
        # Generate
        inputs = self.tokenizer(full_prompt, return_tensors="pt")
        outputs = self.model.generate(
            **inputs,
            max_new_tokens=200,
            temperature=0.7
        )
        
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # History'ye ekle
        profile['conversation_history'].append(f"User: {query}")
        profile['conversation_history'].append(f"Bot: {response}")
        
        return response

# KullanÄ±m
chatbot = PersonalizedChatbot(model, tokenizer)

chatbot.create_user_profile("user123", {
    "name": "Ali",
    "expertise": "intermediate",
    "interests": ["Python", "Machine Learning"],
    "tone": "casual"
})

response = chatbot.generate_response("user123", "Python'da liste comprehension nedir?")
```

---

### 5. **Production Deployment Stratejileri**

<div style="border:1px solid #8E44AD; border-radius:8px; padding:12px; background:#F7F1FA; margin:10px 0;">

**Soru:** LLM modellerini production ortamÄ±na nasÄ±l deploy ederiz?

**Cevap:** REST API, gRPC, Docker containerization ve Kubernetes orchestration ile scalable ve maintainable deployment yapÄ±labilir.

**Deployment SeÃ§enekleri:**

1. **REST API (FastAPI/Flask)**
   - Kolay entegrasyon
   - HTTP/JSON standardÄ±
   - Swagger documentation

2. **gRPC**
   - YÃ¼ksek performans
   - DÃ¼ÅŸÃ¼k latency
   - Protocol Buffers

3. **Docker Containerization**
   - Environment isolation
   - Reproducibility
   - Easy scaling

4. **Kubernetes Orchestration**
   - Auto-scaling
   - Load balancing
   - Health checks

</div>

**Kod Ã–rneÄŸi:**
```python
# FastAPI ile REST API
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import torch

app = FastAPI(title="LLM Inference API")

# Model yÃ¼kleme (startup event)
@app.on_event("startup")
async def load_model():
    global model, tokenizer
    model = AutoModelForCausalLM.from_pretrained(
        "./fine_tuned_model",
        load_in_8bit=True,
        device_map="auto"
    )
    tokenizer = AutoTokenizer.from_pretrained("./fine_tuned_model")

# Request model
class GenerateRequest(BaseModel):
    prompt: str
    max_tokens: int = 100
    temperature: float = 0.7

# Generate endpoint
@app.post("/generate")
async def generate_text(request: GenerateRequest):
    try:
        inputs = tokenizer(request.prompt, return_tensors="pt")
        
        outputs = model.generate(
            **inputs,
            max_new_tokens=request.max_tokens,
            temperature=request.temperature,
            do_sample=True
        )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        return {
            "status": "success",
            "prompt": request.prompt,
            "response": response,
            "tokens_generated": len(outputs[0])
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# Health check
@app.get("/health")
async def health_check():
    return {"status": "healthy", "model_loaded": model is not None}

# Dockerfile
"""
FROM python:3.10-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD ["uvicorn", "api:app", "--host", "0.0.0.0", "--port", "8000"]
"""

# Docker Compose
"""
version: '3.8'

services:
  llm-api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - MODEL_PATH=/models/fine_tuned_model
    volumes:
      - ./models:/models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
"""
```

---

## ğŸ’¡ En Ä°yi Uygulamalar

### PEFT & LoRA Best Practices

```python
# âœ… Ä°yi: Rank deÄŸerini gÃ¶reve gÃ¶re ayarla
lora_config = LoraConfig(
    r=16,                    # Basit gÃ¶revler iÃ§in 8-16
    lora_alpha=32,           # r'nin 1-2 katÄ±
    target_modules=["q_proj", "v_proj", "k_proj"],  # Attention layers
    lora_dropout=0.1
)

# âŒ KÃ¶tÃ¼: Ã‡ok yÃ¼ksek rank
lora_config = LoraConfig(r=256)  # Ã‡ok fazla parametre, PEFT avantajÄ± kaybolur
```

### Training Optimization

```python
# âœ… Ä°yi: Gradient accumulation ile bÃ¼yÃ¼k effective batch size
training_args = TrainingArguments(
    per_device_train_batch_size=4,      # GPU memory'ye sÄ±ÄŸan
    gradient_accumulation_steps=8,      # Effective batch = 32
    fp16=True,                          # Mixed precision
    gradient_checkpointing=True         # Memory optimization
)

# âŒ KÃ¶tÃ¼: Ã‡ok bÃ¼yÃ¼k batch size
training_args = TrainingArguments(
    per_device_train_batch_size=64  # OOM (Out of Memory) hatasÄ±!
)
```

### Inference Optimization

```python
# âœ… Ä°yi: Quantization ve cache kullanÄ±mÄ±
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    load_in_8bit=True,
    device_map="auto"
)

outputs = model.generate(
    input_ids,
    use_cache=True,              # KV-cache aktif
    max_new_tokens=100
)

# âŒ KÃ¶tÃ¼: Optimizasyon yok
model = AutoModelForCausalLM.from_pretrained(model_path)  # FP32, yavaÅŸ
outputs = model.generate(input_ids, use_cache=False)
```

---

## ğŸ“Š Performans KarÅŸÄ±laÅŸtÄ±rmasÄ±

<table align="center">
	<thead>
		<tr>
			<th style="background:#D6EAF8; color:#2980B9;"><b>Metrik</b></th>
			<th style="background:#F9E79F; color:#B7950B;"><b>Full Fine-Tuning</b></th>
			<th style="background:#D5F5E3; color:#229954;"><b>LoRA (r=16)</b></th>
			<th style="background:#FADBD8; color:#C0392B;"><b>Tasarruf</b></th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td><b>EÄŸitilebilir Parametreler</b></td>
			<td>124M</td>
			<td><span style="color:#229954;"><b>294K</b></span></td>
			<td>%99.76 â†“</td>
		</tr>
		<tr>
			<td><b>GPU Memory (EÄŸitim)</b></td>
			<td>24 GB</td>
			<td><b style="color:#CA6F1E;">8 GB</b></td>
			<td>%66.7 â†“</td>
		</tr>
		<tr>
			<td><b>EÄŸitim SÃ¼resi (3 epoch)</b></td>
			<td>4 saat</td>
			<td>1.5 saat</td>
			<td><b style="color:#8E44AD;">%62.5 â†“</b></td>
		</tr>
		<tr>
			<td><b>Model Boyutu</b></td>
			<td>500 MB</td>
			<td>2 MB (adapter)</td>
			<td>%99.6 â†“</td>
		</tr>
		<tr>
			<td><b>Inference Latency (INT8)</b></td>
			<td>250ms</td>
			<td>120ms</td>
			<td>%52 â†“</td>
		</tr>
	</tbody>
</table>

---

## ğŸ“ Ek Kaynaklar

> Modern LLM fine-tuning, PEFT, LoRA ve production deployment iÃ§in baÅŸvurduÄŸum **Ã¶nemli referanslar**:

<details>
<summary>ğŸ“˜ Hugging Face Transformers Documentation</summary>
<a href="https://huggingface.co/docs/transformers" target="_blank">https://huggingface.co/docs/transformers</a>  
ğŸ” Transformers kÃ¼tÃ¼phanesi, model yÃ¼kleme, training ve inference rehberi.
</details>

<details>
<summary>âš¡ PEFT Library</summary>
<a href="https://github.com/huggingface/peft" target="_blank">https://github.com/huggingface/peft</a>  
âš™ï¸ PEFT ve LoRA implementasyonlarÄ±, best practices ve Ã¶rnekler.
</details>

<details>
<summary>ğŸ“‘ LoRA Paper</summary>
<a href="https://arxiv.org/abs/2106.09685" target="_blank">https://arxiv.org/abs/2106.09685</a>  
ğŸ§© "LoRA: Low-Rank Adaptation of Large Language Models" orijinal akademik makale.
</details>

<details>
<summary>ğŸ“ Datasets Library</summary>
<a href="https://huggingface.co/docs/datasets" target="_blank">https://huggingface.co/docs/datasets</a>  
âœ’ï¸ Veri yÃ¼kleme, preprocessing ve transformations rehberi.
</details>

<details>
<summary>ğŸ”„ Trainer API Guide</summary>
<a href="https://huggingface.co/docs/transformers/main_classes/trainer" target="_blank">https://huggingface.co/docs/transformers/main_classes/trainer</a>  
ğŸŒ Training loop otomasyonu ve advanced training techniques.
</details>

<details>
<summary>ğŸš€ BitsAndBytes (Quantization)</summary>
<a href="https://github.com/TimDettmers/bitsandbytes" target="_blank">https://github.com/TimDettmers/bitsandbytes</a>  
âš¡ INT8/4-bit quantization library ve optimization teknikleri.
</details>

<details>
<summary>ğŸ³ Model Deployment with Docker</summary>
<a href="https://docs.docker.com/" target="_blank">https://docs.docker.com/</a>  
ğŸ“¦ Containerization ve deployment best practices.
</details>

<details>
<summary>â˜¸ï¸ Kubernetes for ML</summary>
<a href="https://kubernetes.io/docs/tutorials/stateless-application/" target="_blank">https://kubernetes.io/docs/tutorials/stateless-application/</a>  
ğŸ”§ Orchestration, scaling ve production deployment.
</details>

---

## ğŸ’¡ Ä°puÃ§larÄ± ve Notlar

### ğŸ¯ Fine-Tuning Ä°puÃ§larÄ±
- **LoRA rank seÃ§imi:** Basit gÃ¶revler iÃ§in r=8-16, karmaÅŸÄ±k gÃ¶revler iÃ§in r=32-64
- **Learning rate:** LoRA iÃ§in genellikle full fine-tuning'den 3-10x daha yÃ¼ksek (1e-3 ~ 5e-3)
- **Warmup:** Total steps'in %10'u kadar warmup kullanÄ±n
- **Batch size:** Gradient accumulation ile effective batch size'Ä± artÄ±rÄ±n

### âš¡ Optimization Ä°puÃ§larÄ±
- **Mixed Precision (FP16):** %50 memory tasarrufu, 2-3x hÄ±z artÄ±ÅŸÄ±
- **Gradient Checkpointing:** %30-40 memory tasarrufu, %20 yavaÅŸlama
- **INT8 Quantization:** %50 memory tasarrufu, minimal accuracy kaybÄ±
- **Dynamic Padding:** Batch iÃ§indeki en uzun sequence'e gÃ¶re padding

### ğŸš€ Production Ä°puÃ§larÄ±
- **Model versiyonlama:** Her deployment iÃ§in version tag kullanÄ±n
- **Monitoring:** Latency, throughput ve error rate'i izleyin
- **A/B Testing:** Yeni modelleri production'a kademeli ÅŸekilde alÄ±n
- **Caching:** SÄ±k sorulan sorular iÃ§in response cache kullanÄ±n
- **Rate Limiting:** API abuse'i Ã¶nlemek iÃ§in rate limit uygulayÄ±n

### ğŸ”’ GÃ¼venlik Ä°puÃ§larÄ±
- **API Key Management:** Environment variables veya secret manager kullanÄ±n
- **Input Validation:** User input'larÄ± sanitize edin
- **Output Filtering:** Hassas bilgi leak'ini Ã¶nleyin
- **HTTPS:** Production'da mutlaka HTTPS kullanÄ±n

---

<p align="center" style="font-size:1.1em;">
	<b>ğŸŒŸ <span style="color:#CA6F1E;">PEFT & LoRA</span>, <span style="color:#229954;">modern LLM fine-tuning'in geleceÄŸidir!</span> ğŸŒŸ</b>
</p>

<br>

## ğŸ‰ SonuÃ§

Bu hafta, **modern LLM fine-tuning** tekniklerini, **memory-efficient training** yÃ¶ntemlerini ve **production-ready inference** sistemlerini Ã¶ÄŸrendik. PEFT ve LoRA ile %96+ parametre tasarrufu saÄŸlarken model performansÄ±nÄ± korumayÄ±, Hugging Face Datasets ve Trainer API ile professional training pipeline'larÄ± oluÅŸturmayÄ± ve quantization, batching gibi tekniklerle production ortamÄ±nda optimize edilmiÅŸ inference sistemleri tasarlamayÄ± deneyimledik.

**Ã–ÄŸrendiklerimiz:**
âœ… PEFT & LoRA ile memory-efficient fine-tuning  
âœ… Datasets ve Trainer API ile modern training pipeline  
âœ… Quantization ve optimization teknikleri  
âœ… KiÅŸiselleÅŸtirilmiÅŸ chatbot sistemleri  
âœ… Production deployment stratejileri  
âœ… Performance benchmarking ve monitoring  

**BaÅŸarÄ±lar! ğŸš€**

---

<p align="center">
	<b>Kairu AI - Build with LLMs Bootcamp | Hafta 6</b><br>
	<i>Modern LLM Fine-Tuning & Production Deployment</i>
</p>: Ä°leri DÃ¼zey Model Fine-tuning ve KiÅŸiselleÅŸtirme

Bu hafta, derin Ã¶ÄŸrenme modellerini verimli bir ÅŸekilde fine-tune etme ve kiÅŸiselleÅŸtirme konularÄ±nÄ± ele alacaÄŸÄ±z.

## ğŸ“š Konular

### 1. PEFT (Parameter Efficient Fine-Tuning)
- LoRA (Low-Rank Adaptation) nedir ve nasÄ±l Ã§alÄ±ÅŸÄ±r?
- QLoRA ile bellek optimizasyonu
- Adapter katmanlarÄ±
- PEFT ile model boyutunu kÃ¼Ã§Ã¼k tutma

### 2. Datasets + Trainer KullanÄ±mÄ±
- Hugging Face Datasets kÃ¼tÃ¼phanesi
- Veri Ã¶n iÅŸleme ve tokenization
- Trainer sÄ±nÄ±fÄ± ile model eÄŸitimi
- TrainingArguments konfigÃ¼rasyonu

### 3. Inference ve KiÅŸiselleÅŸtirilmiÅŸ Model
- Fine-tune edilmiÅŸ modeli kullanma
- Inference optimizasyonu
- Model deployment stratejileri
- KiÅŸiselleÅŸtirilmiÅŸ Ã§Ä±ktÄ±lar Ã¼retme

## ğŸ›  Pratik Uygulamalar

Her konu iÃ§in hands-on Ã¶rnekler ve kod snippet'leri iÃ§erir.

## ğŸ“‹ Gereksinimler

```bash
pip install transformers datasets peft accelerate bitsandbytes
```

## ğŸ¯ Ã–ÄŸrenme Hedefleri

Bu hafta sonunda:
- PEFT teknikleri ile verimli fine-tuning yapabileceksiniz
- Datasets ve Trainer kullanarak model eÄŸitimi gerÃ§ekleÅŸtirebileceksiniz
- Kendi modelinizi inference iÃ§in kullanabileceksiniz