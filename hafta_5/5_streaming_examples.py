"""
Hafta 5 - BÃ¶lÃ¼m 5: Streaming Output ve CanlÄ± Veri AkÄ±ÅŸÄ±
LangChain ile streaming ve real-time uygulamalar
"""

# Gerekli kÃ¼tÃ¼phaneleri iÃ§e aktarÄ±yoruz
import os
import time
import asyncio
from typing import Any, Dict, List, Optional

# LangChain kÃ¼tÃ¼phanelerini iÃ§e aktarÄ±yoruz
from langchain_openai import OpenAI
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain.callbacks.base import BaseCallbackHandler
from langchain.schema import LLMResult
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain.agents import AgentExecutor, Tool, initialize_agent, AgentType
from langchain.memory import ConversationBufferMemory

# .env dosyasÄ±ndan Ã§evre deÄŸiÅŸkenlerini yÃ¼klÃ¼yoruz (API anahtarlarÄ± iÃ§in)
from dotenv import load_dotenv
load_dotenv()

# =============================================================================
# Ã–ZEL CALLBACK HANDLER'LAR (Ã–ZEL YANIT YAKALAYICILARI)
# =============================================================================

class CustomStreamingHandler(BaseCallbackHandler):
    """
    Ã–zelleÅŸtirilmiÅŸ streaming handler sÄ±nÄ±fÄ±
    Bu sÄ±nÄ±f AI'Ä±n yanÄ±t Ã¼retme sÃ¼recini adÄ±m adÄ±m yakalar ve Ã¶zel iÅŸlemler yapar
    """
    
    def __init__(self):
        """
        SÄ±nÄ±f baÅŸlatÄ±cÄ±sÄ± - ilk deÄŸerleri ayarlÄ±yoruz
        """
        self.tokens = []  # Gelen token'larÄ± (kelime parÃ§alarÄ±nÄ±) saklayacak liste
        self.current_response = ""  # Åu anki yanÄ±tÄ±n tamamÄ±nÄ± tutacak deÄŸiÅŸken
    
    def on_llm_start(self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any) -> Any:
        """
        LLM (Dil Modeli) yanÄ±t Ã¼retmeye baÅŸladÄ±ÄŸÄ±nda Ã§aÄŸrÄ±lÄ±r
        KullanÄ±cÄ±ya iÅŸlemin baÅŸladÄ±ÄŸÄ±nÄ± bildirir
        """
        print("ğŸ¤– AI yanÄ±t oluÅŸturuyor...\n")
        print("ğŸ“ CanlÄ± YanÄ±t: ", end="", flush=True)  # SatÄ±r sonu yazmadan bekle
    
    def on_llm_new_token(self, token: str, **kwargs: Any) -> Any:
        """
        Her yeni token (kelime parÃ§asÄ±) geldiÄŸinde Ã§aÄŸrÄ±lÄ±r
        Bu sayede yanÄ±t kelime kelime ekranda gÃ¶rÃ¼nÃ¼r (typing efekti)
        """
        print(token, end="", flush=True)  # Token'Ä± hemen yazdÄ±r
        self.tokens.append(token)  # Token'Ä± listeye ekle
        self.current_response += token  # Toplam yanÄ±ta ekle
        time.sleep(0.05)  # Daktilo efekti iÃ§in kÄ±sa bekle
    
    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> Any:
        """
        LLM yanÄ±t Ã¼retmeyi bitirdiÄŸinde Ã§aÄŸrÄ±lÄ±r
        Ä°statistikleri gÃ¶sterir
        """
        print("\n\nâœ… YanÄ±t tamamlandÄ±!")
        print(f"ğŸ“Š Toplam token sayÄ±sÄ±: {len(self.tokens)}")
    
    def on_llm_error(self, error: Exception, **kwargs: Any) -> Any:
        """
        Hata oluÅŸtuÄŸunda Ã§aÄŸrÄ±lÄ±r
        Hata mesajÄ±nÄ± kullanÄ±cÄ±ya gÃ¶sterir
        """
        print(f"\nâŒ Hata: {error}")

class ProgressHandler(BaseCallbackHandler):
    """
    Ä°lerleme gÃ¶sterici handler sÄ±nÄ±fÄ±
    AI'Ä±n yanÄ±t Ã¼retme sÃ¼recini gÃ¶rsel olarak takip etmemizi saÄŸlar
    """
    
    def __init__(self):
        """
        Ä°lerleme gÃ¶stergesi iÃ§in gerekli deÄŸiÅŸkenleri baÅŸlatÄ±r
        """
        self.step_count = 0  # Hangi adÄ±mda olduÄŸumuzu tutar
        # Ä°lerleme adÄ±mlarÄ±nÄ±n listesi
        self.steps = ["ğŸ” Analiz", "ğŸ’­ DÃ¼ÅŸÃ¼nme", "âœï¸ Yazma", "ğŸ¯ Tamamlama"]
    
    def on_llm_start(self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any) -> Any:
        """
        LLM baÅŸladÄ±ÄŸÄ±nda ilerleme Ã§ubuÄŸunu baÅŸlatÄ±r
        """
        print("ğŸ“ˆ Ä°ÅŸlem BaÅŸlÄ±yor:")
        self.show_progress()
    
    def on_llm_new_token(self, token: str, **kwargs: Any) -> Any:
        """
        Her yeni token geldiÄŸinde ilerleme durumunu gÃ¼nceller
        Noktalama iÅŸaretlerini adÄ±m geÃ§iÅŸi olarak algÄ±lar
        """
        if len(token.strip()) > 0 and self.step_count < len(self.steps) - 1:
            # CÃ¼mle sonu iÅŸaretlerinde bir sonraki adÄ±ma geÃ§
            if token in ['.', '!', '?', '\n']:
                self.step_count += 1
                self.show_progress()
    
    def show_progress(self):
        """
        Ä°lerleme Ã§ubuÄŸunu ekranda gÃ¼nceller
        Tamamlanan adÄ±mlarÄ± âœ… ile, bekleyenleri â³ ile gÃ¶sterir
        """
        progress = "["
        for i, step in enumerate(self.steps):
            if i <= self.step_count:
                progress += f"âœ… {step} "  # Tamamlanan adÄ±m
            else:
                progress += f"â³ {step} "  # Bekleyen adÄ±m
        progress += "]"
        print(f"\r{progress}", end="", flush=True)  # AynÄ± satÄ±rda gÃ¼ncelle
    
    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> Any:
        """
        LLM bittiÄŸinde tÃ¼m adÄ±mlarÄ± tamamlanmÄ±ÅŸ olarak gÃ¶sterir
        """
        self.step_count = len(self.steps) - 1
        self.show_progress()
        print("\nğŸ‰ Ä°ÅŸlem TamamlandÄ±!\n")

# =============================================================================
# STREAMING Ã–RNEKLERÄ° (CANLI AKIÅ Ã–RNEKLERÄ°)
# =============================================================================

def basic_streaming_example():
    """
    Temel streaming Ã¶rneÄŸi
    Normal LLM ile streaming LLM arasÄ±ndaki farkÄ± gÃ¶sterir
    """
    print("=" * 60)
    print("1. TEMEL STREAMING OUTPUT")
    print("=" * 60)
    
    # Streaming Ã¶zelliÄŸi aktif olan LLM oluÅŸtur
    # StreamingStdOutCallbackHandler: YanÄ±tlarÄ± otomatik olarak ekrana yazdÄ±rÄ±r
    llm = OpenAI(
        temperature=0.7,  # YaratÄ±cÄ±lÄ±k seviyesi (0-1 arasÄ±)
        streaming=True,   # Streaming Ã¶zelliÄŸini aktifleÅŸtir
        callbacks=[StreamingStdOutCallbackHandler()]  # Otomatik yazdÄ±rma handler'Ä±
    )
    
    # Normal (streaming olmayan) LLM ile karÅŸÄ±laÅŸtÄ±rma
    print("Normal yanÄ±t (streaming yok):")
    normal_llm = OpenAI(temperature=0.7)  # Streaming kapalÄ±
    normal_response = normal_llm("Python hakkÄ±nda kÄ±sa bir aÃ§Ä±klama yaz.")
    print(normal_response)  # YanÄ±t tek seferde gelir
    
    print("\n" + "-" * 40)
    print("Streaming yanÄ±t:")
    # Streaming LLM ile aynÄ± soruyu sor - yanÄ±t kelime kelime gelecek
    streaming_response = llm("Python hakkÄ±nda kÄ±sa bir aÃ§Ä±klama yaz.")
    print("\n")
    
    return streaming_response

def custom_streaming_example():
    """
    Ã–zel streaming handler Ã¶rneÄŸi
    Kendi yazdÄ±ÄŸÄ±mÄ±z CustomStreamingHandler'Ä± kullanÄ±r
    """
    print("\n" + "=" * 60)
    print("2. Ã–ZEL STREAMING HANDLER")
    print("=" * 60)
    
    # Ã–zel handler'Ä±mÄ±zÄ± oluÅŸtur
    custom_handler = CustomStreamingHandler()
    
    # Ã–zel handler ile LLM oluÅŸtur
    llm = OpenAI(
        temperature=0.7,
        streaming=True,
        callbacks=[custom_handler]  # Ã–zel handler'Ä±mÄ±zÄ± kullan
    )
    
    # Uzun bir prompt ile test - daha Ã§ok token Ã¼retilecek
    prompt = """
    Yapay zeka teknolojisinin gelecekte topluma etkilerini detaylÄ± olarak aÃ§Ä±kla.
    Pozitif ve negatif etkileri ayrÄ± ayrÄ± ele al.
    """
    
    # LLM'den yanÄ±t al - custom handler otomatik Ã§alÄ±ÅŸacak
    response = llm(prompt)
    
    # Handler'dan toplanan bilgileri gÃ¶ster
    print(f"\nğŸ“‹ Handler Bilgileri:")
    print(f"- Toplanan token sayÄ±sÄ±: {len(custom_handler.tokens)}")
    print(f"- Ä°lk 5 token: {custom_handler.tokens[:5]}")
    print(f"- Son 5 token: {custom_handler.tokens[-5:]}")
    
    return custom_handler

def progress_streaming_example():
    """
    Ä°lerleme gÃ¶sterici ile streaming Ã¶rneÄŸi
    AI'Ä±n yanÄ±t Ã¼retme sÃ¼recini gÃ¶rsel olarak takip eder
    """
    print("\n" + "=" * 60)
    print("3. Ä°LERLEME GÃ–STERÄ°CÄ°LÄ° STREAMING")
    print("=" * 60)
    
    # Ä°lerleme gÃ¶sterici handler'Ä± oluÅŸtur
    progress_handler = ProgressHandler()
    
    # Ä°lerleme gÃ¶stericili LLM oluÅŸtur
    llm = OpenAI(
        temperature=0.7,
        streaming=True,
        callbacks=[progress_handler]  # Ä°lerleme handler'Ä±nÄ± kullan
    )
    
    # Uzun yanÄ±t gerektiren prompt
    prompt = """
    Bir startup'Ä±n baÅŸarÄ±lÄ± olmasÄ± iÃ§in gerekli 5 temel unsuru aÃ§Ä±kla.
    Her unsur iÃ§in detaylÄ± aÃ§Ä±klama yap.
    """
    
    # LLM'den yanÄ±t al - ilerleme Ã§ubuÄŸu otomatik gÃ¶sterilecek
    response = llm(prompt)
    print(response)
    
    return progress_handler

# =============================================================================
# STREAMING CHAIN Ã–RNEKLERÄ° (ZÄ°NCÄ°RLÄ° Ä°ÅLEMLER)
# =============================================================================

def streaming_chain_example():
    """
    Chain (zincir) ile streaming Ã¶rneÄŸi
    Template kullanarak daha karmaÅŸÄ±k iÅŸlemler yapar
    """
    print("\n" + "=" * 60)
    print("4. STREAMING CHAIN KULLANIMI")
    print("=" * 60)
    
    # Streaming Ã¶zellikli LLM oluÅŸtur
    streaming_llm = OpenAI(
        temperature=0.7,
        streaming=True,
        callbacks=[CustomStreamingHandler()]  # Ã–zel handler kullan
    )
    
    # Prompt template oluÅŸtur - deÄŸiÅŸken yerler iÃ§in {topic} kullan
    prompt = PromptTemplate(
        input_variables=["topic"],  # Girdi deÄŸiÅŸkenleri
        template="""
        Bu konu hakkÄ±nda yaratÄ±cÄ± bir hikaye yaz: {topic}
        Hikaye en az 200 kelime olsun ve heyecanlÄ± detaylar iÃ§ersin.
        """
    )
    
    # LLM ve prompt'u birleÅŸtirerek chain (zincir) oluÅŸtur
    chain = LLMChain(
        llm=streaming_llm,  # Streaming LLM kullan
        prompt=prompt       # Template'i baÄŸla
    )
    
    # Chain'i Ã§alÄ±ÅŸtÄ±r - {topic} yerine deÄŸer geÃ§ir
    print("Hikaye konusu: 'Uzayda kaybolmuÅŸ bir robot'")
    result = chain.run("uzayda kaybolmuÅŸ bir robot")
    
    return result

# =============================================================================
# REAL-TIME SOHBET SÄ°MÃœLASYONU (CANLI SOHBET)
# =============================================================================

class RealTimeChatBot:
    """
    GerÃ§ek zamanlÄ± sohbet botu sÄ±nÄ±fÄ±
    KullanÄ±cÄ±yla sÃ¼rekli sohbet edebilen, geÃ§miÅŸi hatÄ±rlayan bot
    """
    
    def __init__(self):
        """
        Sohbet botunu baÅŸlatÄ±r ve gerekli bileÅŸenleri oluÅŸturur
        """
        # KonuÅŸma geÃ§miÅŸini saklamak iÃ§in hafÄ±za oluÅŸtur
        self.memory = ConversationBufferMemory(
            memory_key="chat_history",  # HafÄ±za anahtarÄ±
            return_messages=True        # MesajlarÄ± dÃ¶ndÃ¼r
        )
        
        # Ã–zel streaming handler oluÅŸtur
        self.streaming_handler = CustomStreamingHandler()
        
        # Streaming Ã¶zellikli LLM oluÅŸtur
        self.llm = OpenAI(
            temperature=0.8,  # Daha yaratÄ±cÄ± yanÄ±tlar iÃ§in yÃ¼ksek deÄŸer
            streaming=True,
            callbacks=[self.streaming_handler]
        )
        
        # Sohbet iÃ§in prompt template oluÅŸtur
        self.prompt = PromptTemplate(
            input_variables=["chat_history", "user_input"],
            template="""
            Sen arkadaÅŸ canlÄ±sÄ± bir sohbet botusun. KullanÄ±cÄ±yla doÄŸal bir sohbet et.
            
            Ã–nceki konuÅŸma:
            {chat_history}
            
            KullanÄ±cÄ±: {user_input}
            
            Bot:
            """
        )
        
        # LLM, prompt ve hafÄ±zayÄ± birleÅŸtiren chain oluÅŸtur
        self.chain = LLMChain(
            llm=self.llm,
            prompt=self.prompt,
            memory=self.memory  # HafÄ±zayÄ± baÄŸla
        )
    
    def chat(self, user_input: str):
        """
        KullanÄ±cÄ± giriÅŸini iÅŸler ve yanÄ±t Ã¼retir
        
        Args:
            user_input (str): KullanÄ±cÄ±nÄ±n yazdÄ±ÄŸÄ± mesaj
            
        Returns:
            str: Botun yanÄ±tÄ±
        """
        print(f"\nğŸ‘¤ KullanÄ±cÄ±: {user_input}")
        
        # "Bot yazÄ±yor" efekti oluÅŸtur
        print("âŒ¨ï¸  Bot yazÄ±yor", end="", flush=True)
        for i in range(3):
            time.sleep(0.5)  # YarÄ±m saniye bekle
            print(".", end="", flush=True)  # Nokta ekle
        print("\n")
        
        # Chain'i Ã§alÄ±ÅŸtÄ±rarak yanÄ±t Ã¼ret (streaming ile)
        response = self.chain.run(user_input=user_input)
        return response

def realtime_chat_example():
    """
    GerÃ§ek zamanlÄ± sohbet Ã¶rneÄŸi
    Sohbet botunu test eder
    """
    print("\n" + "=" * 60)
    print("5. REAL-TIME SOHBET BOT'U")
    print("=" * 60)
    
    # Sohbet botunu oluÅŸtur
    chatbot = RealTimeChatBot()
    
    # Test iÃ§in simÃ¼le edilmiÅŸ konuÅŸma
    conversation = [
        "Merhaba! NasÄ±lsÄ±n?",
        "BugÃ¼n hava Ã§ok gÃ¼zel, sen ne yapÄ±yorsun?",
        "Yapay zeka hakkÄ±nda ne dÃ¼ÅŸÃ¼nÃ¼yorsun?",
        "Bana bir ÅŸaka anlatÄ±r mÄ±sÄ±n?"
    ]
    
    # Her mesajÄ± sÄ±rayla bota gÃ¶nder
    for message in conversation:
        try:
            response = chatbot.chat(message)  # MesajÄ± gÃ¶nder
            time.sleep(1)  # Sohbet akÄ±ÅŸÄ± iÃ§in bekle
        except Exception as e:
            print(f"Sohbet hatasÄ±: {e}")
            break  # Hata durumunda dÃ¶ngÃ¼den Ã§Ä±k
    
    return chatbot

# =============================================================================
# ASYNC STREAMING Ã–RNEKLERÄ° (ASENKRON CANLI AKIÅ)
# =============================================================================

async def async_streaming_example():
    """
    Asynchronous (eÅŸzamansÄ±z) streaming Ã¶rneÄŸi
    Birden fazla iÅŸlemi aynÄ± anda yapabilir
    """
    print("\n" + "=" * 60)
    print("6. ASYNC STREAMING (SimÃ¼le EdilmiÅŸ)")
    print("=" * 60)
    
    # FarklÄ± yanÄ±t parÃ§alarÄ±nÄ± simÃ¼le et
    responses = [
        "Python Ã§ok gÃ¼Ã§lÃ¼ bir programlama dilidir.",
        "Web geliÅŸtirme, veri analizi, yapay zeka alanlarÄ±nda kullanÄ±lÄ±r.",
        "SÃ¶z dizimi basit ve okunabilir olduÄŸu iÃ§in Ã¶ÄŸrenmesi kolaydÄ±r.",
        "GeniÅŸ kÃ¼tÃ¼phane ekosistemi sayesinde hÄ±zlÄ± geliÅŸtirme saÄŸlar."
    ]
    
    print("ğŸš€ Async streaming baÅŸlÄ±yor...\n")
    
    # Her yanÄ±t parÃ§asÄ±nÄ± iÅŸle
    for i, response in enumerate(responses, 1):
        print(f"ğŸ“¦ Chunk {i}: ", end="", flush=True)
        
        # Her karakteri ayrÄ± ayrÄ± yazdÄ±r (typing efekti)
        for char in response:
            print(char, end="", flush=True)
            await asyncio.sleep(0.03)  # Asenkron bekle
        
        print()  # Yeni satÄ±r
        await asyncio.sleep(0.5)  # Chunk'lar arasÄ± bekle
    
    print("\nâœ… Async streaming tamamlandÄ±!")

# =============================================================================
# PERFORMANS KARÅILAÅTIRMASI
# =============================================================================

def streaming_performance_comparison():
    """
    Streaming vs Normal LLM performans karÅŸÄ±laÅŸtÄ±rmasÄ±
    Hangi yÃ¶ntemin ne zaman kullanÄ±lacaÄŸÄ±nÄ± anlamaya yardÄ±mcÄ± olur
    """
    print("\n" + "=" * 60)
    print("7. PERFORMANS KARÅILAÅTIRMASI")
    print("=" * 60)
    
    # Test iÃ§in ortak prompt
    prompt = "Python programlama dilinin avantajlarÄ±nÄ± listele ve aÃ§Ä±kla."
    
    # Normal LLM testini yap ve sÃ¼resini Ã¶lÃ§
    print("â±ï¸  Normal LLM testi...")
    normal_llm = OpenAI(temperature=0.7)  # Streaming kapalÄ±
    start_time = time.time()  # BaÅŸlangÄ±Ã§ zamanÄ±
    normal_response = normal_llm(prompt)
    normal_time = time.time() - start_time  # GeÃ§en sÃ¼re
    
    # Normal LLM sonuÃ§larÄ±nÄ± gÃ¶ster
    print(f"Normal LLM sÃ¼resi: {normal_time:.2f} saniye")
    print(f"Normal yanÄ±t uzunluÄŸu: {len(normal_response)} karakter\n")
    
    # Streaming LLM testini yap ve sÃ¼resini Ã¶lÃ§
    print("â±ï¸  Streaming LLM testi...")
    streaming_llm = OpenAI(
        temperature=0.7,
        streaming=True,
        callbacks=[StreamingStdOutCallbackHandler()]  # CanlÄ± yazdÄ±rma
    )
    start_time = time.time()  # BaÅŸlangÄ±Ã§ zamanÄ±
    streaming_response = streaming_llm(prompt)
    streaming_time = time.time() - start_time  # GeÃ§en sÃ¼re
    
    # Streaming LLM sonuÃ§larÄ±nÄ± gÃ¶ster
    print(f"\nStreaming LLM sÃ¼resi: {streaming_time:.2f} saniye")
    print(f"Streaming yanÄ±t uzunluÄŸu: {len(streaming_response)} karakter")
    
    # KarÅŸÄ±laÅŸtÄ±rma analizi
    print(f"\nğŸ“Š Analiz:")
    print(f"- SÃ¼re farkÄ±: {abs(normal_time - streaming_time):.2f} saniye")
    print(f"- Streaming kullanÄ±cÄ± deneyimi: Daha iyi (canlÄ± feedback)")
    print(f"- Normal LLM: Daha hÄ±zlÄ± iÅŸlem (tek seferde)")

# =============================================================================
# ANA FONKSÄ°YON
# =============================================================================

def main():
    """
    Ana program fonksiyonu
    TÃ¼m Ã¶rnekleri sÄ±rayla Ã§alÄ±ÅŸtÄ±rÄ±r
    """
    print("LANGCHAIN STREAMING VE CANLI VERÄ° AKIÅI Ã–RNEKLERÄ°")
    print("Bu Ã¶rneklerde streaming output ve real-time uygulamalarÄ± Ã¶ÄŸreneceksiniz.\n")
    
    try:
        # TÃ¼m streaming Ã¶rneklerini sÄ±rayla Ã§alÄ±ÅŸtÄ±r
        basic_streaming_example()          # 1. Temel streaming
        custom_streaming_example()         # 2. Ã–zel handler
        progress_streaming_example()       # 3. Ä°lerleme gÃ¶stergesi
        streaming_chain_example()          # 4. Chain ile streaming
        realtime_chat_example()            # 5. CanlÄ± sohbet
        
        # Asenkron Ã¶rneÄŸi Ã§alÄ±ÅŸtÄ±r
        asyncio.run(async_streaming_example())  # 6. Async streaming
        
        # Performans karÅŸÄ±laÅŸtÄ±rmasÄ±
        streaming_performance_comparison()  # 7. Performans testi
        
        # Bitirme mesajÄ±
        print("\n" + "=" * 60)
        print("TÃœM STREAMING Ã–RNEKLERÄ° TAMAMLANDI!")
        print("ArtÄ±k kendi real-time uygulamalarÄ±nÄ±zÄ± geliÅŸtirebilirsiniz.")
        print("=" * 60)
        
        # KullanÄ±cÄ± iÃ§in pratik ipuÃ§larÄ±
        print("\nğŸ¯ STREAMING Ä°PUÃ‡LARI:")
        print("1. Uzun yanÄ±tlar iÃ§in streaming kullanÄ±n")      # KullanÄ±cÄ± beklemez
        print("2. KullanÄ±cÄ± deneyimini iyileÅŸtirir")           # CanlÄ± feedback
        print("3. Custom handler'lar ile Ã¶zelleÅŸtirin")        # Kendi ihtiyaÃ§larÄ±nÄ±za gÃ¶re
        print("4. Progress indicator'lar ekleyin")             # Ä°lerleme gÃ¶stergesi
        print("5. Error handling'i unutmayÄ±n")                 # Hata yÃ¶netimi Ã¶nemli
        
    except Exception as e:
        # Genel hata yakalama
        print(f"Genel hata: {e}")
        print("OpenAI API anahtarÄ±nÄ±zÄ± kontrol edin!")

# Program baÅŸlangÄ±Ã§ noktasÄ±
if __name__ == "__main__":
    main()  # Ana fonksiyonu Ã§alÄ±ÅŸtÄ±r